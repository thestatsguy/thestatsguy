<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning on The Stats Guy</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine learning on The Stats Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Jul 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Random Forests in R</title>
      <link>/post/2016/07/20/random-forests-in-r/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/07/20/random-forests-in-r/</guid>
      <description>IntroductionAs the name suggests, random forest models basically contain an ensemble of decision tree models, with each decision tree predicting the same response variable. The response may be categorical, in which case being a classification problem, or continuous / numerical, being a regression problem.
In this short tutorial, we will go through the use of tree-based methods (decision tree, bagging model, and random forest) for both classification and regression problems.</description>
    </item>
    
  </channel>
</rss>