<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML &amp; Stats on The Stats Guy</title>
    <link>/categories/ml-stats/</link>
    <description>Recent content in ML &amp; Stats on The Stats Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 11 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/ml-stats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[short] An uncommon approach in tackling class imbalance</title>
      <link>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</guid>
      <description>In supervised learning, one challenged faced by data scientists is classification class imbalance, where in a binary classification problem, instances in one class severely outnumbers instances in the other. This poses a problem as model performances may be misleading: a naive example would be to always predict negative in a 10% positive-90% negative dataset - accuracy would then be 90%, but the model would be utterly useless.
The typical approaches in alleviating class imbalance include using robust metrics such as the ROC-AUC, or performing downsampling of majority class or upsampling of minority class (e.</description>
    </item>
    
    <item>
      <title>Seven tips for working on analytics delivery projects</title>
      <link>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</guid>
      <description>Following are seven tips / tricks / hacks that I came to learnt (some of them the hard way) and compiled as a data scientist / delivery consultant / data science consultant. In brief, they are:
You to Yourself
 Develop a strategy
 Keep a delivery journal
 Plan your daily activities
 Frontload your projects
  You to Others
 Show mediocre output to no one</description>
    </item>
    
    <item>
      <title>Paper Review: To Tune or Not to Tune the Number of Trees in Random Forest</title>
      <link>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</guid>
      <description>Plotting different performance metrics against the number of trees in random forest. Source 
I came across the following paper during my Masters coursework that addresses a practical issue in the use of the random forest model, and in general, any other bootstrap aggregating ensembles:
Probst, P. &amp;amp; Boulestix, A-L. (2018). To Tune or Not to Tune the Number of Trees in Random Forest. Journal of Machine Learning Research, 18(181), 1-18.</description>
    </item>
    
    <item>
      <title>[short] My Master of Science in Statistics programme in NUS</title>
      <link>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>I have gotten quite a couple of questions regarding my current MSc Statistics programme in NUS. Here are some broadstroke information about the programme and how I am approaching it.
 I&amp;rsquo;m doing the MSc by Coursework programme, which means a research thesis is not part of my curriculum. A MSc by Research option is available.
 Under the Coursework programme, there is a Track 1 (40MC) programme and a Track 2 (80MC) one.</description>
    </item>
    
    <item>
      <title>Using waterfall charts to visualize feature contributions</title>
      <link>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</guid>
      <description>IntroductionI am using waterfall charts drawn in ggplot2 to visualize GLM coefficients, for regression and classification. Source Rmd file can be found here.
Waterfall chart: a simple visualization to illustrate the constituent components (numeric values) that make up the final model prediction, starting from the intercept term \(\beta_0\). The idea is quickly see which features contribute positively and which negatively, and by how much. Important thing to note here is that the waterfall chart will differ from test datapoint to test datapoint - we first have to make a prediction using a test sample \([x_1, x_2, .</description>
    </item>
    
    <item>
      <title>[short] Worked example on setting up SQL Server with R ODBC connection</title>
      <link>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</guid>
      <description>This is a worked example on how to set up SQL Server, SQL Server Management Studio, and a ODBC connection with R.
Step 1: Install SQL Server from https://www.microsoft.com/en-us/sql-server/sql-server-downloads. The SQL Server 2017 Express was good enough for me to run some analysis and modelling on my own. Once done, you should have a screen like this:
Step 2: Click on the “Install SSMS” button. SSMS stands for SQL Server Management Studio.</description>
    </item>
    
    <item>
      <title>The Machine Learning Life Cycle: how to run a ML project</title>
      <link>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</guid>
      <description>I recently came across this page in the DataRobot Artificial Intelligence Wiki. If you don&#39;t already know, DataRobot is currently one of the top automated machine learning platform in the market, with emphasis on supervised learning and citizen data science. I am quite a big fan of their platform - even though I don&#39;t use it in my work, I believe that they and their competitors in the market are heading into the right direction towards automated machine learning.</description>
    </item>
    
    <item>
      <title>[short] Feature Contribution - another way to think about feature importance</title>
      <link>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</guid>
      <description>A typical feature importance plot. Image source.  In many machine learning models, feature importance or variable importance is an important output from the model as it informs us about the relative or absolute importance of each feature in contributing to the model. More specifically, feature importance tells us which are the features that are highly differentiating, in the case of classification, and which are those that are not.</description>
    </item>
    
    <item>
      <title>Some data scientist interview questions - with a twist</title>
      <link>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</guid>
      <description>Machine Learning. Nothing to do with my intended topic, just a random xkcd comic that I thought is funny. Source 
Been wanting to do this consolidation for some time, so here goes. I won&amp;rsquo;t be touching on any language-specific questions (e.g. packages or functions), as I don&amp;rsquo;t believe they are relevant in this Google/Stack Overflow era - kind of missing the forest for the trees. Also, won&amp;rsquo;t be going over basic questions like &amp;ldquo;What is logistic regression&amp;rdquo; or &amp;ldquo;How does collaborative filtering works&amp;rdquo; or the like.</description>
    </item>
    
    <item>
      <title>Why ensemble modelling works so well - and one often neglected principle</title>
      <link>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</guid>
      <description>Putting models together in an ensemble learning fashion is a popular technique amongst data scientists 
Ensemble learning is the simultaneous use of multiple predictive models to arrive at a single prediction, based on a collective decision made together by all models in the ensemble. It&#39;s a common and popular technique used in predictive modelling, especially when individual models are failing to produce the required performance levels, in terms of e.</description>
    </item>
    
    <item>
      <title>[short] Quick start to using Git and GitHub</title>
      <link>/post/2017/04/12/short-quick-start-to-using-git-and-github/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/04/12/short-quick-start-to-using-git-and-github/</guid>
      <description>Git is pretty awesome. My most common &amp;ldquo;use case&amp;rdquo; for Git is working as a solo developer, working on different machines (home and work machines) and using GitHub to keep my work in place.
Following is the most straightforward way to start use Git and Github in this no-brainer manner. During development, only run steps indicated by * (7 - code, 9 - stage, 10 - commit, 12 - push).</description>
    </item>
    
    <item>
      <title>[short] Taking a part-time Masters this year in 2017</title>
      <link>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</link>
      <pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</guid>
      <description>I am leaning towards taking a part-time Masters this year in 2017. Points of considerations:
 Masters or PhD? Full-time or part-time? (If masters) technical or non-technical? (If full-time) Overseas or local? (If local) NUS or NTU?  Full-time is personally a non-option for me, as I don&amp;rsquo;t see the financial sense in taking a sabbatical to pursue a full-time programme. That leaves full-time and overseas out.
Based on what I know about a PhD programme, part-time PhD sounds a nightmare.</description>
    </item>
    
    <item>
      <title>Random Forests in R</title>
      <link>/post/2016/07/20/random-forests-in-r/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/07/20/random-forests-in-r/</guid>
      <description>IntroductionAs the name suggests, random forest models basically contain an ensemble of decision tree models, with each decision tree predicting the same response variable. The response may be categorical, in which case being a classification problem, or continuous / numerical, being a regression problem.
In this short tutorial, we will go through the use of tree-based methods (decision tree, bagging model, and random forest) for both classification and regression problems.</description>
    </item>
    
  </channel>
</rss>