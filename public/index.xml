<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Stats Guy</title>
    <link>/</link>
    <description>Recent content on The Stats Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A fuller review of my Master of Science in Statistics programme in NUS</title>
      <link>/post/2020/05/10/a-fuller-review-of-my-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/05/10/a-fuller-review-of-my-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Back in January 2017, I first seriously considered taking up a postgraduate degree, as a means to improve myself and continue learning. Well, it wasn&amp;rsquo;t a long and hard decision, really, as I had and still have the freedom, the capacity and the means to study more.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Taking a part-time Masters this year in 2017&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;2017/01/30&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I am leaning towards taking a part-time Masters this year in 2017. Points of considerations:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Masters or PhD?&lt;/li&gt;
&lt;li&gt;Full-time or part-time?&lt;/li&gt;
&lt;li&gt;(If masters) technical or non-technical?&lt;/li&gt;
&lt;li&gt;(If full-time) Overseas or local?&lt;/li&gt;
&lt;li&gt;(If local) NUS or NTU?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Full-time is personally a non-option for me, as I don’t see the financial sense in taking a sabbatical to pursue a full-time programme. That leaves full-time and overseas out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Based on what I know about a PhD programme, part-time PhD sounds like a nightmare. That leaves part-time Masters in Singapore as my option.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Next question: technical or non-technical? Well I am leaning towards to doing something with technical content when studying - non-technical content can be picked up most of the time simply by being widely read and learning from work experiences. This means statistics or computing for me.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;And NUS is probably the better choice than NTU. SMU is not in my consideration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;So for now, my choice is going to be M.Sc. Statistics from NUS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;So I decided to go with a Master of Science in Statistics programme in NUS, and starting my first semester in August 2017. I then took 5 semesters, all the way to December 2019, to complete the programme. Today, I have since happily graduated and have had a good experience with the programme.&lt;/p&gt;
&lt;p&gt;Around midway through the programme, I wrote a short review on the logistics and my experience of the programme so far.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;My Master of Science in Statistics programme in NUS&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;2019/02/09&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I have gotten quite a couple of questions regarding my current &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;MSc Statistics programme&lt;/a&gt; in NUS. Here are some broadstroke information about the programme and how I am approaching it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m doing the MSc by Coursework programme, which means a research thesis is not part of my curriculum. A MSc by Research option is available.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Under the Coursework programme, there is a Track 1 (40MC) programme and a Track 2 (80MC) one. Basically dependent on whether you have a Honours in your Bachelor&amp;rsquo;s degree. I&amp;rsquo;m doing the Track 1 programme - 40MC is equivalent to 10 modules. Under usual circumstances, it takes 2 full-time semesters to finish 10 modules, i.e. 1 academic year. Semesters run as per typical undergraduate semesters in Singapore.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;There&amp;rsquo;s also the part-time option, where one would take 4 to 5 semesters to finish the 10 modules - 5 semesters is basically 2 modules x 5 semesters = 10 modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m on the part-time programme. Personally, 3 modules on a part-time basis per semester is too much for me to handle - so I opt to finish my MSc in 5 semesters, or 2.5 academic years.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m currently in my 4th semester, so would be finishing the programme requirements by Dec 2019 and graduate during July 2020 (commencement).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;For MSc Statistics, lectures typically run from 7pm to 10pm weeknights. Each module has 1 lecture per week, with the typical workload of tutorials, homework assignments, individual or group projects, subjected to respective lecturer&amp;rsquo;s discretion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Lastly, the programme &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GDTuitionCurrent.pdf&#34;&gt;cost&lt;/a&gt; &lt;!-- raw HTML omitted --&gt;$2,500 per semester for Singaporeans who are taking this MSc programme as their first higher qualification programme under the &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GD-Eligibility-Guidelines.pdf&#34;&gt;MOE Subsidy&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;. Yes it&amp;rsquo;s pretty value for money if you ask me. This tuition fee amount is not unique to MSc Statistics, and is general to many other programmes in NUS, again provided if you belong to the above category.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;My experience with the programme is a positive one so far.  Difficulty and commitment level is within my comfort zone, and I managed to learn quite a couple of new things. Modules that I have taken include:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Applied Data Mining&lt;/li&gt;
&lt;li&gt;Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;Spatial Statistics&lt;/li&gt;
&lt;li&gt;Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;Experimental Design&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Also, in any case, it feels good to be a student again. Each time I go to my stats lecture after a long day of work, it &lt;!-- raw HTML omitted --&gt;almost&lt;!-- raw HTML omitted --&gt; always feels therapeutic. Yea, almost.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, if you are looking to advance your data science street cred via a postgraduate degree, this is just one of many options, even within NUS or Singapore. Do your research wisely before committing to any!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Well, this post first appeared on my WordPress blog &lt;a href=&#34;https://thestatsguy.home.blog/2019/02/09/my-master-of-science-in-statistics-programme-in-nus/&#34;&gt;here&lt;/a&gt;, and I realised that this post on WordPress turned out to be one of the top results in a number of Google searches on this topic. The other search results are of course dominated by links to NUS and Faculty of Science itself.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Turns out that I got a decent number of views on this post. Not bad considering that I don&amp;rsquo;t expect much (if any at all) traffic on my blog.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;a-fuller-review&#34;&gt;A fuller review&lt;/h3&gt;
&lt;p&gt;Since there is &lt;strong&gt;&lt;em&gt;some&lt;/em&gt;&lt;/strong&gt; interest in this topic, I thought I would spend some time in this post to do a more complete review of the programme and my experience.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Admission and logistics&lt;/li&gt;
&lt;li&gt;Review of some of the stats modules I took
&lt;ul&gt;
&lt;li&gt;ST5201 Basic Statistical Theory&lt;/li&gt;
&lt;li&gt;ST5202 Applied Regression Analysis&lt;/li&gt;
&lt;li&gt;ST5225 Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;ST5218 Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;ST5211 Sampling From Finite Populations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some relevant Singaporean hacks
&lt;ul&gt;
&lt;li&gt;SkillsFuture Credit&lt;/li&gt;
&lt;li&gt;Post-Secondary Education Account (PSEA)&lt;/li&gt;
&lt;li&gt;Income Tax Course Fees Relief&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How I went about my life while being a part-time student&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;admission-and-logistics&#34;&gt;Admission and logistics&lt;/h3&gt;
&lt;p&gt;On this, what I have written before &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/&#34;&gt;here&lt;/a&gt; pretty much summarized it, except that I just want to quickly refer you to this &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;link&lt;/a&gt; for the admission criteria and requirements.&lt;/p&gt;
&lt;h3 id=&#34;review-of-some-of-the-stats-modules-i-took&#34;&gt;Review of some of the stats modules I took&lt;/h3&gt;
&lt;h4 id=&#34;st5201-basic-statistical-theory&#34;&gt;ST5201 Basic Statistical Theory&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;ST5201 Basic Statistical Theory, later renamed to Statistical Foundations of Data Science^[I believe the content in this module became more applied and less theoretical after the renaming. I took it before the renaming.], is one of the two core modules for the MSc. Recommended to take during the very first semester of the programme, this module is the pre-requisite to several other MSc modules^[Though later on I realise that in the MSc programme, fulfilling pre-requisites is understandably loosely followed. It&amp;rsquo;s OK to take 5201 beyond your first semester.], and covers basic statistics and probability theory - &amp;ldquo;basic&amp;rdquo; as in fundamental and theoretical, not easy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploratory data analysis including heat map and concentration map&lt;/li&gt;
&lt;li&gt;Random variables&lt;/li&gt;
&lt;li&gt;Joint distributions&lt;/li&gt;
&lt;li&gt;Expected values&lt;/li&gt;
&lt;li&gt;Limit theorems.&lt;/li&gt;
&lt;li&gt;Estimation of parameters including maximum likelihood estimation, Bayesian approach to parameter estimation&lt;/li&gt;
&lt;li&gt;Testing hypotheses and confidence intervals, bootstrap method of finding confidence interval, generalized likelihood ratio statistics&lt;/li&gt;
&lt;li&gt;Summarizing data: measures of location and dispersion, estimating variability using Bootstrap method, empirical cumulative distribution function, survival function, kernel probability density estimate&lt;/li&gt;
&lt;li&gt;Basic ideas of predictive analytics using multiple linear and logistic regressions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was quite a challenging module for me as I never had a formal background in theoretical statistics via my BSc. The only way I could counteract this was to spend more time and energy in making up for my not-so-strong theoretical background. It was also my first semester in the MSc so this module was the one that set my expectations for the subsequent semesters, in terms of the amount of work needed per module. I was taught by Dr Choi Yunjin.&lt;/p&gt;
&lt;h4 id=&#34;st5202-applied-regression-analysis&#34;&gt;ST5202 Applied Regression Analysis&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;ST5202 Applied Regression Analysis is the second of the 2 core modules in the programme. Unlike 5201, content in 5202 was more platable to me, with these regression modelling techniques being more applied than theoretical. If you are largely familiar with regression analysis then this module is mainly a refresher more than anything else. Like 5201, 5202 is a pre-req to many other modules in the programme. I was also taught by Dr Choi Yunjin for this module.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple regression&lt;/li&gt;
&lt;li&gt;Model diagnostics, remedial measures&lt;/li&gt;
&lt;li&gt;Variable selection techniques&lt;/li&gt;
&lt;li&gt;Non-least squares estimation&lt;/li&gt;
&lt;li&gt;Nonlinear models&lt;/li&gt;
&lt;li&gt;One and two factor analysis of variance, analysis of covariance&lt;/li&gt;
&lt;li&gt;Linear model as special case of generalized linear model&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;st5225-statistical-analysis-of-networks&#34;&gt;ST5225 Statistical Analysis of Networks&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;ST5225 Statistical Analysis of Networks was taught by Dr Wang Wanjie. Quite an interesting module that is a little different from the other stats modules. Typically, network / graph analysis are covered more by a computer science course than a statistics course.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph structures, adjacency matrix&lt;/li&gt;
&lt;li&gt;Graph sampling&lt;/li&gt;
&lt;li&gt;Centrality, cohesion, density, cliques, clustering&lt;/li&gt;
&lt;li&gt;Graph partitions&lt;/li&gt;
&lt;li&gt;Matching markets&lt;/li&gt;
&lt;li&gt;The World Wide Web, PageRank&lt;/li&gt;
&lt;li&gt;Graph models, random graph, stochastic block model, exponential random graph model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another interesting point for me on this module was that the lectures took place on Saturdays 1pm to 3pm, which turned out to be a good timing for lectures^[What I did was that I would spend the whole Saturday morning to rest and get ready and then have lunch in school. After the lecture, I would then head over to VivoCity for dinner, coffee, and then do a bit more of work or studying. It was rather therapeutic.]. I don&amp;rsquo;t believe this module is offered regularly.&lt;/p&gt;
&lt;h4 id=&#34;st5218-advanced-statistical-methods-in-finance&#34;&gt;ST5218 Advanced Statistical Methods in Finance&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Of all the MSc modules that I took, this one stands out to be my favorite and resonates with me the most. Guess that’s mainly because I like topic, as well as of the fact that I was heavily experimenting with investing on my own during that time. I also found that using finance as the backdrop or context to study certain statistical concepts, such as copula or factor analysis, to be more engaging than perhaps studying these topics in vaccum.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistical distributions: Value-at-Risk (VaR)&lt;/li&gt;
&lt;li&gt;Linear regression: Capital Asset Pricing Model (CAPM)&lt;/li&gt;
&lt;li&gt;Factor analysis: Arbitrage Pricing Theory&lt;/li&gt;
&lt;li&gt;Time series analysis: price forecast, volatility modelling&lt;/li&gt;
&lt;li&gt;Copulae: tail dependence of asset prices&lt;/li&gt;
&lt;li&gt;Estimation of covariance matrix and optimization: Markowitz’s portfolio theory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many other topics that could just as well fit into the theme of the module, but unfortunately 13 weeks isn&amp;rsquo;t a very long time. I was taught by Prof Xia Yingcun and he is a great lecturer who painstakingly explains each and every little detail and concept so that it&amp;rsquo;s clear for his students^[Other than &lt;a href=&#34;https://en.wikipedia.org/wiki/Copula_(probability_theory)&#34;&gt;copulae&lt;/a&gt;. I had a tough time understanding and appreciating the concept of a copula, and happened to find this &lt;a href=&#34;https://twiecki.io/blog/2018/05/03/copulas/&#34;&gt;blog post&lt;/a&gt; do an expert job at demystifying it.]. I highly recommend this module if you have a chance to take it.&lt;/p&gt;
&lt;h4 id=&#34;st5211-sampling-from-finite-populations&#34;&gt;ST5211 Sampling From Finite Populations&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Finally, ST5211 Sampling From Finite Populations is my very last module during the MSc. I was taught by Prof Zhou Wang, who also painstakingly explains every detail to his students.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple random sampling&lt;/li&gt;
&lt;li&gt;Stratified sampling&lt;/li&gt;
&lt;li&gt;Ratio and regression estimation&lt;/li&gt;
&lt;li&gt;Sampling with unequal probabilities&lt;/li&gt;
&lt;li&gt;Systematic sampling&lt;/li&gt;
&lt;li&gt;Single stage cluster sampling&lt;/li&gt;
&lt;li&gt;Two-stage cluster sampling&lt;/li&gt;
&lt;li&gt;Design-based versus model-based inference&lt;/li&gt;
&lt;li&gt;Small domain estimation&lt;/li&gt;
&lt;li&gt;Nonresponse and other nonsampling errors&lt;/li&gt;
&lt;li&gt;Survey quality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I had a lot of fun with this module as it was the only module I had left during my final semester, so it was more enjoyable than it was hard work. And the content in this module is genuinely interesting and applicable in real life problems.&lt;/p&gt;
&lt;h3 id=&#34;some-relevant-singaporean-hacks&#34;&gt;Some relevant Singaporean hacks&lt;/h3&gt;
&lt;h4 id=&#34;skillsfuture-credit&#34;&gt;SkillsFuture Credit&lt;/h4&gt;
&lt;p&gt;This course is eligible for claim from your &lt;a href=&#34;https://www.skillsfuture.sg/Credit&#34;&gt;SkillsFuture Credit (SFC)&lt;/a&gt; - if you haven&amp;rsquo;t yet use any SFC before, then you should have $500 worth of opening credits to boot. To claim for SFC, log in into your SkillsFuture account and go to this &lt;a href=&#34;https://www.myskillsfuture.sg/content/portal/en/training-exchange/course-directory/course-detail.html?courseReferenceNumber=NUS-200604346E-01-1006ST1CWK&#34;&gt;page&lt;/a&gt; and click on &amp;ldquo;Claim SkillsFuture Credit&amp;rdquo;. Of course you can only claim for any one semester so feel free to claim it as early as possible.&lt;/p&gt;
&lt;p&gt;You have to do this before the semester for which you want to claim for SFC, and &lt;strong&gt;before (not after)&lt;/strong&gt; your Student Bill is finalized. What happens is that once approved, SkillsFuture will credit your $500 SFC credit directly to NUS, which will then appear in your Student Bill. You then pay the balance for your tuition fees.&lt;/p&gt;
&lt;h4 id=&#34;post-secondary-education-account-psea&#34;&gt;Post-Secondary Education Account (PSEA)&lt;/h4&gt;
&lt;p&gt;As a Singaporean student, the PSEA account is created when you turn 16, and balance from your Edusave account will be transferred to the PSEA account. Like the Edusave account, the PSEA can be used for education purposes. In addition, with the PSEA account, you get an interest of 2.5% per annum, and this account will be held until you are 30 years old - after which the balance will be transferred into your CPF-OA^[Don&amp;rsquo;t ask me why this Edusave -&amp;gt; PSEA -&amp;gt; CPF-OA transferring exists.].&lt;/p&gt;
&lt;p&gt;To use the PSEA for your MSc, go to this NUS Student Service &lt;a href=&#34;https://www.askstudentservice.nus.edu.sg/app/answers/detail/a_id/2425/related/1&#34;&gt;page&lt;/a&gt; and complete the Standing Order (SO) form, then submit it accordingly.&lt;/p&gt;
&lt;h4 id=&#34;income-tax-course-fees-relief&#34;&gt;Income Tax Course Fees Relief&lt;/h4&gt;
&lt;p&gt;Finally, this course is also eligible for Course Fees Relief for your Income Tax, up to a maximum of $5,500 per year. This &lt;a href=&#34;https://www.iras.gov.sg/IRASHome/Individuals/Locals/Working-Out-Your-Taxes/Deductions-for-Individuals/Course-Fees-Relief/&#34;&gt;page&lt;/a&gt; on the IRAS website spells out all the necessary details. Like most other reliefs, you can claim the Course Fees Relief by submitting it in your annual tax assessment - so that means claiming 2 semesters at a time. Of course, this only applies if you are an employed part-time student.&lt;/p&gt;
&lt;h3 id=&#34;how-i-went-about-my-life-while-being-a-part-time-student&#34;&gt;How I went about my life while being a part-time student&lt;/h3&gt;
&lt;p&gt;My time as a part-time student (2.5 years in total^[Throughout the 2.5 years, I had in fact switched jobs twice (another story for another time). Guess this didn&amp;rsquo;t really affect anything for my studying, other than going to school from different workplaces, and figuring out different printer settings in 3 offices to print my lecture notes.]) basically flew by, simply because of the packed schedules and constant back and forth between work and school. Since most lectures happened between 7pm to 10pm on weeknights, on lecture nights I would (gladly) leave work on time or early and make my way to school for dinner and lecture. I tried very much to not skip any lectures regardless of lecture recordings, but this proved to be occasionally impossible. And there were nights where I was simply too exhausted to go to school and sit for 3 hours to absorb content. While most of the time I would take MRT/bus to school, sometimes I would splurge a little and take a Grab. It was always good to reach school earlier, so that I can take my time with my dinner and enjoy the cheap Science canteen food and a cup of coffee.&lt;/p&gt;
&lt;p&gt;On most non-lecture nights I typically don&amp;rsquo;t touch any of my schoolwork - but I would dedicate one day of my weekend (usually the Sunday) to catch up on lectures and work on tutorials and assignments. During these days, I would spend the day in Utown and sort of &amp;ldquo;blend in&amp;rdquo; with the other undergraduates. If it&amp;rsquo;s not at Utown, then I would either spend the day in the Medical library or the Science library. Either way, there is still plenty of cheap food and coffee in school to replenish myself throughout my mugging.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Of course, sometimes one day simply isn&amp;rsquo;t enough so it would spill over to the other weeknights from time to time. Even as a part-time student, Recess Week was always great as it means no need to travel to school, no new content, and more time to catch up, and of course prepare for the mid-term exam or assignment.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;When it comes to the final exams, my protocol has always been to take roughly about 1.5 weeks of paid or study leave to prepare for the 2 final exams I have per semester. This was always a good break from work^[Studying to prepare for final exams is fun, but writing those &amp;ldquo;cheatsheets&amp;rdquo; definitely is not. They were a bane. If you don&amp;rsquo;t know what &amp;ldquo;cheatsheets&amp;rdquo; are, good for you.], as I typically don&amp;rsquo;t spend that much time away from work^[Yes, I am a little bit of a workaholic. Just a bit.]. I might also take one extra day of leave after the last paper to just relax and &amp;ldquo;celebrate&amp;rdquo; the fact that I finished yet another semester, before going back to work.&lt;/p&gt;
&lt;p&gt;In all, it was a rewarding experience and I am very glad that I took the plunge to commit to the MSc for the 5 semesters. It was great being a student again, having blocks of time during weeknights and weekends focusing on nothing else but the content on my lecture notes and assignments. I guess for those of you who have been in the workforce for a while now and would like a change of pace or a break in stagnancy, going back to school is definitely an option, be it full-time or part-time. In any case, I hope this post was as fun for you to read as it was for me to write. Thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Declining interest rates amidst Covid-19 - and a hidden star</title>
      <link>/post/2020/05/07/declining-interest-rates-amidst-covid-19-and-a-hidden-star/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/05/07/declining-interest-rates-amidst-covid-19-and-a-hidden-star/</guid>
      <description>&lt;p&gt;As Covid-19 continues to wreck havoc in many countries around the world, including the US, massive sell-offs have taken place in Wall Street and other markets.&lt;/p&gt;
&lt;p&gt;In a move to protect the US economy from the effects of the pandemic, the US Federal Reserve has cut interest rates twice during March 2020 - &lt;a href=&#34;https://www.channelnewsasia.com/news/business/in-an-emergency-move-us-federal-reserve-cuts-interest-rates-to-12496552&#34;&gt;first in early March&lt;/a&gt;, cutting rates by 50 basis points to a targeted range of 1% to 1.25%, then &lt;a href=&#34;https://www.straitstimes.com/business/economy/us-federal-reserve-cuts-interest-rates-to-near-zero-coordinates-with-other-central&#34;&gt;another cut in mid March&lt;/a&gt; to a targeted range of 0% to 0.25%. At the same time, a &lt;a href=&#34;https://www.cnbc.com/2020/03/15/federal-reserve-cuts-rates-to-zero-and-launches-massive-700-billion-quantitative-easing-program.html&#34;&gt;quantitative easing&lt;/a&gt; (QE) programme was also launched, entailing US$700 billion worth of asset purchases comprising US Treasury bonds and mortgage-backed securities, i.e. the Fed buys these securities from the market, and basically inject liquidity back into the market.&lt;/p&gt;
&lt;p&gt;The last time the Fed has taken steps similiar to this, &lt;a href=&#34;https://www.nytimes.com/2019/07/31/business/economy/federal-reserve-interest-rate-cut.html&#34;&gt;was during the 08 Global Financial Crisis&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;impact-on-singapore-and-ordinary-singaporeans&#34;&gt;Impact on Singapore and ordinary Singaporeans&lt;/h3&gt;
&lt;p&gt;Inevitably, interest rates in Singapore (SIBOR, SOR) have always been closely tied to US Fed rates, and hence these rates have been falling as well. For retail investors and lay Singaporeans like myself, there are several effects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Falling mortgage rates - housing loans will be cheaper, and refinancing may be viable if it&amp;rsquo;s an option for you.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.straitstimes.com/business/companies-markets/some-singapore-retail-investors-using-cheap-cash-to-load-up-on-stocks&#34;&gt;Or, you can even go on leverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Local banks and the STI suffer - our favorite DBS, OCBC, and UOB will have their Net Interest Margin (NIM) decrease, &lt;a href=&#34;https://www.businesstimes.com.sg/companies-markets/analysts-downgrade-singapore-banks-after-us-fed-rate-cut&#34;&gt;affecting their earnings&lt;/a&gt;. And with &lt;a href=&#34;https://sginvestors.io/market/sgx-share-price-performance/straits-times-index-constituents&#34;&gt;~35% of the STI&lt;/a&gt; is made up of these banks (at time of writing), the STI will drop as well. Of course the banks are not the only reason why the STI suffers.&lt;/li&gt;
&lt;li&gt;Deposit account rates fall - likewise, our favorite DBS Multiplier, OCBC 360, UOB One, and the likes will see a &lt;a href=&#34;https://www.straitstimes.com/business/banking/banks-here-cut-deposit-rates-in-line-with-global-markets&#34;&gt;cut in interest rates&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Singapore Government Securities rates fall - additionally, SGS bonds and SSBs will also see a decrease in yield.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;my-three-main-cash-stashes-uob-one-singapore-savings-bond-and-cimb-fastsaver&#34;&gt;My three main cash stashes: UOB One, Singapore Savings Bond, and CIMB FastSaver&lt;/h3&gt;
&lt;h4 id=&#34;uob-one&#34;&gt;UOB One&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.uob.com.sg/personal/save/chequeing/one-account.page&#34;&gt;UOB One&lt;/a&gt; has been my workhorse account for salary crediting, GIRO, credit card spend etc for several months now. The effective rate of the UOB One for 75K used to be 2.436%, if you meet the bonus interest requirements. Effective 1st May 2020, here is what it looks like:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The effective interest rates is now 1.796% for 75K. On the upside, UOB has retained the existing qualifying criteria so that there are no disruptions to our savings routine. Other deposit accounts have also cut their interest rates accordingly.&lt;/p&gt;
&lt;h4 id=&#34;singapore-savings-bond&#34;&gt;Singapore Savings Bond&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;SSB rates have fallen significantly over the past months, since early 2019. The June 2020 issue of SSB has a first year yield of 0.57% (10-year yield average of 1.05%) - the lowest it has ever been since the launch of SSBs in 2015.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Of course, SSBs are issued and guaranteed by the Singapore government. Nonetheless, the current state of SSBs, with a first year yield of 0.57% and 10-year average of 1.05%, is a shadow of its former self.&lt;/p&gt;
&lt;h4 id=&#34;cimb-fastsaver---the-hidden-star&#34;&gt;CIMB FastSaver - the hidden star&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cimbbank.com.sg/en/personal/products/accounts/savings-accounts/cimb-fastsaver-account.html&#34;&gt;CIMB FastSaver&lt;/a&gt; is my third cash stash. Unlike the typical high-interest workhorse accounts like DBS Multiplier or OCBC 360, the CIMB FastSaver does not have any special requirements of salary crediting, credit card spend, bill payments etc in order to be granted an interest rate of 1% (for first 50K). Considering there is no special requirements, 1% is definitely on the high side, especially in comparison to SSB and current FD rates:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Compare that with the CIMB FastSaver interest rates, as a deposit account:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;As of the time of writing this, there doesn&amp;rsquo;t seem to be any indications from CIMB to adjust the interest rate of the FastSaver account. I sure hope I didn&amp;rsquo;t jinxed it and it stays this way! Instead of buying any more SSBs, I will place spilled over cash from my UOB One into CIMB FastSaver for now - but I guess I won&amp;rsquo;t be surprised if there is going to be an adjustment to the CIMB FastSaver rates, putting out the light from the hidden star.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[wip] Introduction to modern portfolio theory</title>
      <link>/post/2020/04/10/wip-introduction-to-modern-portfolio-theory/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/04/10/wip-introduction-to-modern-portfolio-theory/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-formulation&#34;&gt;Problem formulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#portfolio-weights&#34;&gt;Portfolio weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#portfolio-expectation-and-variance&#34;&gt;Portfolio expectation and variance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-math-behind-diversification&#34;&gt;The math behind diversification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#two-uncorrelated-assets&#34;&gt;Two uncorrelated assets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-correlated-assets&#34;&gt;Two correlated assets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#starting-with-two-assets&#34;&gt;Starting with two assets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#efficient-portfolios&#34;&gt;Efficient Portfolios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-variance-portfolio&#34;&gt;Minimum variance portfolio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wip&#34;&gt;WIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I would like to talk about modern portfolio theory (MPT), a key topic in finance. The first couple of lines in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Modern_portfolio_theory&#34;&gt;MPT Wikipedia article&lt;/a&gt; explains it very well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(MPT) is a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. It is a formalization and extension of diversification in investing, the idea that owning different kinds of financial assets is less risky than owning only one type. Its key insight is that an asset’s risk and return should not be assessed by itself, but by how it contributes to a portfolio’s overall risk and return. It uses the variance of asset prices as a proxy for risk.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The objective of the MPT is to simultaneously maximise the returns of a portfolio, while at the same time, minimize the risk of the portfolio. Generally, we all know that as higher returns come with higher risk, and therefore the dual goals of maximising returns while minimising risk are at odds with each other. After all, why would an investor place himself in a riskier position without any due reward or &lt;strong&gt;risk premium&lt;/strong&gt;? It wouldn’t make sense. Note that in this and many statistical finance framework, risk is often measured by means of standard deviation of returns, either that of the asset or the portfolio&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Risk premium&lt;/em&gt;&lt;/strong&gt; refers to the difference between the expected return between a risky asset, like a stock, and that of a risk-free asset, like a Treasury Bill or a Singapore Savings Bond. Without any risk premium, there’s no reason to invest in anything other than risk-free assets.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem formulation&lt;/h2&gt;
&lt;p&gt;Suppose there are &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; assets with &lt;span class=&#34;math inline&#34;&gt;\(R_i, i = 1, ..., N\)&lt;/span&gt; denoting the random variables that represent their respective returns in a given time period, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R} = (R_1, ..., R_N)^{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Their returns and risks are respectively&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
r_i = ER_i \\\
\sigma_i = Var(R_i)\\\
cov(\mathbf{R}) = \Sigma = (\sigma_{ij})_{1 \le i,j \le N}
\end{array}
\]&lt;/span&gt;
We can then form a portfolio consisting of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; assets such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
R_{pf} = w_1R_1 + ... + w_NR_N = w^T\mathbf{R},
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(R_{pf}\)&lt;/span&gt; is the return of the portfolio and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; is the weight of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset in the portfolio, with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i = 1}^{N} w_i = 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In other words, the portfolio is the linear combination of the various assets under consideration. Of course, the question that becomes: how to choose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w} = (w_1, ..., w_N)^T\)&lt;/span&gt; such that expected returns are maximised while minimising risk?&lt;/p&gt;
&lt;div id=&#34;portfolio-weights&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Portfolio weights&lt;/h3&gt;
&lt;p&gt;A technical point to bring out here is that while &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i = 1}^{N} w_i = 1\)&lt;/span&gt;, there is no constraint on individual &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; to be non-negative. In fact, a negative &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; implies a short position in the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset, i.e. &lt;strong&gt;short selling&lt;/strong&gt;. This is in constrast to a &lt;em&gt;long&lt;/em&gt; position, i.e. buying the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset. In this post, we will only consider positive &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; only.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Holding a &lt;strong&gt;&lt;em&gt;long&lt;/em&gt;&lt;/strong&gt; position in a particular asset simply means buying that particular asset. On the other hand, &lt;strong&gt;&lt;em&gt;short selling&lt;/em&gt;&lt;/strong&gt; is where one sells an asset without owning it in the first place. The asset, e.g. a stock, is borrowed from a broker or another customer of the broker. At a later point in time, a stock must then be bought back from the market and then return to the lender. This closes the short position, and the idea is that if one is able to selling the borrowed stock at a higher price and returning it at a lower price, a profit is made.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;portfolio-expectation-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Portfolio expectation and variance&lt;/h3&gt;
&lt;p&gt;With this set-up, the expectation and variance of the portfolio would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ER_{pf} = \sum_iw_iER_i = \mathbf{w}^TE\mathbf{R}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = \sum_iw_i^2\sigma_i^2 + \sum_i\sum_{j\neq i}w_iw_j\sigma_{ij}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ij}\)&lt;/span&gt; is the covariance between the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th asset.&lt;/p&gt;
&lt;p&gt;Alternatively,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = \sum_i\sum_jw_iw_j\sigma_{ij} = \mathbf{w}_T\Sigma \mathbf{w}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-math-behind-diversification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The math behind diversification&lt;/h2&gt;
&lt;p&gt;We all know “not to put our eggs in one basket”, and have a diversified portfolio. Intutitively, we know that if we were to put all our money on a single stock, then we have placed a large bet on that one company. While we could instantly make it big just by having our one stock making it big, &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_aversion&#34;&gt;loss aversion&lt;/a&gt; would indicate that we would rather minimize the risk of losing it all in a single stock.&lt;/p&gt;
&lt;p&gt;In this sense, what divserification does to a portfolio is that it minimizes risk of the portfolio. While the mean return of a portfolio depends on the mean returns of the individual assets and their respective weights, the risk of the portfolio depends on both the risk of the individual assets, as well as each asset’s relationships with the others, in terms of correlations.&lt;/p&gt;
&lt;p&gt;This means that placing a right mix of weights and assets would reduce the risk of the portfolio - a fundamental idea in portfolio theory. Diversification, by way of investing in multiple assets, reduces risks.&lt;/p&gt;
&lt;p&gt;Let’s consider two instructive examples.&lt;/p&gt;
&lt;div id=&#34;two-uncorrelated-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two uncorrelated assets&lt;/h3&gt;
&lt;p&gt;Suppose we have 2 assets with returns &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;, with the same mean and variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{}
ER_1 = ER_2 = \mu \\\
Var(R_1) = Var(R_2) = \sigma^2
\end{array}
\]&lt;/span&gt;
Also, &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt; are uncorrelated, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Investing 100% in either &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt; would yield exactly the same return and the same risk. Now consider a portfiolio with some weight on both assets. Let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; be the weight for &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; - then we would have &lt;span class=&#34;math inline&#34;&gt;\(1-w\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. In this portfolio, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ER_{pf} = wER_1 + (1-w)ER_2 = w\mu + (1-w)\mu = \mu\]&lt;/span&gt;
With this, we know that the return of this portfolio does not depend on &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, given that the assets are uncorrelated, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = w^2Var(R_1) + (1-w)^2Var(R_2) + cov(R_1, R_2)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(cov(R_1, R_2) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = (w^2 + (1-w)^2)\sigma^2\]&lt;/span&gt;
When &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, we would have &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) = \sigma^2\)&lt;/span&gt;. However, for any other value of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;w&amp;lt;1\)&lt;/span&gt;, we would have &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) &amp;lt; \sigma^2\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-correlated-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two correlated assets&lt;/h3&gt;
&lt;p&gt;Let’s consider a similar scenario, but this time, &lt;span class=&#34;math inline&#34;&gt;\(cov(R_1, R_2) \neq 0\)&lt;/span&gt;. This also means that &lt;span class=&#34;math inline&#34;&gt;\(-1&amp;lt; \rho_{12} &amp;lt; 1\)&lt;/span&gt;. Consider a portfolio where &lt;span class=&#34;math inline&#34;&gt;\(w = \frac{1}{2}\)&lt;/span&gt;, i.e. placing equal weight on both assets &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. We would then have &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf} = \frac{1}{2}ER_1 + \frac{1}{2}ER_2 = \mu\)&lt;/span&gt;. More importantly,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(R_{pf}) = \frac{1}{4}Var(R_1) + \frac{1}{4}Var(R_2) + 2\frac{1}{2}\frac{1}{2}Cov(R_1, R_2) \\\
= \frac{1}{4}\sigma^2 + \frac{1}{4}\sigma^2 + \frac{1}{2}\rho_{12}\sigma^2 \\\
= \frac{1}{2}(1+\rho_{12})\sigma^2
\]&lt;/span&gt;
Therefore, for any value of &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(-1 &amp;lt; \rho_{12} &amp;lt; 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) &amp;lt; \sigma^2\)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-with-two-assets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starting with two assets&lt;/h2&gt;
&lt;p&gt;Again, the objective of treating portfolio selection as a statistical problem is to select individual &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; is large and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; is small. As with our two instructive examples above, we can start solving this problem by first considering &lt;span class=&#34;math inline&#34;&gt;\(N = 2\)&lt;/span&gt;, that is, choosing the weights &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; between two assets only. Inevitably, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
ER_{pf} = \sum_iw_iER_i = w_1ER_1 + w_2ER_2 \\\
Var(R_{pf}) = \sum_iw_i^2\sigma_i^2 + \sum_i\sum_{j\neq i}w_iw_jcov(R_1, R_2) = w_1^2\sigma_1^2 + w_2^2\sigma_2^2 + w_1w_2\rho_{12}\sigma_1\sigma_2 \\\
w_1+w_2 = 1, w_2 = 1 - w_1
\end{array}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt; is the correlation between &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. Again, we can let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; be the weight for &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; - then we would have &lt;span class=&#34;math inline&#34;&gt;\(1-w\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. Simplifying,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
ER_{pf} = wER_1 + (1-w)ER_2 = w\mu_1 + (1-w)\mu_2 \\\
Var(R_{pf}) = w^2Var(R_1) + (1-w)^2Var(R_2) + 2w_1w_2cov(R_1, R_2) \\\
= w^2\sigma_1^2+ (1-w)^2\sigma_2^2 + 2w(1-w)\rho_{12}\sigma_1\sigma_2
\]&lt;/span&gt;
Notice that in this set-up, &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt; are considered to be known and constant&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. We are concerned with choosing a value for &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; is maximised, while &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; is minimised, i.e. we treat both &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; as univariate functions of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s visualise this with a numerical example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# expected returns, standard deviation, and correlation of two assets
mu1 &amp;lt;- 0.2
mu2 &amp;lt;- 0.1
sigma1 &amp;lt;- 0.1
sigma2 &amp;lt;- 0.05
rho12 &amp;lt;- 0.25

# calculate return and risks based on w
cal_pf_return &amp;lt;- function(w){
  return(w*mu1 + (1-w)*mu2)
}
cal_pf_risk &amp;lt;- function(w){
  return(w^2*sigma1^2 + (1-w)^2*sigma2^2 + 2*w*(1-w)*rho12*sigma1*sigma2)
}

# weights span from -1 to 1; this considers short positions as well for illustration
weights &amp;lt;- seq(-1, 1, by = 0.01)

returns &amp;lt;- NULL
risks &amp;lt;- NULL

for(w in weights){
  returns &amp;lt;- c(returns, cal_pf_return(w))
  risks &amp;lt;- c(risks, cal_pf_risk(w))
}

plot(risks, returns,
     xlab = &amp;quot;Portfolio risk&amp;quot;,
     ylab = &amp;quot;Portfolio return&amp;quot;,
     main = &amp;quot;Risk-return relationship across weights&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-10-introduction-to-modern-portfolio-theory_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;efficient-portfolios&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Efficient Portfolios&lt;/h3&gt;
&lt;p&gt;Take a look at the plot above. Notice that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For every possible value of return, there is one corresponding value of risk.&lt;/li&gt;
&lt;li&gt;However, for every possible value of risk, there are two corresponding values of returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Naturally, for a given amount of risk that we would like to take on, we would want a higher return than a lower one. Therefore, any combination of risk-return that belongs to the top half of the plot would be portfolios that we would want, as compared to the lower half. In particular, each combination in the top half of the plot are considered as &lt;strong&gt;efficient portfolios&lt;/strong&gt;. At this point, we might also want to know which is the portfolio or weight that has the lowest risks. This portfolio is also known as the &lt;strong&gt;minimum variance portfolio&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Efficient portfolios&lt;/em&gt;&lt;/strong&gt; lie on on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Efficient_frontier&#34;&gt;&lt;strong&gt;efficient frontier&lt;/strong&gt;&lt;/a&gt;, the top half of our plot. By definition, the efficient frontier consists of portfolios such that for each of these portfolios and their respective returns, there exists no other portfolios that carries a lower portfolio risk for a given return (hence “efficient”&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;). The &lt;strong&gt;minimum variance portfolio&lt;/strong&gt; is one special case of an efficient portfolio - no other portfolios carries a lower portfolio risk than itself, and is the leftmost point of our plot.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min_risk_w &amp;lt;- weights[which(risks == min(risks))]
min_risk_return &amp;lt;- mean(cal_pf_return(min_risk_w))

efficient_idx &amp;lt;- which(returns &amp;gt;= min_risk_return)
inefficient_idx &amp;lt;- which(returns &amp;lt; min_risk_return)

plot(risks[efficient_idx], returns[efficient_idx],
     xlab = &amp;quot;Portfolio risk&amp;quot;,
     ylab = &amp;quot;Portfolio return&amp;quot;,
     main = &amp;quot;The Efficient Frontier&amp;quot;,
     ylim = c(0,0.2), xlim = c(0.002, 0.01))

lines(risks[inefficient_idx], returns[inefficient_idx], lty = &amp;quot;dotted&amp;quot;)

abline(h = min_risk_return)
text(x = 0.003, y = min_risk_return-0.01, &amp;quot;min variance portfolio&amp;quot;)
text(x = 0.006, y = 0.15, &amp;quot;efficient frontier&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-10-introduction-to-modern-portfolio-theory_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-variance-portfolio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Minimum variance portfolio&lt;/h3&gt;
&lt;p&gt;In the example above, we arrived at the minimum variance portfolio by doing some simple calculations. A more robust method would be arrive at the minimum variance portfolio analytically by treating the problem as a optimization problem, minimizing &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wip&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;WIP&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;min variance portfolio&lt;/li&gt;
&lt;li&gt;lagrangian multiplier&lt;/li&gt;
&lt;li&gt;risk free assets&lt;/li&gt;
&lt;li&gt;one risky and one risk free asset&lt;/li&gt;
&lt;li&gt;sharpe ratio, tangency portfolio&lt;/li&gt;
&lt;li&gt;generalise to N assets&lt;/li&gt;
&lt;li&gt;r examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In my personal opinion, using standard deviation as a measure of risk is a little iffy, for several reasons. For example, risk could be better represented as a probability of huge losses, rather than the sd of expected returns - but let’s stick to this for now.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In particular, choosing &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; minimizes &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma}{\sqrt{2}}\)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;It’s commonly said that stocks and bonds are considered to be negatively correlated. Assuming this is true, consider the two subdomains &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} &amp;lt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} &amp;gt; 0\)&lt;/span&gt;. We have &lt;span class=&#34;math inline&#34;&gt;\(\forall\rho_{12} &amp;lt; 0, Var(R_{pf}) &amp;lt; \frac{1}{2}\sigma^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\forall\rho_{12} &amp;gt; 0, Var(R_{pf}) &amp;gt; \frac{1}{2}\sigma^2\)&lt;/span&gt;, i.e. portfolio risk reduces when you have two negatively correlated assets in the portfolio - an intuitive result and justification for proper diversification and &lt;a href=&#34;https://www.investopedia.com/terms/s/strategicassetallocation.asp&#34;&gt;strategic asset allocation&lt;/a&gt;. This of course begs the question on whether stocks and bonds are indeed negatively correlated, in good economic times and in bad, and correlated by how much. We can explore this topic deeper in the future. Also, the math here is similar to that of &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/&#34;&gt;ensemble modelling&lt;/a&gt; in machine learning.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Of course the challenge in real life is to estimate them using historical data.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The same way an estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is &lt;a href=&#34;https://en.wikipedia.org/wiki/Efficient_estimator&#34;&gt;efficient&lt;/a&gt; if &lt;span class=&#34;math inline&#34;&gt;\(Var(\hat{\theta})\)&lt;/span&gt; is minimized.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Again, for simplification, we only consider long positions &lt;span class=&#34;math inline&#34;&gt;\(0 \leqslant w \leqslant 1\)&lt;/span&gt;, &lt;code&gt;weights &amp;lt;- seq(0, 1, by = 0.01)&lt;/code&gt; in this post. If short positions are considered, then &lt;span class=&#34;math inline&#34;&gt;\(-\infty \leqslant w \leqslant \infty\)&lt;/span&gt;.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>[short] Statistical methods in finance series</title>
      <link>/post/2020/03/19/short-statistical-methods-in-finance-series/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/03/19/short-statistical-methods-in-finance-series/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;It&amp;rsquo;s a sleepless night, so I decided to write a little. This will be the first part of a series that I would like to write about, circling around the use of statistical methods in finance. Most of the content that I will be exploring would originate from one of my NUS M.Sc Statistics modules, Advanced Statistical Methods in Finance. I was taught by Prof Xia Yingcun when I did this module.&lt;/p&gt;
&lt;p&gt;Of all the modules that I did during my M.Sc, I always found this one to resonate with me the most - guess that&amp;rsquo;s mainly because I like topic, as well as of the fact that I was heavily experimenting with investing on my own during that time. I also found that using finance as the backdrop or context to study certain statistical concepts, such as copula or factor analysis, to be more engaging than perhaps studying these topics in vaccum.&lt;/p&gt;
&lt;h1 id=&#34;statistical-methods-in-finance&#34;&gt;Statistical methods in finance&lt;/h1&gt;
&lt;p&gt;There are a number of statistical methods that can be directly mapped to their applications in finance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistical distributions: Value-at-Risk (VaR)&lt;/li&gt;
&lt;li&gt;Linear regression: Capital Asset Pricing Model (CAPM)&lt;/li&gt;
&lt;li&gt;Factor analysis: Arbitrage Pricing Theory&lt;/li&gt;
&lt;li&gt;Logit/probit models: credit scoring/rating&lt;/li&gt;
&lt;li&gt;Time series analysis: price forecast, volatility modelling&lt;/li&gt;
&lt;li&gt;Nonlinear regression: term structures of interest rates&lt;/li&gt;
&lt;li&gt;Monte-Carlo simulations: pricing of assets&lt;/li&gt;
&lt;li&gt;Copulae: tail dependence of asset prices&lt;/li&gt;
&lt;li&gt;Estimation of covariance matrix and optimization: Markowitz&amp;rsquo;s portfolio theory&lt;/li&gt;
&lt;li&gt;and many more&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this series, I would like to tackle some of these topics, by combining statistical theory, finance theory, and the use of R where relevant.&lt;/p&gt;
&lt;h1 id=&#34;broader-question-can-statistics-be-used-to-make-a-profit&#34;&gt;Broader question: can statistics be used to make a profit?&lt;/h1&gt;
&lt;p&gt;Since there are folks who decided to spend the time and energy to study the application of statistical methods in capital markets, surely it&amp;rsquo;s a worthwhile effort and making money is possible? Well, yes and no. It&amp;rsquo;s well known in statistical finance is that financial data has extremely low signal-to-noise (SNR) ratio. This means that for a model that predicts price, even if the model is correctly specified, due to the low SNR, it&amp;rsquo;s unlikely for the predictions to be useful. Most folks would hence consider the study of price volatility to be more important than price itself. If we are able to have an informed point of view about future volatility (perhaps due to the fact that volatility autocorrelation is strong =&amp;gt; volatility may be predicted in a GARCH/ARCH model), we would be able to act accordingly. We will talk about this along the way in this series.&lt;/p&gt;
&lt;h1 id=&#34;stylized-fact-in-statistical-finance&#34;&gt;Stylized fact in statistical finance&lt;/h1&gt;
&lt;p&gt;I previously (well, a year ago) wrote about &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/04/04/short-stylized-facts-in-statistical-finance/&#34;&gt;stylized facts&lt;/a&gt; in statistical finance, where I briefly listed without explanations various &amp;ldquo;facts&amp;rdquo;. Stylized facts are commonly used in statistical finance to address and summarize phenomenon directly observed from historical data, and explainable with a certain level of theoretical consistency and logic. There are a number of well-known ones, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Financial data has extremely low signal-to-noise ratio.&lt;/li&gt;
&lt;li&gt;Price changes are less volatile in bull markets and more volatile bear markets.&lt;/li&gt;
&lt;li&gt;Volatility clustering is typically observed in financial data; that is, large changes tend to be followed by large changes of either sign, and small changes tend to be followed by small changes of either sign.&lt;/li&gt;
&lt;li&gt;and others&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this series, we will necessarily also delve a little deeper into some of these stylized facts, by considering both empirical (historical data) and theoretical perspectives.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it for a brief introduction to series - looking forward to do deep dives into various topics!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fundamental Analysis 101: A basic overview of the fundamental analysis of stocks</title>
      <link>/post/2019/06/08/fundamental-analysis-101-a-basic-overview-of-the-fundamental-analysis-of-stocks/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/06/08/fundamental-analysis-101-a-basic-overview-of-the-fundamental-analysis-of-stocks/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;In this post, I would like to give an overview of the principles and concepts in the fundamental analysis of stocks.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Well, fundamental analysis (FA) is the analysis of the fundamentals of a company. The fundamentals of the company are trying to capture essentially the health of the company, in the present and future. The objective of FA is to discern a point of view on the current and future prospects of a company – the actionability from this point of view is of course whether to buy (or short-sell) the stock. If our FA suggests that the company is likely to perform well in the future, we would like a piece of the action.&lt;/p&gt;
&lt;p&gt;In this sense, there is an inherent notion of valuation – based on our view of the stock, we could, for example, conclude that the stock is currently under-valued and is likely to grow to at least a fair valuation. Therefore, we buy. Nonetheless, we don’t know when our under-valued stock would grow to our fair valuation. It could take days, months or years.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Any current under- or over-valuation of a stock implies that the market is not valuing the stock “correctly”, and is hence inefficient. One of the dominant criticisms against FA is the efficient market hypothesis (EMH). EMH states that at any point in time, the market is efficient and security prices are exactly where they should be – every security is valued correctly, taking into account all aspects of the security and the company. For example, the resultant price plunge from a JNJ lawsuit or a Boeing plane crash is exactly reflective of the inherent value of JNJ or Boeing.&lt;/p&gt;
&lt;p&gt;Corollary to the EMH is as follows – it would be impossible to derive any form of “correct” valuation or insights based on e.g. the fundamentals of JNJ or Boeing, no matter how we analyse the fundamentals. Research has shown that the strong condition of the EMH is largely true (reference), and there are occasions where inefficiencies occur.&lt;/p&gt;
&lt;p&gt;On the other hand, proponents of technical analysis (TA) argue that price movements are driven by signals in the historical prices themselves. TA relies on e.g. trading on momentum of the stock, and similar to the EMH, suggests that the JNJ lawsuit or Boeing plane crash has already been priced into the stock, and stock prices constantly moves to capture these news and signals.&lt;/p&gt;
&lt;p&gt;Based on my current understanding on securities and prices, FA makes sense to me. As someone with scientific and statistical background, it is challenging for me to convince myself that TA is not some variant of data dredging or confirmation bias. But of course, you can always take what you will and maintain healthy skepticism.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;As we mentioned above, the objective of FA is to discern a point of view on the current and future prospects of a company. The actionability from this point of view is of course whether to buy (or short-sell) the stock. More specifically, what are we interested in from an exercise of FA is to answer questions such as the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Is the company actually making money?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can the company continue to make money?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can the company beat its competition in time to come?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is the company is able to fulfil its debt obligations?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As simple as they are crafted, these questions are difficult to tackle and sometimes give ambiguous answers, leaving the analyst to provide a subjective opinion. Personally, this is one reason why FA appeals to me – the fact that the same set of numbers can lead to different analysis and stories meant that there’s more to the performance of a company than financials, much like data scientists would use data to tell a story and derive insights.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;There’s no need to further elaborate the difference between quantitative and qualitative fundamentals. While revenue, sales, and costs can be objectively measured, notions like brand loyalty, intellectual product, and management competence are hard to place a price tag on. Following table illustrates examples what we would like to look for in both aspects of FA.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Let’s take a look at each of them, starting from the qualitative factors.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;What exactly does a company do? The nature of a company’s business can vary from simplistic to downright complex and opaque. Something like McDonalds is not difficult to look at, while it is challenging to comprehensively evaluate Amazon or Google. The whole idea of understanding a company’s business model is consider whether its profits are sustainable in the short and long term, or a fluke. In addition, in this digital era, is the company and industry that we are looking at prone to disruption (think Kodak), or are the barriers to entry significant? It’s wiser and much easier to invest with a piece of mind on companies that we understand.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Likewise, is the industry a weary and inefficient one, or something that investment dollars are pouring into? Is it on the fringe of disruption or well-protected? Is the industry growing or shrinking?&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;How do the competitors in the same space match up? What is their respective market share? Is it a duopoly like Boeing and Airbus, or are there plenty of strong players? Are there any up and coming small players that could disrupt? Or are there any external, non-traditional players looking to enter the market? Are the R&amp;amp;D initiatives bearing fruits that can shake things up? Or are customers simply to loyal to certain brands for anything to change at all? Knowing the competitive landscape helps in making that informed decision.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Industries that are heavily regulated by authorities typically spend significant amounts of money lobbying, or have to be reactive to policy changes. Something like Medicare for all would make a dent on the bottomlines of healthcare companies.  Therefore, understanding the relationship between the company and the government helps in making an informed forecast.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;A solid business model won’t fly if the management is incompetent, leading to poor execution. For a retail investor, there is no truly good or robust way to assessing management competence. We would be able to fling a few million dollars around and demand attendance from the C-suite. The best would be to rely on Wall Street analyst opinions as well as to attend quarterly earnings calls with the CEO and/or CFO and judge for yourself. Are they open to tough line of questioning or are they dodging, spinning, or giving non-answers? We could also consider their historical records. In all, not easy to judge.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Finally, checks and balances are required to ensure that a company is compliant with laws and regulations. Financial transparency allows shareholders and investors information that they should rightly have access to. Conflicts of interest in management structure should be nonexistent and shareholders’ rights and interests should be upheld and represented.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The quantitative fundamentals of a company can all be found in its financial statements, which are quarterly, semi-annually or annually compiled and published. There are three major financial statements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Balance sheet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Income statement&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cash flow statement&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The balance sheet is a record of the company’s assets, liabilities, and equities at a given point in time. Assets are the all resources in the company, inclusive of free cash, inventory, real estate, machinery etc. The relationship between assets, liabilities, and equities are given in the fundamental accounting equation:&lt;/p&gt;
&lt;p&gt;Assets = Liabilities + Shareholders’ equity&lt;/p&gt;
&lt;p&gt;The right hand side of the equation represents the way in which these assets are financed, either by debt, i.e. liabilities, or by equity, i.e. all the value that has been contributed to the company by shareholders.&lt;/p&gt;
&lt;p&gt;The balance sheet is essentially a snapshot of the company at point in time, like an X-ray or MRI. While important, investors oftentimes overlook the balance sheet as revenue or earnings, which require a notion of time elapsed e.g. quarterly earnings, are not found in the balance sheet. On the other hand, the balance sheet informs investors about the amount of debt that the company has, how much cash and cash-like assets it possesses and so on – like how we take a blood test to look at various biomarkers like blood glucose or red blood cell count.&lt;/p&gt;
&lt;p&gt;There are two categories of assets – current and non-current assets. Current assets are largely liquid, perhaps with a latency of up to 12 months or less. Examples of current assets include cash, inventories, and account receivables, of course with cash being the most liquid.&lt;/p&gt;
&lt;p&gt;Typically, we can think of large amounts of free cash in a company as being an attractive investment. Simply put, there is an option value in cash for companies to grow in strategic directions, or provide some cushion during tough times – the proverbial “saving for the rainy day”. Another way to think about free cash is that the company is so caught up in earning money that its management has yet to figure a way to put the free cash into good use. Yet another way to see this is that its management is not far-sighted enough to know how to invest the cash. You can see that a small stockpile of cash can be argued accordingly in both ways as well.&lt;/p&gt;
&lt;p&gt;Inventories are goods that yet to be sold, while account receivables are outstanding payments due to the company by its customers. Inventories can be thought of as “held up” revenue, in that the goods have already been produced, but no revenue comes in because they are not sold yet. Like free cash, inventory levels can be analyzed accordingly. For example, inventory turnover (sales / average inventory) indicates how fast goods are moving through the system to customers. We would want inventories to remain at a steady level.&lt;/p&gt;
&lt;p&gt;Account receivables are credit issued to customers for their purchases. This allows the company to book top-line revenue and stimulate sales. Inevitably, too long of a credit is bad for the company.&lt;/p&gt;
&lt;p&gt;Non-current assets are essentially assets that cannot be liquidated in a jiffy – these includes real estate, factories etc. Even though non-current assets are counted in the balance sheet as assets, in reality they are not exactly resources that the company can rally in times of difficulty for any quick maneuvers.&lt;/p&gt;
&lt;p&gt;Liabilities are essentially debt, and can also be categorized under current liabilities and non-current liabilities, with the time horizon of current liabilities being say a year or less. For example, when a company issues a corporate bond of 10 years in tenure, this would be a non-current liability.&lt;/p&gt;
&lt;p&gt;Debt is critical for a company to succeed, when it manages to leverage appropriately. Of course, too much debt is unhealthy. The quick ratio, (current assets - inventories) / current liabilities, is a quick indicator – a quick ratio of more than 1 implies that short term debt obligations can be taken care of.&lt;/p&gt;
&lt;p&gt;Equity is the difference between assets and liabilities, represents what the shareholders own in the company. There are two key items in equity, namely the paid-in capital and retained earnings. Paid-in capital is the amount of money shareholders paid in the first place during public offering to own a share in the company. Retained earnings represent all the earnings made by the company, that instead of being paid out as dividend to shareholders, are retained to further the growth and operations of the company.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The income statement is a little less boring than the balance sheet, because it directly shows the revenue numbers of the company over a period of time. Numbers like revenue, earnings, earnings per share (EPS) illustrates the performance of the company. Revenue, specifically revenue growth is often a strong investing signal as it can show potential of the company and the market as well as the company’s positioning in the market.&lt;/p&gt;
&lt;p&gt;To get from revenue to earnings, we need to consider expenses. There are typically two key types of expenses, namely cost of goods sold (COGS) and selling, general, and administrative expenses (SG&amp;amp;A) – both are rather self-explanatory. With these, we can then get to gross margin (net sales – COGS) and operating income (gross income – operating expenses, i.e. excluding expenses related to e.g. taxes and interest).&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;While the cash flow statement sounds similar to the income statement, they are very different beasts for a simple reason – while the income statement captures revenue and expense transaction, if there’s no actual cash flow, no records will be made on the cash flow statement. This manner of recording in the income statement is known as accrual accounting. Cash flow records in the cash flow statement could come from operations, financing, and investing.&lt;/p&gt;
&lt;p&gt;As we discussed previously, cash is an important component in the health of the company. Cash means flexibility and agility. One of the key considerations in looking at a company’s cash flow statement is to assess its ability to generate free cash flow, or FCF. Think of it as all our monthly expenses being accounted for from our salary – the rest is free for us to save, invest, or spend on a little indulgence. The same logic applies to a company. A company with plenty of FCF can embark on various ways to make use of the cash, perhaps in R&amp;amp;D or paying some of them back to shareholders.&lt;/p&gt;
&lt;p&gt;A closely related topic in cash flow, or specifically, discounted cash flow, is that of valuation, which I would like to touch on more in-depth in a future post.&lt;/p&gt;
&lt;p&gt;Summing up, another key topic in fundamental analysis is the use of valuation ratio, such as earnings per share, or EPS, or price-to-earnings or P/E ratio, to evaluate and summarize a company – another topic that justifies a separate in-depth exploration.&lt;/p&gt;
&lt;p&gt;That’s all for now! Hope you have gained something from this brief introduction. Thanks for reading 😊&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Medtech 101: An overview of the medical device industry</title>
      <link>/post/2019/05/27/medtech-101-an-overview-of-the-medical-device-industry/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/27/medtech-101-an-overview-of-the-medical-device-industry/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>REITs 101: Introduction to real estate investment trusts</title>
      <link>/post/2019/05/20/reits-101-introduction-to-real-estate-investment-trusts/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/20/reits-101-introduction-to-real-estate-investment-trusts/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] An uncommon approach in tackling class imbalance</title>
      <link>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] Stylized facts in statistical finance</title>
      <link>/post/2019/04/04/short-stylized-facts-in-statistical-finance/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/04/04/short-stylized-facts-in-statistical-finance/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Reviewing means of stashing cash with impending recession in sight</title>
      <link>/post/2019/03/28/reviewing-means-of-stashing-cash-with-impending-recession-in-sight/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/28/reviewing-means-of-stashing-cash-with-impending-recession-in-sight/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Seven tips for working on analytics delivery projects</title>
      <link>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Following are seven tips / tricks / hacks that I came to learnt (some of them the hard way) and compiled as a data scientist / delivery consultant / data science consultant. In brief, they are:&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;You to Yourself&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Develop a strategy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Keep a delivery journal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plan your daily activities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Frontload your projects&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;You to Others&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Show mediocre output to no one&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow up on everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have 30 seconds responses to every possible question from the customer you can think of&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before I elaborate, let me clarify that by &amp;ldquo;customers&amp;rdquo;, I mean anyone who is related to the project, most possibly only with the exception of yourself, your teammates, and your project manager. If you work as a consultant, the idea of a customer is obvious. If you work in an in-house analytics outfit, then your customer is someone who will use your final output; could be your boss, the business owners, the IT department and engineers, the end-users etc.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;YOU TO YOURSELF&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;DEVELOP A STRATEGY&lt;!-- raw HTML omitted --&gt; - At the start of the project, develop a strategy or a plan. Take a piece of paper and a pen, write down how you want to tackle the problem. &lt;!-- raw HTML omitted --&gt;Writing it down is key&lt;!-- raw HTML omitted --&gt;. Consider everything - how data flows from point A and B, what models do you want to try/you think might work, what data pre-proc do you think you need/might be necessary, what is the ideal set of results/output for you - down to the data structures (R: data frame, list, vector, matrix; python: pandas dataframe, dictionaries, lists; pyspark: broadcasts, accumulators, local vs. distributed etc.), what packages do you think you will require (version numbers/compatibility?), what visualizations do you want to see, what would the ideal plot look like, what assumptions are you making, how much time do you think you need for each task, how big are the intermediate results, is the cluster/HDFS sized correctly, what difficulties do you think you will face. Draw flowcharts and diagrams, draw your pipelines. &lt;!-- raw HTML omitted --&gt;EVERYTHING&lt;!-- raw HTML omitted --&gt;. Again, &lt;!-- raw HTML omitted --&gt;writing them down is key&lt;!-- raw HTML omitted --&gt;. I strongly recommend using pen and paper for this, or a notebook. Don&amp;rsquo;t be afraid to take 1 or 2 hours on this. Be thorough. After you are done, take a picture of it with your phone and save it somewhere. Unless you are extremely clear right at the beginning what you want to do, this should probably be one of many drafts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;KEEP A DELIVERY JOURNAL&lt;!-- raw HTML omitted --&gt; - Keep a delivery journal. Document everything - What happened today, problems faced, how long did a certain procedure take, your modelling strategy, your thoughts, your gut feelings, meeting notes, what went wrong, what went right, insights, mistakes… everything. Think of it as a diary. Do it on a daily basis. Show this journal to no one but yourself. Write as the day progresses, don’t wait until the end of the day. If you find it hard to do this, try to jot down in concise but substantive points, and expound on them at the end of your day. Also, make reference to the strategy you developed. Did anything change for the better or the worse after today? Personally, I keep a Evernote window open while I work and write in it as the day progresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;PLAN YOUR DAILY ACTIVITIES&lt;!-- raw HTML omitted --&gt; - Plan your daily activities. At the end of your day, plan what do you want to achieve at the end of the next manday. Write them down. Don’t do this in the customer’s office - do this after you had your dinner, took a shower. I find myself writing more accurate projections of my following manday when I write this at home or in the hotel room, i.e. outside of the office. I do this using Evernote as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;FRONTLOAD YOUR PROJECT&lt;!-- raw HTML omitted --&gt; - To frontload your project means to do as much of the work as possible at the start of the project - preferably in the first week. I always had this idea subconsciously, even when I was still in university, but never verbalized it until I read &lt;a href=&#34;https://www.bookdepository.com/McKinsey-Edge-Success-Principles-from-Worlds-Most-Powerful-Consulting-Firm-Shu-Hattori/9781259588686&#34;&gt;The McKinsey Edge by Shu Hattori&lt;/a&gt;. Basically, if you say this to yourself: “This is the first week of the project, so I should just take it easy”, &lt;!-- raw HTML omitted --&gt;YOU ARE DEAD WRONG&lt;!-- raw HTML omitted --&gt;. If you have this sentiment during the first 5 mandays of your project, you are not doing the right thing. The first manweek is critical. Use it for the following:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Information (get up to speed with project kickoff materials, timeline, exact format of deliverables etc.)&lt;/li&gt;
&lt;li&gt;Clarifications (clarifications with your customers - this is the most important)&lt;/li&gt;
&lt;li&gt;Strategy (delivery or dev strategy, as above)&lt;/li&gt;
&lt;li&gt;Workflow (software, tools, access credentials)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, frontloading your project does not mean you jump right into developing the scripts and codebase and writing as much as possible. This is counter-productive, and most likely your codebase will turn out to be utterly useless by the second or third manweek. Instead, use the time to get the above issues out of the way, so that once you get into the dev rhythm, you don&amp;rsquo;t have to stop and mind about these pesky nonsense that will hurt your productivity.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;YOU TO OTHERS&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;SHOW MEDIOCRE OUTPUT TO NO ONE - AKA you always need a story to tell&lt;!-- raw HTML omitted --&gt; - As it turns out, impressions matter even in a delivery project, with defined deliverables and outcomes. When you meet your customers for the first time during presales or sign-off or the like, you get sized up. The next and the most crucial juncture in which you are sized up again is perhaps at your first intermediate output or milestone, whether it&amp;rsquo;s a MVP dashboard or some intermediate flat table of results or a deck depicting your first iteration of modelling using CRISP-DM. Never show mediocre output to your customers, even if you prefaced it with &amp;ldquo;This is just some intermediary results&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you ponder about this point for a little, you might realise that for a modelling project, this might be difficult to accomplish. What if after putting in 2 or 3 manweeks of modelling effort, your ROC-AUC is still stuck at 0.65? In this case, there are several things you can think about and show to your customers, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some EDA plot or statistic or metric to illustrate the quality of the data, or lack thereof&lt;/li&gt;
&lt;li&gt;Likewise, some plot or statistic or metric to show that one or more assumptions made in the project is not true, but only ostensible or perhaps even outright false. (In the latter case, you should go hammer your presales guy who scoped and sized this project. If you are the one who scoped it, you deserve it.)&lt;/li&gt;
&lt;li&gt;An alternative approach, not limited to changing performance metrics, including additional data or features, targetting or neglecting a specific subsample of the data for subsequent efforts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basically, you need a &lt;!-- raw HTML omitted --&gt;story&lt;!-- raw HTML omitted --&gt;. If your immediate output looks great, great, you have a story to tell. But if your immediate output is less than ideal, then you need to craft a story on how to improve things going forward.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;FOLLOW UP ON EVERYTHING&lt;!-- raw HTML omitted --&gt; - This ties in with frontloading your project - follow up on every single doubt you have in your mind. This is important because you will meet customers who know that you, as the delivery consultant, lack a certain piece of critical information, but didn&amp;rsquo;t share it with you anyway - simply because you didn&amp;rsquo;t ask. I had to learn this this hard way. And it&amp;rsquo;s a sure-lose situation for you because there is no good answer to their question &amp;ldquo;Why didn&amp;rsquo;t you ask me?&amp;quot;. However, do be careful when you follow up on questions with your customers. Make sure your question is thought-out and well-researched. Remember, impressions count.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;HAVE A 30-SECOND RESPONSE TO EVERY POSSIBLE QUESTION from the customer you can think of&lt;!-- raw HTML omitted --&gt; - This is another one that I picked up from the McKinsey Edge, and I really like this a lot. It&amp;rsquo;s simple to implement, yet so impactful and well thought-out. Inevitably, you can&amp;rsquo;t have responses to every single question there is - just make sure you have the responses to the key and obvious questions. For example,&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Why do you use this feature over the other?&lt;/li&gt;
&lt;li&gt;Why do you log-transform this feature?&lt;/li&gt;
&lt;li&gt;Why are you using &lt;!-- raw HTML omitted --&gt; over &lt;!-- raw HTML omitted --&gt;?&lt;/li&gt;
&lt;li&gt;Why are you using &lt;!-- raw HTML omitted --&gt; over &lt;!-- raw HTML omitted --&gt;?&lt;/li&gt;
&lt;li&gt;How do you interpret these results?&lt;/li&gt;
&lt;li&gt;How can I use these results?&lt;/li&gt;
&lt;li&gt;Etc etc etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The objective is to not babble like a fool for 2 or 3 mins and have zero concrete ideas or responses put across. No one has the time or patience to listen to your uninsightful babbling. We all know this one person in our workplace who keeps talking continuously but nothing substantial is actually put forth. Therefore, make sure you convey your idea across as concisely and succinctly as possible. 30 seconds is just a heuristic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Review: To Tune or Not to Tune the Number of Trees in Random Forest</title>
      <link>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Do Less, Better</title>
      <link>/post/2019/02/23/do-less-better/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/23/do-less-better/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] My Master of Science in Statistics programme in NUS</title>
      <link>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;I have gotten quite a couple of questions regarding my current &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;MSc Statistics programme&lt;/a&gt; in NUS. Here are some broadstroke information about the programme and how I am approaching it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;m doing the MSc by Coursework programme, which means a research thesis is not part of my curriculum. A MSc by Research option is available.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under the Coursework programme, there is a Track 1 (40MC) programme and a Track 2 (80MC) one. Basically dependent on whether you have a Honours in your Bachelor&amp;rsquo;s degree. I&amp;rsquo;m doing the Track 1 programme - 40MC is equivalent to 10 modules. Under usual circumstances, it takes 2 full-time semesters to finish 10 modules, i.e. 1 academic year. Semesters run as per typical undergraduate semesters in Singapore.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There&amp;rsquo;s also the part-time option, where one would take 4 to 5 semesters to finish the 10 modules - 5 semesters is basically 2 modules x 5 semesters = 10 modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;m on the part-time programme. Personally, 3 modules on a part-time basis per semester is too much for me to handle - so I opt to finish my MSc in 5 semesters, or 2.5 academic years.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;m currently in my 4th semester, so would be finishing the programme requirements by Dec 2019 and graduate during July 2020 (commencement).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For MSc Statistics, lectures typically run from 7pm to 10pm weeknights. Each module has 1 lecture per week, with the typical workload of tutorials, homework assignments, individual or group projects, subjected to respective lecturer&amp;rsquo;s discretion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lastly, the programme &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GDTuitionCurrent.pdf&#34;&gt;cost&lt;/a&gt; &lt;!-- raw HTML omitted --&gt;$2,500 per semester for Singaporeans who are taking this MSc programme as their first higher qualification programme under the &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GD-Eligibility-Guidelines.pdf&#34;&gt;MOE Subsidy&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;. Yes it&amp;rsquo;s pretty value for money if you ask me. This tuition fee amount is not unique to MSc Statistics, and is general to many other programmes in NUS, again provided if you belong to the above category.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My experience with the programme is a positive one so far.  Difficulty and commitment level is within my comfort zone, and I managed to learn quite a couple of new things. Modules that I have taken include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applied Data Mining&lt;/li&gt;
&lt;li&gt;Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;Spatial Statistics&lt;/li&gt;
&lt;li&gt;Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;Experimental Design&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, in any case, it feels good to be a student again. Each time I go to my stats lecture after a long day of work, it &lt;!-- raw HTML omitted --&gt;almost&lt;!-- raw HTML omitted --&gt; always feels therapeutic. Yea, almost.&lt;/p&gt;
&lt;p&gt;Finally, if you are looking to advance your data science street cred via a postgraduate degree, this is just one of many options, even within NUS or Singapore. Do your research wisely before committing to any!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using waterfall charts to visualize feature contributions</title>
      <link>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I am using waterfall charts drawn in ggplot2 to visualize GLM coefficients, for regression and classification. Source Rmd file can be found &lt;a href=&#34;https://github.com/thestatsguy/thestatsguy/blob/master/content/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Waterfall chart: inspired by their commonplace use in finance&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a simple visualization to illustrate the constituent components (numeric values) that make up the final model prediction, starting from the intercept term &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. The idea is quickly see which features contribute positively and which negatively, and by how much. Important thing to note here is that the waterfall chart will differ from test datapoint to test datapoint - we first have to make a prediction using a test sample &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2, ..., x_p]\)&lt;/span&gt;, get the prediction, then visualize individual &lt;strong&gt;absolute&lt;/strong&gt; feature contribution to the prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/static/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-1.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature contributions chart: this one is simpler. Same idea as above (also dependent on test sample &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2, ..., x_p]\)&lt;/span&gt;), but plotted by ranking the numeric contributions by their proportions &lt;strong&gt;relative&lt;/strong&gt; to the prediction&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; like this: &lt;code&gt;contribution_proportion = feature_contribution / prediction&lt;/code&gt;, written below as &lt;code&gt;cont_prop &amp;lt;- featcont/pred&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/static/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.3
library(magrittr)
library(ggplot2)

data(Boston)
set.seed(123)

# mean centering
b2 &amp;lt;- preProcess(Boston, method = &amp;quot;center&amp;quot;) %&amp;gt;% predict(., Boston)

idx &amp;lt;- createDataPartition(b2$medv, p = 0.8, list = FALSE)
train &amp;lt;- Boston[idx,]
test &amp;lt;- Boston[-idx,]

mod0 &amp;lt;- lm(data = train, medv ~.)

sm &amp;lt;- summary(mod0)
betas &amp;lt;- sm$coefficients[,1]

testcase &amp;lt;- test[1,]
pred &amp;lt;- predict(mod0, testcase)

# dot product between feature vector and beta
featvec &amp;lt;- testcase[-which(testcase %&amp;gt;% names == &amp;quot;medv&amp;quot;)] %&amp;gt;% as.matrix
betas2 &amp;lt;- betas[-1]

nm &amp;lt;- names(betas)
#betas2 %*% t(featvec)

# feature contributions
featcont &amp;lt;- betas2*featvec
featcont &amp;lt;- c(betas[1], featcont, pred)
names(featcont) &amp;lt;- c(nm, &amp;quot;Prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;waterfall-chart-on-regression-feature-contributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Waterfall chart on regression feature contributions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# waterfall chart on feature contribution
plotdata &amp;lt;- data.frame(coef = names(featcont), featcont = featcont, row.names = NULL)
plotdata$coef &amp;lt;- factor(plotdata$coef, levels = plotdata$coef)
plotdata$id &amp;lt;- seq_along(plotdata$coef)
plotdata$Impact &amp;lt;- ifelse(plotdata$featcont &amp;gt; 0, &amp;quot;+ve&amp;quot;, &amp;quot;-ve&amp;quot;)
plotdata[plotdata$coef %in% c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;Prediction&amp;quot;), &amp;quot;Impact&amp;quot;] &amp;lt;- &amp;quot;Initial/Net&amp;quot;
plotdata$end &amp;lt;- cumsum(plotdata$featcont)
plotdata$end &amp;lt;- c(head(plotdata$end, -1), 0)
plotdata$start &amp;lt;- c(0, head(plotdata$end, -1))
plotdata &amp;lt;- plotdata[, c(3, 1, 4, 6, 5, 2)]

gg &amp;lt;- ggplot(plotdata, aes(fill = Impact)) +
 geom_rect(aes(coef,
               xmin = id - 0.45,
               xmax = id + 0.45,
               ymin = end,
               ymax = start)) +
 theme_minimal() +
 #scale_fill_manual(values=c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;))
 scale_fill_manual(values=c(&amp;quot;darkred&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkblue&amp;quot;)) +
 theme(axis.text.x=element_text(angle=90, hjust=1))
## Warning: Ignoring unknown aesthetics: x
#coord_flip()

if(sign(plotdata$end[1]) != sign(plotdata$start[nrow(plotdata)]))
 gg &amp;lt;- gg + geom_hline(yintercept = 0)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
cont_prop &amp;lt;- featcont/pred

plot_data &amp;lt;- data.frame(coef = names(cont_prop),
                        cont_prop = cont_prop,
                        row.names = NULL)
plot_data &amp;lt;- plot_data[-nrow(plot_data),]

plot_data &amp;lt;- plot_data[order(plot_data$cont_prop, decreasing = FALSE),]

plot_data$coef &amp;lt;- factor(plot_data$coef, levels = plot_data$coef)

p&amp;lt;-ggplot(data=plot_data, aes(x=coef, y = cont_prop)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, fill = &amp;quot;darkblue&amp;quot;) +
    coord_flip() +
    theme_minimal() +
    xlab(&amp;quot;Features&amp;quot;) +
    ggtitle(&amp;quot;Feature Contributions&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;div id=&#34;preparation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kernlab)
## Warning: package &amp;#39;kernlab&amp;#39; was built under R version 3.5.3
## 
## Attaching package: &amp;#39;kernlab&amp;#39;
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     alpha
library(caret)
library(magrittr)
library(ggplot2)

data(spam)
set.seed(123)

# mean centering
s2 &amp;lt;- preProcess(spam, method = &amp;quot;center&amp;quot;) %&amp;gt;% predict(., spam)

idx &amp;lt;- createDataPartition(s2$type, p = 0.8, list = FALSE)
train &amp;lt;- s2[idx,]
test &amp;lt;- s2[-idx,]

mod0 &amp;lt;- glm(data = train, type ~., family =  binomial(link = logit))
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

sm &amp;lt;- summary(mod0)
betas &amp;lt;- sm$coefficients[,1]

testcase &amp;lt;- test[1,]
pred &amp;lt;- predict(mod0, testcase)

# dot product between feature vector and beta
featvec &amp;lt;- testcase[-which(testcase %&amp;gt;% names == &amp;quot;type&amp;quot;)] %&amp;gt;% as.matrix
betas2 &amp;lt;- betas[-1]

nm &amp;lt;- names(betas)
#betas2 %*% t(featvec)

# feature contributions
featcont &amp;lt;- betas2*featvec
featcont &amp;lt;- c(betas[1], featcont, pred)
names(featcont) &amp;lt;- c(nm, &amp;quot;Prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;waterfall-chart-on-classification-feature-contributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Waterfall chart on classification feature contributions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# waterfall chart on feature contribution
plotdata &amp;lt;- data.frame(coef = names(featcont), featcont = featcont, row.names = NULL)
plotdata$coef &amp;lt;- factor(plotdata$coef, levels = plotdata$coef)
plotdata$id &amp;lt;- seq_along(plotdata$coef)
plotdata$Impact &amp;lt;- ifelse(plotdata$featcont &amp;gt; 0, &amp;quot;+ve&amp;quot;, &amp;quot;-ve&amp;quot;)
plotdata[plotdata$coef %in% c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;Prediction&amp;quot;), &amp;quot;Impact&amp;quot;] &amp;lt;- &amp;quot;Initial/Net&amp;quot;
plotdata$end &amp;lt;- cumsum(plotdata$featcont)
plotdata$end &amp;lt;- c(head(plotdata$end, -1), 0)
plotdata$start &amp;lt;- c(0, head(plotdata$end, -1))
plotdata &amp;lt;- plotdata[, c(3, 1, 4, 6, 5, 2)]

gg &amp;lt;- ggplot(plotdata, aes(coef, fill = Impact)) +
 geom_rect(aes(x = coef,
               xmin = id - 0.45,
               xmax = id + 0.45,
               ymin = end,
               ymax = start)) +
 theme_minimal() +
 #scale_fill_manual(values=c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;))
 scale_fill_manual(values=c(&amp;quot;darkred&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkblue&amp;quot;)) +
 theme(axis.text.x=element_text(angle=90, hjust=1))
## Warning: Ignoring unknown aesthetics: x
 #coord_flip()
 
if(sign(plotdata$end[1]) != sign(plotdata$start[nrow(plotdata)]))
 gg &amp;lt;- gg + geom_hline(yintercept = 0)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/c_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Something like &lt;a href=&#34;https://en.wikipedia.org/wiki/Waterfall_chart&#34;&gt;this&lt;/a&gt; or &lt;a href=&#34;http://blog.slidemagic.com/2008/08/how-to-create-mckinsey-waterfall-chart.html&#34;&gt;this&lt;/a&gt;, for example&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/&#34;&gt;this&lt;/a&gt; on feature contributions.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>[short] Worked example on setting up SQL Server with R ODBC connection</title>
      <link>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</guid>
      <description>


&lt;p&gt;This is a worked example on how to set up SQL Server, SQL Server Management Studio, and a ODBC connection with R.&lt;/p&gt;
&lt;p&gt;Step 1: Install SQL Server from &lt;a href=&#34;https://www.microsoft.com/en-us/sql-server/sql-server-downloads&#34; class=&#34;uri&#34;&gt;https://www.microsoft.com/en-us/sql-server/sql-server-downloads&lt;/a&gt;. The SQL Server 2017 Express was good enough for me to run some analysis and modelling on my own. Once done, you should have a screen like this:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/post/images/Capture.PNG&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;p&gt;Step 2: Click on the “Install SSMS” button. SSMS stands for SQL Server Management Studio. Once done, connect to the server:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/post/images/Capture3.PNG&#34;&gt;
&lt;/center&gt;
&lt;p&gt;Step 3: Create a database on the server. You may follow the steps given in this page as a quick start: &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/ssms/tutorials/connect-query-sql-server?view=sql-server-2017&#34; class=&#34;uri&#34;&gt;https://docs.microsoft.com/en-us/sql/ssms/tutorials/connect-query-sql-server?view=sql-server-2017&lt;/a&gt;. If you do, you should have a database created named “TutorialDB” and a table named “Customers”.&lt;/p&gt;
&lt;p&gt;Step 4: Install and load the RODBC package in R.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#install.packages(&amp;quot;RODBC&amp;quot;)
library(RODBC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 5: Connect to the server and the database, and run a sample query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conn &amp;lt;- odbcDriverConnect(&amp;#39;driver={SQL Server};server=SNG1049387\\SQLEXPRESS;database=TutorialDB;trusted_connection=true&amp;#39;)
customers &amp;lt;- sqlQuery(conn, &amp;#39;select * from dbo.Customers&amp;#39;)
str(customers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# &amp;#39;data.frame&amp;#39;: 4 obs. of  4 variables:
#  $ CustomerId: int  1 2 3 4
#  $ Name      : Factor w/ 4 levels &amp;quot;Donna&amp;quot;,&amp;quot;Janet&amp;quot;,..: 4 3 1 2
#  $ Location  : Factor w/ 4 levels &amp;quot;Australia&amp;quot;,&amp;quot;Germany&amp;quot;,..: 1 3 2 4
#  $ Email     : Factor w/ 4 levels &amp;quot;&amp;quot;,&amp;quot;donna0@adventure-works.com&amp;quot;,..: 1 4 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 6: Write an R data frame into your database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df &amp;lt;- read.csv(&amp;quot;data/adult.csv&amp;quot;)
sqlSave(conn, df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 7: Refresh the Database node in SMSS to verify if the data frame has been written into the database as a table.&lt;/p&gt;
&lt;p&gt;You are now ready to use SQL Server, SSMS, and R to run some analysis and modelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Machine Learning Life Cycle: how to run a ML project</title>
      <link>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] Feature Contribution - another way to think about feature importance</title>
      <link>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>A guide to dollar cost averaging in Singapore</title>
      <link>/post/2019/01/13/a-guide-to-dollar-cost-averaging-in-singapore/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/13/a-guide-to-dollar-cost-averaging-in-singapore/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Some data scientist interview questions - with a twist</title>
      <link>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Why ensemble modelling works so well - and one often neglected principle</title>
      <link>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Accuracy of ensemble = 0.216 + (0.144)(3) = 0.648 &amp;gt; 0.6
(&lt;!-- raw HTML omitted --&gt;3 times because there are 3 different ways of getting 2 correct, 1 wrong.&lt;!-- raw HTML omitted --&gt;)
&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] Good news for fans of Singapore Savings Bonds</title>
      <link>/post/2018/12/21/short-good-news-for-fans-of-singapore-savings-bonds/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/21/short-good-news-for-fans-of-singapore-savings-bonds/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of the Phillip SING Income ETF</title>
      <link>/post/2018/10/20/analysis-of-the-phillip-sing-income-etf/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/20/analysis-of-the-phillip-sing-income-etf/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>OCBC 360 Interest structure change: what&#39;s the impact on you and me?</title>
      <link>/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>[short] Book Review: Rich by Retirement by Joshua Giersch</title>
      <link>/post/2018/09/16/short-book-review-rich-by-retirement-by-joshua-giersch/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/09/16/short-book-review-rich-by-retirement-by-joshua-giersch/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Joshua Giersch, better known as “Shiny Things”, is an investment guru who has been generously giving investing advice on &lt;a href=&#34;https://forums.hardwarezone.com.sg/money-mind-210/%2Aofficial%2A-shiny-things-club-part-2-a-5813566.html&#34;&gt;HardwareZone&lt;/a&gt; and his &lt;a href=&#34;https://moneygowherefind.com/joshua-giersch-aka-shiny-things-hwz/&#34;&gt;blog&lt;/a&gt;. His advice are consistently dished out with the SG context in mind, and his book &lt;a href=&#34;https://www.amazon.com/Rich-Retirement-Singaporeans-Invest-Wealthy-ebook/dp/B01JXW17ZM&#34;&gt;&amp;ldquo;Rich by Retirement: How Singaporeans Can Invest Smart and Retire Wealthy&amp;rdquo;&lt;/a&gt;, is, in essence, the consolidation of all of his tips and hacks on being a successful investor in SG.&lt;/p&gt;
&lt;p&gt;The most interesting component of his philosophy is his very simple portfolio strategy. Whether it is too simple for your liking is for you to decide, and in this post I will summarize, from my perspective, the key points in his book. Enjoy.&lt;/p&gt;
&lt;h1 id=&#34;emergency-fund&#34;&gt;Emergency fund&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Keep an emergency fund with 6 months worth of expense&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;insurance&#34;&gt;Insurance&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Get hospitalization plan (paid in part using Medisave), and term life if you have dependents&lt;/li&gt;
&lt;li&gt;All other forms of insurance are neither necessary nor worthwhile&lt;/li&gt;
&lt;li&gt;Especially avoid ILPs and endowments&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;brokerage-accounts&#34;&gt;Brokerage accounts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Only pay &amp;lt;= 0.2% for trading SG stocks, &amp;lt;= 0.3% for US/UK stocks; UTs/ETFs with &amp;lt;= 0.5% expense ratio are ok&lt;/li&gt;
&lt;li&gt;Anything beyond these percentages are not acceptable&lt;/li&gt;
&lt;li&gt;Ideally regulated in SG or US&lt;/li&gt;
&lt;li&gt;No custody or dividend handling fees&lt;/li&gt;
&lt;li&gt;Recommended: SCB, Interactive Brokers&lt;/li&gt;
&lt;li&gt;Exception to the &amp;lt;= 0.2% recommendation: POSB Invest-Saver, because of the lack of min amount per txn. Good for small investors who are just starting out&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;asset-class-allocation&#34;&gt;Asset class allocation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Stocks: 110 – current age; bonds: the rest&lt;/li&gt;
&lt;li&gt;For stocks, 50:50 on SG and int’l&lt;/li&gt;
&lt;li&gt;For bonds, only SG&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;exchange-traded-funds&#34;&gt;Exchange traded funds&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ETFs are great because of the instant diversification. The ideal ETF would be one with &amp;lt;= 0.3% expense ratio, &amp;gt; 100mil in assets, and is a physical ETF&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-shiny-things-3-etf-portfolio&#34;&gt;The Shiny Things 3-ETF portfolio&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;IWDA.LN (iShares Core MSCI World UCITS ETF – for Int’l exposure)&lt;/li&gt;
&lt;li&gt;ES3.SI (SPDR Straits Times Index ETF – for SG exposure)&lt;/li&gt;
&lt;li&gt;A35.SI (ABF Singapore Bond Index Fund – for SG bonds)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cpfsrs&#34;&gt;CPF/SRS&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Keep CPF in cash, don’t buy any counters&lt;/li&gt;
&lt;li&gt;Consider using SRS top-ups for tax breaks; can use SRS to buy ES3 (use OCBC BCIP – though A35 is not available)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;dollar-cost-averaging&#34;&gt;Dollar cost averaging&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Don’t time the market – “time in market” &amp;gt; “time the market”&lt;/li&gt;
&lt;li&gt;POSB Invest-Saver is great for DCA&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;portfolio-rebalancing&#34;&gt;Portfolio rebalancing&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Only rebalance in May and Nov every year&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple enough? Pick up his book from Amazon or NLB (I had to make a reservation – $1.55 reservation fee) and you will be able to understand his justifications for each of these pointers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Quick start to using Git and GitHub</title>
      <link>/post/2017/04/12/short-quick-start-to-using-git-and-github/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/04/12/short-quick-start-to-using-git-and-github/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Git is pretty awesome. My most common &amp;ldquo;use case&amp;rdquo; for Git is working as a solo developer, working on different machines (home and work machines) and using GitHub to keep my work in place.&lt;/p&gt;
&lt;p&gt;Following is the most straightforward way to start use Git and Github in this no-brainer manner. During development, only run steps indicated by * (7 - code, 9 - stage, 10 - commit, 12 - push).&lt;/p&gt;
&lt;h3 id=&#34;initial-set-up&#34;&gt;Initial set-up&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-Installing-Git&#34;&gt;Install Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/signing-up-for-a-new-github-account&#34;&gt;Create GitHub account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-new-repository&#34;&gt;Create GitHub repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-set-up-credentials-useremail&#34;&gt;1. Set up credentials, user.email:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git config --global user.email &amp;quot;&amp;lt;your github email&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;1a-set-up-credentials-useremail-specific-to-a-git-repo-only&#34;&gt;1a. Set up credentials, user.email, specific to a git repo only:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git config user.email &amp;quot;&amp;lt;your github email&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2-set-up-credentials-username&#34;&gt;2. Set up credentials, user.name:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git config --global user.name &amp;quot;&amp;lt;your github username&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2a-set-up-credentials-username-specific-to-a-git-repo-only&#34;&gt;2a. Set up credentials, user.name, specific to a git repo only:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git config user.name &amp;quot;&amp;lt;your github username&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2b-show-all-config&#34;&gt;2b. Show all config:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git config --list
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3-create-the-remote-repo-online-on-github&#34;&gt;3. Create the remote repo online on Github&lt;/h3&gt;
&lt;h3 id=&#34;4-create-a-folder-with-the-same-name-as-the-remote-name-locally&#34;&gt;4. Create a folder with the same name as the remote name locally&lt;/h3&gt;
&lt;h3 id=&#34;5-initialize-an-empty-repo-navigate-to-the-folder-in-command-line-then-run&#34;&gt;5. Initialize an empty repo: navigate to the folder in command line, then run:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git init
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;6-pull-the-remote&#34;&gt;6. Pull the remote:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git pull &amp;lt;url of remote (.git)&amp;gt; &amp;lt;branch name, e.g. master&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;7-go-ahead-and-code&#34;&gt;*7. Go ahead and code&lt;/h3&gt;
&lt;h3 id=&#34;8-add-remote-repo&#34;&gt;8. Add remote repo:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git remote add &amp;lt;name of repo&amp;gt; &amp;lt;url of repo (.git)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;9-stage-all-changes&#34;&gt;*9. Stage all changes:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git add --all
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;10-commit-all-staged-changes&#34;&gt;*10. Commit all staged changes:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git commit -m &amp;quot;&amp;lt;commit message&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;11-set-remote-as-upstream&#34;&gt;11. Set remote as upstream:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git push --set-upstream &amp;lt;repo name&amp;gt; &amp;lt;branch name, e.g. master&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;12-push-to-remote&#34;&gt;*12. Push to remote:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git push &amp;lt;repo name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>[short] Travelling and productivity</title>
      <link>/post/2017/03/05/travelling-and-productivity/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/03/05/travelling-and-productivity/</guid>
      <description>&lt;p&gt;Two things I want to talk about here: how travelling affects productivity and how to alleviate some of its effects.&lt;/p&gt;
&lt;p&gt;Since October 2016, I have been travelling in out of the Philippines for a data science project. Between October 2016 to February 2017, I made 11 trips to the Philippines, each trip spanning on average about 1 work week.&lt;/p&gt;
&lt;p&gt;At first it seems fun; which young professional wouldn’t want to experience a work life of constant flying, staying in 4 or 5 stars hotels while working on interesting projects? I would be lying if I said these didn’t excite me at the start.&lt;/p&gt;
&lt;p&gt;But, there is one major downside to all the travelling - the loss of routine and schedule. If you are someone like me who values routine for maximal productivity, then you may not like travelling as much. I recall my most productive times when schooling and working is when I follow the same schedule, week in, week out. With travelling, you are almost guaranteed to lose most, if not all of this scheduling.&lt;/p&gt;
&lt;p&gt;Of course on the plus side, you get uninterrupted time on the flight, lest for mealtime and the “please put your seat back up upright sir” from the air stewardess. I usually take this time to read or do some writing or thinking. It does helps, actually, that you have no internet connection.&lt;/p&gt;
&lt;p&gt;So, to help mitigate the lowered productivity due to travelling, these are some of the things I do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Have a business trip packing list. This reduces my mental burden when packing. By referring to the same list whenever I pack, over time I know this list is reliable and has everything I need - which means I can pack mindlessly from then on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure that I have an aisle seat. As a tall person, I need to have an aisle seat, otherwise it will be an extremely extremely miserable flight. No way I can do any work if I’m not even comfortable. To ensure that I have an aisle seat, I constantly check seats availability online. You can do so for Singapore Airlines even before check-in is open.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check-in for my flight. This is so that when I get to the airport, I can go straight to the &amp;ldquo;internet check-in” queue which is a lot faster. This is especially important for MNL-SIN flights because there aren’t any self check-in machines at the Manila airport, and the SIA queues are usually long.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the airport early. Yes early, not &amp;ldquo;just nice”. This is best for productivity because I can work in both Changi Airport and Ninoy Aquino Airport. More importantly, reaching early means I can break my working time up in larger chunks. Compare this to reaching the airport just 1 or 2 hours prior - you get inside the transit area, grab a bite and try to settle down to work. By the time you unpack your laptop and notebook and truly settle down, its probably time to go to the gate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bring a book and a notebook in my carry-on. Needless to say why.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Get to the hotel as quickly as possible after touch down and check-in. If I’m taking the 7pm SIN-MNL flight, I will go to bed immediately so that I can still get up early the following day. If I’m taking an earlier flight, then I can take my time, check-in and spend the rest of the day working on personal stuff. I usually don’t head to the office on the same day of my flight unless there’s an important meeting.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My learnings on Apache Spark</title>
      <link>/post/2017/02/14/my-learnings-on-apache-spark/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/14/my-learnings-on-apache-spark/</guid>
      <description>&lt;h2 id=&#34;one-simple-way-to-optimise-spark-jobs-on-yarn&#34;&gt;One simple way to optimise Spark jobs on YARN&lt;/h2&gt;
&lt;p&gt;When submitting Spark jobs to YARN on the CLI, we would use a submission script that typically looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark-submit \
--master yarn-cluster \
--driver-memory 20G \
--driver-cores 10 \
--executor-cores 10 \
--executor-memory 20G \
--num-executors 10 \
--total-executor-cores 100\
script_to_submit.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These are options that allows the user to specify the amount of resources to allocate to the submitted job. Not every option is always available - it depends on the type of cluster manager. There are currently three types available to Spark: standalone, Mesos, and YARN.&lt;/p&gt;
&lt;p&gt;Simply put, the standalone cluster manager comes with the Spark distribution, while Mesos and YARN are clusters managers designed to be compatible to Spark, with YARN coming together with Hadoop distributions.&lt;/p&gt;
&lt;p&gt;In brief, the available options for each cluster manager are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standalone - driver-cores, executor-cores, total-executor-cores&lt;/li&gt;
&lt;li&gt;Mesos - total-executor-cores&lt;/li&gt;
&lt;li&gt;YARN - driver-cores, executor-cores, num-executors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The rest, namely driver-memory and executor-memory are available to all three.&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t had any experience with the standalone manager as well as Mesos, so I will just talk about YARN. On the YARN web UI, under &amp;ldquo;Cluster Metrics&amp;rdquo;, there are two entries that read &amp;ldquo;Memory Total&amp;rdquo; and &amp;ldquo;VCores Total&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To optimise the amount of resources allocated to your job:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Memory Total&amp;rdquo; should be roughly and less than num-executors x executormemory&lt;/li&gt;
&lt;li&gt;&amp;ldquo;VCores Total&amp;rdquo; should be roughly and less than num-executors x executor-cores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Seems intuitive but I didn&amp;rsquo;t fully put this optimisation in my conscious mind until one of our engineers explicitly enlighten me of this.&lt;/p&gt;
&lt;h2 id=&#34;other-learnings-on-spark&#34;&gt;Other learnings on Spark&lt;/h2&gt;
&lt;p&gt;This is PySpark.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Writing text files to HDFS using sc.saveAsTextFile() - use high driver memory. RDD has to fit in the driver memory when writing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use rdd.coalesce(n) to save to n text files. On the YARN UI, each file will be represented as a task.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the saveAsTextFile() stage keeps stopping at the last task, check the data. There is most probably something wrong with the data in the program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is a difference between using 50GB RAM times 10 executors versus 20GB times 30 executors. The memory used reflected on the YARN UI differs greatly - for my case, the former gives 550GB while the latter, 220GB. I&amp;rsquo;m guessing it&amp;rsquo;s best to match the number of executors to the number of datanodes in the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Whenever a task or a stage cannot succeed, check the data within the program - columns, counts, datatypes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A conventional way to debug code or scripts is always to print statements or data onto the console or terminal. Note that this debugging technique cannot work for some spark Spark apps, because of Spark&amp;rsquo;s lazy evaluation. Methods in Spark can be classified as either actions or transformations. Unlike actions, transformation methods are parsed and interpreted by Spark, without any actual work done on the data structures; only when actions are called will work be done. Therefore interjecting your code with print statements doesn&amp;rsquo;t help too much.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;UDFs can run without checking for errors in the data within the program. Suspect that UDFs are transformations and not actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In YARN, container RAM is the RAM of 1 datanode. When setting the RAM for each container, leave about 5GB for overheads and OS functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;df.printSchema() can work even without reading any data into the program - even lazier than transformations if I&amp;rsquo;m not wrong.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Actions / transformations of one RDD cannot be performed inside the actions / transformations of another RDD, as all actions and transformations of the former RDD will require the spawning of new workers and jobs, within the current workers and jobs on the latter RDD, which is not supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The UDF function turns a regular Python function to a function that is applied on all elements of the input column. This function cannot any Spark functions, as calling any Spark functions may require the needs to spawn new workers and jobs. (10) is a generalisation of this.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;book-getting-started-with-apache-spark-from-inception-to-production&#34;&gt;Book: Getting Started with Apache Spark: From Inception to Production&lt;/h2&gt;
&lt;p&gt;This book, published by MapR, serves as an introduction to Apache Spark. It&amp;rsquo;s a free book I got from the Strata Hadoop 2016 conference in Singapore. A relatively short and lightweight intro to Spark, this is a good read for anyone who wants to learn a little more about Spark. Topics include installation, architecture overview, Hadoop and Spark, data streaming, and machine learning using MLlib.&lt;/p&gt;
&lt;p&gt;Pdf version available here: &lt;a href=&#34;http://www.bigdatatoronto.com/2016/assets/getting_started_with_apache_spark.pdf&#34;&gt;http://www.bigdatatoronto.com/2016/assets/getting_started_with_apache_spark.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Scientific desktop research: tentative conclusions</title>
      <link>/post/2017/02/13/scientific-desktop-research-tentative-conclusions/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/13/scientific-desktop-research-tentative-conclusions/</guid>
      <description>&lt;p&gt;Useful tip I got from &amp;ldquo;Extreme Productivity&amp;rdquo; by Robert Pozen, still reading.&lt;/p&gt;
&lt;p&gt;Desktop research, or simply known as googling in today&amp;rsquo;s context, is the act of collecting information about a particular topic or domain, with a broad/generic question in mind, or even without any question at all.&lt;/p&gt;
&lt;p&gt;One way to do this is simply keep googling related search terms and noting down facts and statistics, and constantly drilling down on a specific term until it&amp;rsquo;s milked - sort of like a depth-first search of collecting information. After we are satisfied with all the information we collect, we then try to piece together a story. But is this a good approach?&lt;/p&gt;
&lt;p&gt;Pozen argues that &amp;ldquo;although extensive research might seem a logical first step, it&amp;rsquo;s actually very inefficient.&amp;rdquo; This is because &amp;ldquo;there are literally thousands of facts that could be relevant to any project; do you really want to collect them all?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Instead, Pozen suggests that we first form &amp;ldquo;tentative conclusions&amp;rdquo;, by first thinking hard about our problem. &amp;ldquo;After a day or so of gathering relevant information, write down your tentative conclusions for the project. These will allow you to more quickly engage in analysis - rather than description - by providing a focus for your subsequent research.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;If you think about it, this makes a lot of sense and sounds more productive. And it&amp;rsquo;s a pretty scientific way of doing things - scientists don&amp;rsquo;t endeavour to do all the experiments that they can possibly imagine and then collecting all the data. Instead, they formulate hypotheses and design experiments to refute their claims. If their claims are not refuted, or are even proven, then they become published and reliable facts, for now.&lt;/p&gt;
&lt;p&gt;And this applies to more than just desktop research:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Think of how you first learn a programming language outside of a school setting. Odds are you &amp;ldquo;hacked&amp;rdquo; your way through to your current level of proficiency, by relying on Google and Stackoverflow. You were probably required to write some production or project code, had an idea, didn&amp;rsquo;t work out, googled, and try again and again. Not going through things like &amp;ldquo;Basic data structures in Python&amp;rdquo; and work from thereon. I am definitely guilty of this - when I first started learning about Spark and writing PySpark code, I tried multiple times to start from some tutorial and slowly work my way up toward proficiency. Each time, I failed either because the material gets boring, or I lose patience, or something like that. It just doesn&amp;rsquo;t work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Which is the more efficient of doing exploratory data analysis - plotting every single plot and collecting every single summary statistics there is to your data, or coming up with several hypotheses and then visualising and testing them? &amp;ldquo;My guess is that age is not normally distributed in this dataset, let&amp;rsquo;s see.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[short] Types of tasks</title>
      <link>/post/2017/02/12/types-of-tasks/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/12/types-of-tasks/</guid>
      <description>&lt;p&gt;In my work so far, there are typically a few type of tasks:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;I think they can divided into four quadrants:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;There&amp;rsquo;s a lot of unpack here, so one by one:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visible results: a deck, an email, a document, a discussion with potential customers - tangible stuff&lt;/li&gt;
&lt;li&gt;Value: high value means generating a good idea, building strong relationships with customers or fellow colleagues in the same or different team, getting a clearer idea of something that allows you to move forward from current status quo&lt;/li&gt;
&lt;li&gt;A good day at work would be to spend most if not all of your time in Quadrant 1, generating both value and results&lt;/li&gt;
&lt;li&gt;Quadrant 2 gives high value because these are the types of tasks that spark the team and yourself off towards a good week or month. These tasks generally give ideas and directions, and hence high value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a morning person, my energy dwindle towards the end of the day, especially with a trough after lunch. Within this framework there must be a way to optimise my productivity in a work day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Taking a part-time Masters this year in 2017</title>
      <link>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</link>
      <pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</guid>
      <description>&lt;p&gt;I am leaning towards taking a part-time Masters this year in 2017. Points of considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Masters or PhD?&lt;/li&gt;
&lt;li&gt;Full-time or part-time?&lt;/li&gt;
&lt;li&gt;(If masters) technical or non-technical?&lt;/li&gt;
&lt;li&gt;(If full-time) Overseas or local?&lt;/li&gt;
&lt;li&gt;(If local) NUS or NTU?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full-time is personally a non-option for me, as I don&amp;rsquo;t see the financial sense in taking a sabbatical to pursue a full-time programme. That leaves full-time and overseas out.&lt;/p&gt;
&lt;p&gt;Based on what I know about a PhD programme, part-time PhD sounds like a nightmare. That leaves part-time Masters in Singapore as my option.&lt;/p&gt;
&lt;p&gt;Next question: technical or non-technical? Well I am leaning towards to doing something with technical content when studying - non-technical content can be picked up most of the time simply by being widely read and learning from work experiences. This means statistics or computing for me.&lt;/p&gt;
&lt;p&gt;And NUS is probably the better choice than NTU. SMU is not in my consideration.&lt;/p&gt;
&lt;p&gt;So for now, my choice is going to be M.Sc. Statistics from NUS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three Needs Theory - Power, Achievement, Affliation</title>
      <link>/post/2017/01/02/three-needs-theory-power-achievement-affliation/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/01/02/three-needs-theory-power-achievement-affliation/</guid>
      <description>&lt;p&gt;The Three Needs Theory was developed by psychologist David McClelland in the 1960s. The theory identifies three main needs in the professional setting, which form the drivers of motivation. These needs are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The need for power (n-Pow)&lt;/li&gt;
&lt;li&gt;The need for achievement (n-Ach)&lt;/li&gt;
&lt;li&gt;The need for affiliation (n-Aff)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I first encountered this idea in the book Managing Scientists: Leadership Strategies in Scientific Research by Alice Sapienza, which I have yet to finish reading. Simply put, every individual identifies with one, two (usually one major, one minor), or even three (rare) of these needs. Fulfilling these needs act as motivators, which in turn determine one&amp;rsquo;s course of action in a given set of circumstances and options.&lt;/p&gt;
&lt;p&gt;In general, individuals who identify significantly with a particular need tend towards certain types of behaviour, which may serve as tell-tale signs from the perspective of others. However, this doesn&amp;rsquo;t mean these behaviours are indeed exhibited in reality, due to constraints such as rank or resources.&lt;/p&gt;
&lt;p&gt;Folks with high n-Pow tend to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Control other people for personal or greater good (need for personal power or need for institutional power; personal power or socialised power)&lt;/li&gt;
&lt;li&gt;Seek neither recognition nor approval, only agreement and compliance&lt;/li&gt;
&lt;li&gt;Be argumentative&lt;/li&gt;
&lt;li&gt;Be assertive&lt;/li&gt;
&lt;li&gt;Practise discipline&lt;/li&gt;
&lt;li&gt;Be concerned about reputation&lt;/li&gt;
&lt;li&gt;Arouse emotions in others to push towards a cause&lt;/li&gt;
&lt;li&gt;There is also a distinction between the need for personal power versus institutional power.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Needless to say, the need for personal power is not exactly the healthiest need in a professional setting.&lt;/p&gt;
&lt;p&gt;On the other hand, folks with high n-Ach tend to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enjoy practising and honing their skillsets&lt;/li&gt;
&lt;li&gt;Enjoy winning&lt;/li&gt;
&lt;li&gt;Seek improvements&lt;/li&gt;
&lt;li&gt;Seek to do things more efficiently&lt;/li&gt;
&lt;li&gt;Appeal more to the intrinsic value of a task (technical difficulties or complexities) than the &amp;ldquo;extrinsic value&amp;rdquo; of the task towards a greater goal (company P&amp;amp;L, personal or company reputation etc.)&lt;/li&gt;
&lt;li&gt;Enjoy tasks where effort is approximately proportionate to results&lt;/li&gt;
&lt;li&gt;Prefer tasks that are neither low risk (not challenging) nor high risk (too much left to chance but not effort)&lt;/li&gt;
&lt;li&gt;Enjoy constructive feedback, both positive and negative&lt;/li&gt;
&lt;li&gt;Set self-imposed goals&lt;/li&gt;
&lt;li&gt;Be inventive or creative&lt;/li&gt;
&lt;li&gt;Enjoy working alone&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lastly, folks with high n-Aff tend to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value interpersonal relationships&lt;/li&gt;
&lt;li&gt;Conform to norms in the workplace&lt;/li&gt;
&lt;li&gt;Prefer collaboration over competition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Naturally, an individual&amp;rsquo;s dominant need(s) will determine the type of role in which he/she will excel in. For example, entrepreneurs tend to be of high n-Ach. The best senior management leaders tend to be of high n-Pow (institutional power) and high n-Aff. The best middle management leaders, on the other hand, tend to be high n-Ach and a bit less of n-Aff. (Can you figure out why? This is because in a typical corporate setting, middle managers are promoted into their positions due to the competencies they have shown in the working level, displaying high n- Ach. However, to rise further up into senior management, these without high n-Pow will be eliminated eventually.) The best salesperson will be of high-Ach and high n-Aff, while a high n-Aff works well for project managers.&lt;/p&gt;
&lt;p&gt;Personally, I find this theory to be a useful mental model - look into your workplace, and consider both your colleagues (peers and reportees), and your immediate boss. Can you confidently identify their motivational needs, be it power, achievement, or affiliation? It&amp;rsquo;s not hard to do so, isn&amp;rsquo;t it&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s probably worthwhile to learn more about these needs, how they manifest in the workplace and the role they play in motivation, productivity, and workplace relationships. I will definitely write another piece on this. Even if you are not interested in how these needs play out amongst others in your workplace, you would still be interested in identifying your personal needs (though if you think this way, you probably belong to the low n-Pow, high n-Ach, and low n-Aff camp).&lt;/p&gt;
&lt;p&gt;For now, here&amp;rsquo;s a rather insightful HBR article discussing the three needs and the role they play in leadership: &lt;a href=&#34;https://hbr.org/2003/01/power-is-the-great-motivator&#34;&gt;https://hbr.org/2003/01/power-is-the-great-motivator&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Book Review: The McKinsey Edge</title>
      <link>/post/2016/12/31/book-review-the-mckinsey-edge/</link>
      <pubDate>Sat, 31 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/12/31/book-review-the-mckinsey-edge/</guid>
      <description>&lt;p&gt;The McKinsey Edge by Shu Hattori is a collection of &amp;ldquo;principles&amp;rdquo; that the author collected while he was at
consultant at McKinsey. The forty-odd principles in the book, while easily understood, stood out
when they are collated together in a single book, as many productivity and professional hacks
strung together. A rather interesting read.&lt;/p&gt;
&lt;p&gt;Following are his principles divided into four chapters, and in bold are those that I thought to be
particularly useful, and would like to expound on them in future writings.&lt;/p&gt;
&lt;p&gt;Building the Better Self&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Focus on what really matters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Start with the hard stuff in the morning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Catch small signals and make a difference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a 30-second answer to everything&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frontload your project&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create the right end output image&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smile when you are under stress&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go beyond your self-perceived limit&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Always imagine the worst-case scenario&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Start following up&lt;/li&gt;
&lt;li&gt;Push back with less emotion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be flexible on the perception of your passion&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What would Marvin do?&amp;rdquo; Find your role models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Know what gives your the most energy in your day&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go jogging to smell the flowers&lt;/li&gt;
&lt;li&gt;Create a commitment plan&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Growing with Others&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always memorize the first three sentences of a presentation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Communicate using fewer words&lt;/li&gt;
&lt;li&gt;Pause three seconds before answering difficult questions&lt;/li&gt;
&lt;li&gt;Question more and talk less&lt;/li&gt;
&lt;li&gt;Turn no into yes&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t show half-baked output&lt;/li&gt;
&lt;li&gt;Instantly find a connection in the room&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be a giver, not a receiver&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Find the best intent in people&lt;/li&gt;
&lt;li&gt;Learn team member&amp;rsquo;s defining moments and personal sides&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Think of everyone as a helpful individual, not a &amp;ldquo;resource&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go out for a meal with interesting people every week&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consciously gauge your people&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign team members meaningful tasks&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create followership through deliberate on-the-job coaching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Deliver feedback using positive criticism&lt;/li&gt;
&lt;li&gt;Please your assistants and support staff&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Execlling in Process Management&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always prepare an agenda before meetings&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create &amp;ldquo;four boxes&amp;rdquo; to dos&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus on outcomes not activities&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Know your meeting modes in advance&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proactively manage e-mail communication using the 5D rules&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speak up as early as possible&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create a minimalist presentation toolkit&lt;/li&gt;
&lt;li&gt;Create an easy-to-use template for updates&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Going the Extra mile&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Give away knowledge and tools unsparingly&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Get rid of your physical barriers&lt;/li&gt;
&lt;li&gt;Ask the second order questions&lt;/li&gt;
&lt;li&gt;Learn to write fewer notes&lt;/li&gt;
&lt;li&gt;Prepare to renew your life&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create your own profile as a leader&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Random Forests in R</title>
      <link>/post/2016/07/20/random-forests-in-r/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/07/20/random-forests-in-r/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As the name suggests, random forest models basically contain an ensemble of decision tree models, with each decision tree predicting the same response variable. The response may be categorical, in which case being a classification problem, or continuous / numerical, being a regression problem.&lt;/p&gt;
&lt;p&gt;In this short tutorial, we will go through the use of tree-based methods (decision tree, bagging model, and random forest) for both classification and regression problems. Each section of this tutorial corresponds to an individual R script that can be found in the GitHub repo at &lt;a href=&#34;https://github.com/thestatsguy/RUGS-RF&#34; class=&#34;uri&#34;&gt;https://github.com/thestatsguy/RUGS-RF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This tutorial is divided into two sections. We will first use tree-based methods for classification on the &lt;strong&gt;spam&lt;/strong&gt; dataset from the &lt;strong&gt;kernlab&lt;/strong&gt; package - the same dataset used in the previous RUGS workshop on support vector machines (SVM). Subsequently, we will apply these methods on a regression problem, with the &lt;strong&gt;imports85&lt;/strong&gt; dataset from the &lt;strong&gt;randomForest&lt;/strong&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tree-based-methods-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tree-based methods for classification&lt;/h2&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Let’s start by loading the spam dataset and doing some preparations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# packages that we will need:
#  @ kernlab:      for the spam dataset
#  @ tree:         for decision tree construction
#  @ randomForest: for bagging and RF
#  @ beepr:        for a little beep
#  @ pROC:         for plotting of ROC

# code snippet to install and load multiple packages at once
# pkgs &amp;lt;- c(&amp;quot;kernlab&amp;quot;,&amp;quot;tree&amp;quot;,&amp;quot;randomForest&amp;quot;,&amp;quot;beepr&amp;quot;,&amp;quot;pROC&amp;quot;)
# sapply(pkgs,FUN=function(p){
#        print(p)
#        if(!require(p)) install.packages(p)
#        require(p)
# })

# load required packages
suppressWarnings(library(kernlab))
suppressWarnings(library(tree))
suppressWarnings(library(randomForest))
## randomForest 4.6-14
## Type rfNews() to see new features/changes/bug fixes.
suppressWarnings(library(beepr)) # try it! beep()
suppressWarnings(library(pROC))
## Type &amp;#39;citation(&amp;quot;pROC&amp;quot;)&amp;#39; for a citation.
## 
## Attaching package: &amp;#39;pROC&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     cov, smooth, var

# load dataset
data(spam)

# take a look
str(spam)
## &amp;#39;data.frame&amp;#39;:    4601 obs. of  58 variables:
##  $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
##  $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...
##  $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
##  $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ our              : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
##  $ over             : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...
##  $ remove           : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...
##  $ internet         : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...
##  $ order            : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...
##  $ mail             : num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...
##  $ receive          : num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...
##  $ will             : num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...
##  $ people           : num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...
##  $ report           : num  0 0.21 0 0 0 0 0 0 0 0 ...
##  $ addresses        : num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...
##  $ free             : num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...
##  $ business         : num  0 0.07 0.06 0 0 0 0 0 0 0 ...
##  $ email            : num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...
##  $ you              : num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...
##  $ credit           : num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...
##  $ your             : num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...
##  $ font             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num000           : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...
##  $ money            : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...
##  $ hp               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hpl              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ george           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num650           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ lab              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ labs             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ telnet           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num857           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ data             : num  0 0 0 0 0 0 0 0 0.15 0 ...
##  $ num415           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num85            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ technology       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num1999          : num  0 0.07 0 0 0 0 0 0 0 0 ...
##  $ parts            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ pm               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ direct           : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ cs               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ meeting          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ original         : num  0 0 0.12 0 0 0 0 0 0.3 0 ...
##  $ project          : num  0 0 0 0 0 0 0 0 0 0.06 ...
##  $ re               : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ edu              : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ table            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ conference       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charSemicolon    : num  0 0 0.01 0 0 0 0 0 0 0.04 ...
##  $ charRoundbracket : num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...
##  $ charSquarebracket: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charExclamation  : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
##  $ charDollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
##  $ charHash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...
##  $ capitalAve       : num  3.76 5.11 9.82 3.54 3.54 ...
##  $ capitalLong      : num  61 101 485 40 40 15 4 11 445 43 ...
##  $ capitalTotal     : num  278 1028 2259 191 191 ...
##  $ type             : Factor w/ 2 levels &amp;quot;nonspam&amp;quot;,&amp;quot;spam&amp;quot;: 2 2 2 2 2 2 2 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we will attempt to predict whether an email is spam or nonspam. To do so, we will construct models on one subset of the data (training data), and use the constructed model on another disparate subset of the data (the testing data). This is known as cross validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preparation for cross validation:
# split the dataset into 2 halves,
# 2300 samples for training and 2301 for testing
num.samples &amp;lt;- nrow(spam) # 4,601
num.train   &amp;lt;- round(num.samples/2) # 2,300
num.test    &amp;lt;- num.samples - num.train # 2,301
num.var     &amp;lt;- ncol(spam) # 58

# set up the indices
set.seed(150715)
idx       &amp;lt;- sample(1:num.samples)
train.idx &amp;lt;- idx[seq(num.train)]
test.idx  &amp;lt;- setdiff(idx,train.idx)

# subset the data
spam.train &amp;lt;- spam[train.idx,]
spam.test  &amp;lt;- spam[test.idx,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a quick glance at the &lt;strong&gt;type&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(spam.train$type)
## 
## nonspam    spam 
##    1397     903
table(spam.test$type)
## 
## nonspam    spam 
##    1391     910&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-tree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision tree&lt;/h3&gt;
&lt;p&gt;Now that we are done with the preparation, let’s start by constructing a decision tree model, using the &lt;strong&gt;tree&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree.mod &amp;lt;- tree(type ~ ., data = spam.train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how our model looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(tree.mod)
title(&amp;quot;Decision tree&amp;quot;)
text(tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model may be overtly complicated. Typically, after constructing a decision tree model, we may want to prune the model, by collapsing certain edges, nodes and leaves together without much loss of performance. This is done by iteratively comparing the number of leaf nodes with the model’s performance (by k-fold cross validation &lt;em&gt;within the training set&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.prune &amp;lt;- cv.tree(tree.mod, FUN = prune.misclass)
plot(cv.prune$size, cv.prune$dev, pch = 20, col = &amp;quot;red&amp;quot;, type = &amp;quot;b&amp;quot;,
     main = &amp;quot;Decision tree: Cross validation to find optimal size of tree&amp;quot;,
     xlab = &amp;quot;Size of tree&amp;quot;, ylab = &amp;quot;Misclassifications&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Having 9 leaf nodes may be good (maximising performance while minimising complexity).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best.tree.size &amp;lt;- 9

# pruning (cost-complexity pruning)
pruned.tree.mod &amp;lt;- prune.misclass(tree.mod, best = best.tree.size)

# here&amp;#39;s the new tree model
plot(pruned.tree.mod)
title(paste(&amp;quot;Pruned decision tree (&amp;quot;, best.tree.size, &amp;quot; leaf nodes)&amp;quot;,sep = &amp;quot;&amp;quot;))
text(pruned.tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with our new model, let’s make some predictions on the testing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree.pred &amp;lt;- predict(pruned.tree.mod,
                     subset(spam.test, select = -type), 
                     type = &amp;quot;class&amp;quot;)

# confusion matrix
# rows are the predicted classes
# columns are the actual classes
print(tree.pred.results &amp;lt;- table(tree.pred, spam.test$type))
##          
## tree.pred nonspam spam
##   nonspam    1308  164
##   spam         83  746

# What is the accuracy of our tree model?
print(tree.accuracy &amp;lt;- (tree.pred.results[1,1] + tree.pred.results[2,2]) / sum(tree.pred.results))
## [1] 0.8926554&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our decision tree model is able to predict spam vs. nonspam emails with about 89.27% accuracy. We will make comparisons of accuracies with other models later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Next, we turn our attention to the bagging model. Recall that bagging, a.k.a. &lt;em&gt;bootstrap aggregating&lt;/em&gt;, is the process of sampling (with replacement), samples from the training data. Each of these subsets are known as bags, and we construct individual decision tree models using each of these bags. Finally, to make a classification prediction, we use the majority vote from the ensemble of decision tree models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.mod&amp;lt;-randomForest(type ~ ., data = spam.train,
                     mtry = num.var - 1, # try all variables at each split, except the response variable
                     ntree = 300,
                     proximity = TRUE,
                     importance = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the bagging, and also the random forest model, there are often only two hyperparameters that we are interested in: &lt;strong&gt;mtry&lt;/strong&gt;, which is the number of variables to try from for each tree and at each split, and &lt;strong&gt;ntree&lt;/strong&gt;, the number of trees in the ensemble. Tuning the number of trees is relatively easy by looking at the out-of-bag (OOB) error estimate of the ensemble at each step of the way. For more details, refer to the slides. We set &lt;strong&gt;proximity = TRUE&lt;/strong&gt; and &lt;strong&gt;importance = TRUE&lt;/strong&gt;, in order to get some form of visualization of the model, and the variable importances respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(bg.mod$err.rate[,1], type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Bagging: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, 300 trees seems more than sufficient. One advantage of bagging and random forest models is that they provide a way of doing feature or variable selection, by considering the importance of each variable in the model. For exact details on how these importance measures are defined, refer to the slides.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(bg.mod,
           main = &amp;quot;Bagging: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In addition, we can visualize the classification done by the model using a multidimensional plot on the proximity matrix. The green samples in the figure represent nonspams, while the red samples are spams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MDSplot(bg.mod,
        fac = spam.train$type,
        palette = c(&amp;quot;green&amp;quot;,&amp;quot;red&amp;quot;),
        main = &amp;quot;Bagging: MDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let’s make some predictions on the testing data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.pred &amp;lt;- predict(bg.mod,
                   subset(spam.test, select = -type), 
                   type = &amp;quot;class&amp;quot;)

# confusion matrix
# rows are the predicted classes
# columns are the actual classes
print(bg.pred.results &amp;lt;- table(bg.pred, spam.test$type))
##          
## bg.pred   nonspam spam
##   nonspam    1336   87
##   spam         55  823

# what is the accuracy of our bagging model?
print(bg.accuracy &amp;lt;- sum(diag((bg.pred.results))) / sum(bg.pred.results))
## [1] 0.9382877&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our bagging model predicts whether an email is spam or not with about 93.83% accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;The only difference between the bagging model and random forest model is that the latter uses chooses only from a subset of variables to split on at each node of each tree. In other words, only the &lt;strong&gt;mtry&lt;/strong&gt; argument differs between bagging and random forest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.mod &amp;lt;- randomForest(type ~ ., data = spam.train,
                       mtry = floor(sqrt(num.var - 1)), # 7; only difference from bagging is here
                       ntree = 300,
                       proximity = TRUE,
                       importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees:
plot(rf.mod$err.rate[,1], type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Random forest: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Besides tuning the &lt;strong&gt;ntree&lt;/strong&gt; hyperparameter, we might also be interested in tuning the &lt;strong&gt;mtry&lt;/strong&gt; hyperparameter in random forest. The random forest model may be built using the &lt;strong&gt;mtry&lt;/strong&gt; value that minimises the OOB error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuneRF(subset(spam.train, select = -type),
       spam.train$type,
       ntreeTry = 100)
## mtry = 7  OOB error = 5.52% 
## Searching left ...
## mtry = 4     OOB error = 6.26% 
## -0.1338583 0.05 
## Searching right ...
## mtry = 14    OOB error = 5.83% 
## -0.05511811 0.05
##        mtry   OOBError
## 4.OOB     4 0.06260870
## 7.OOB     7 0.05521739
## 14.OOB   14 0.05826087
title(&amp;quot;Random forest: Tuning the mtry hyperparameter&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# variable importance
varImpPlot(rf.mod,
           main = &amp;quot;Random forest: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# multidimensional scaling plot
# green samples are non-spam,
# red samples are spam
MDSplot(rf.mod,
        fac = spam.train$type,
        palette = c(&amp;quot;green&amp;quot;,&amp;quot;red&amp;quot;),
        main = &amp;quot;Random forest: MDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# now, let&amp;#39;s make some predictions
rf.pred &amp;lt;- predict(rf.mod,
                   subset(spam.test,select = -type), 
                   type=&amp;quot;class&amp;quot;)

# confusion matrix
print(rf.pred.results &amp;lt;- table(rf.pred, spam.test$type))
##          
## rf.pred   nonspam spam
##   nonspam    1353   82
##   spam         38  828

# Accuracy of our RF model:
print(rf.accuracy &amp;lt;- sum(diag((rf.pred.results))) / sum(rf.pred.results))
## [1] 0.9478488&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our random forest model predicts whether an email is spam or not with about 94.78% accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-performances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization of performances&lt;/h3&gt;
&lt;p&gt;Let’s go ahead and make some comparisons on the performances of our model. For comparison sake, let’s also construct a logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log.mod &amp;lt;- glm(type ~ . , data = spam.train,
             family = binomial(link = logit))
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

# predictions
log.pred.prob &amp;lt;- predict(log.mod,
                         subset(spam.test, select = -type), 
                         type = &amp;quot;response&amp;quot;)
log.pred.class &amp;lt;- factor(sapply(log.pred.prob,
                                FUN = function(x){
                                        if(x &amp;gt;= 0.5) return(&amp;quot;spam&amp;quot;)
                                        else return(&amp;quot;nonspam&amp;quot;)
                                }))

# confusion matrix
log.pred.results &amp;lt;- table(log.pred.class, spam.test$type)

# Accuracy of logistic regression model:
print(log.accuracy &amp;lt;- sum(diag((log.pred.results))) / sum(log.pred.results))
## [1] 0.9135159&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s compare the performances, considering first the model accuracies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(c(tree.accuracy,
          bg.accuracy,
          rf.accuracy,
          log.accuracy),
        main=&amp;quot;Accuracies of various models&amp;quot;,
        names.arg=c(&amp;quot;Tree&amp;quot;,&amp;quot;Bagging&amp;quot;,&amp;quot;RF&amp;quot;, &amp;quot;Logistic&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_viz1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that the ensemble models (bagging and random forest) outperforms the single decision tree, and also the logistic regression model. It turns out here that the bagging and the random forest models have about the same classification performance. Understanding the rationale of &lt;em&gt;random subspace sampling&lt;/em&gt; (refer to slides) should allow us to appreciate the potential improvement of random forest over the bagging model.&lt;/p&gt;
&lt;p&gt;Finally, let’s plot the ROC curves of the various models. The ROC is only valid for models that give probabilistic output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.pred.prob &amp;lt;- predict(bg.mod ,
                        subset(spam.test, select = -type),
                        type = &amp;quot;prob&amp;quot;)

rf.pred.prob &amp;lt;- predict(rf.mod ,
                        subset(spam.test, select = -type),
                        type = &amp;quot;prob&amp;quot;)

plot.roc(spam.test$type,
         bg.pred.prob[,1], col = &amp;quot;blue&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.3,
         main = &amp;quot;ROC-AUC of various models&amp;quot;)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;gt; cases

plot.roc(spam.test$type,
         rf.pred.prob[,1], col = &amp;quot;green&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.2,
         add = TRUE)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;gt; cases

plot.roc(spam.test$type,
         log.pred.prob, col = &amp;quot;red&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.1,
         add = TRUE)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;lt; cases

legend(x = 0.6, y = 0.8, legend = c(&amp;quot;Bagging&amp;quot;,
                                    &amp;quot;Random forest&amp;quot;,
                                    &amp;quot;Logistic regression&amp;quot;),
       col = c(&amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;), lwd = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_viz2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tree-based-methods-for-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tree-based methods for regression&lt;/h2&gt;
&lt;p&gt;In the following section, we will consider the use of tree-based methods for regression. The materials that follows are analogous to that above, if not the similar.&lt;/p&gt;
&lt;div id=&#34;preparation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tree)
library(randomForest)

data(imports85)
imp &amp;lt;- imports85

# The following data preprocessing steps on
# the imports85 dataset are suggested by
# the authors of the randomForest package
# look at
# &amp;gt; ?imports85
imp &amp;lt;- imp[,-2]  # Too many NAs in normalizedLosses.
imp &amp;lt;- imp[complete.cases(imp), ]
# ## Drop empty levels for factors
imp[] &amp;lt;- lapply(imp, function(x) if (is.factor(x)) x[, drop=TRUE] else x)

# Also removing the numOfCylinders and fuelSystem
# variables due to sparsity of data
# to see this, run the following lines:
# &amp;gt; table(imp$numOfCylinders)
# &amp;gt; table(imp$fuelSystem)
# This additional step is only necessary because we will be
# making comparisons between the tree-based models
# and linear regression, and linear regression cannot
# handle sparse data well
imp &amp;lt;- subset(imp, select = -c(numOfCylinders,fuelSystem))

# also removing the make variable
imp &amp;lt;- subset(imp, select = -make)

# Preparation for cross validation:
# split the dataset into 2 halves,
# 96 samples for training and 97 for testing
num.samples &amp;lt;- nrow(imp) # 193
num.train   &amp;lt;- round(num.samples / 2) # 96
num.test    &amp;lt;- num.samples - num.train # 97
num.var     &amp;lt;- ncol(imp) # 25

# set up the indices
set.seed(150715)
idx       &amp;lt;- sample(1:num.samples)
train.idx &amp;lt;- idx[seq(num.train)]
test.idx  &amp;lt;- setdiff(idx,train.idx)

# subset the data
imp.train &amp;lt;- imp[train.idx,]
imp.test  &amp;lt;- imp[test.idx,]

str(imp.train)
## &amp;#39;data.frame&amp;#39;:    96 obs. of  22 variables:
##  $ symboling       : int  1 0 0 3 2 1 1 1 3 2 ...
##  $ fuelType        : Factor w/ 2 levels &amp;quot;diesel&amp;quot;,&amp;quot;gas&amp;quot;: 2 2 2 2 1 2 2 2 2 2 ...
##  $ aspiration      : Factor w/ 2 levels &amp;quot;std&amp;quot;,&amp;quot;turbo&amp;quot;: 2 1 2 1 1 1 1 1 2 2 ...
##  $ numOfDoors      : Factor w/ 2 levels &amp;quot;four&amp;quot;,&amp;quot;two&amp;quot;: 1 1 1 2 2 2 1 2 2 1 ...
##  $ bodyStyle       : Factor w/ 5 levels &amp;quot;convertible&amp;quot;,..: 4 4 4 3 4 4 4 3 3 4 ...
##  $ driveWheels     : Factor w/ 3 levels &amp;quot;4wd&amp;quot;,&amp;quot;fwd&amp;quot;,&amp;quot;rwd&amp;quot;: 2 2 3 3 2 3 2 2 2 2 ...
##  $ engineLocation  : Factor w/ 2 levels &amp;quot;front&amp;quot;,&amp;quot;rear&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ wheelBase       : num  96.3 97.2 108 102.9 97.3 ...
##  $ length          : num  172 173 187 184 172 ...
##  $ width           : num  65.4 65.2 68.3 67.7 65.5 64 63.8 66.5 65.4 66.5 ...
##  $ height          : num  51.6 54.7 56 52 55.7 52.6 54.5 53.7 49.4 56.1 ...
##  $ curbWeight      : int  2403 2302 3130 2976 2261 2265 1971 2385 2370 2847 ...
##  $ engineType      : Factor w/ 5 levels &amp;quot;dohc&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;ohc&amp;quot;,..: 3 3 2 1 3 1 3 3 3 1 ...
##  $ engineSize      : int  110 120 134 171 97 98 97 122 110 121 ...
##  $ bore            : num  3.17 3.33 3.61 3.27 3.01 3.24 3.15 3.39 3.17 3.54 ...
##  $ stroke          : num  3.46 3.47 3.21 3.35 3.4 3.08 3.29 3.39 3.46 3.07 ...
##  $ compressionRatio: num  7.5 8.5 7 9.3 23 9.4 9.4 8.6 7.5 9 ...
##  $ horsepower      : int  116 97 142 161 52 112 69 84 116 160 ...
##  $ peakRpm         : int  5500 5200 5600 5200 4800 6600 5200 4800 5500 5500 ...
##  $ cityMpg         : int  23 27 18 20 37 26 31 26 23 19 ...
##  $ highwayMpg      : int  30 34 24 24 46 29 37 32 30 26 ...
##  $ price           : int  9279 9549 18150 16558 7775 9298 7499 10595 9959 18620 ...
str(imp.test)
## &amp;#39;data.frame&amp;#39;:    97 obs. of  22 variables:
##  $ symboling       : int  -1 1 -1 1 1 -2 0 0 2 2 ...
##  $ fuelType        : Factor w/ 2 levels &amp;quot;diesel&amp;quot;,&amp;quot;gas&amp;quot;: 2 2 1 2 2 2 1 2 2 2 ...
##  $ aspiration      : Factor w/ 2 levels &amp;quot;std&amp;quot;,&amp;quot;turbo&amp;quot;: 1 1 2 1 1 1 2 1 1 1 ...
##  $ numOfDoors      : Factor w/ 2 levels &amp;quot;four&amp;quot;,&amp;quot;two&amp;quot;: 1 1 1 2 2 1 2 2 2 2 ...
##  $ bodyStyle       : Factor w/ 5 levels &amp;quot;convertible&amp;quot;,..: 4 4 5 4 4 4 2 4 1 3 ...
##  $ driveWheels     : Factor w/ 3 levels &amp;quot;4wd&amp;quot;,&amp;quot;fwd&amp;quot;,&amp;quot;rwd&amp;quot;: 3 3 3 3 2 3 3 3 3 2 ...
##  $ engineLocation  : Factor w/ 2 levels &amp;quot;front&amp;quot;,&amp;quot;rear&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ wheelBase       : num  115.6 103.5 110 94.5 94.5 ...
##  $ length          : num  203 189 191 169 165 ...
##  $ width           : num  71.7 66.9 70.3 64 63.8 67.2 70.3 70.6 65.6 64.4 ...
##  $ height          : num  56.5 55.7 58.7 52.6 54.5 56.2 54.9 47.8 53 50.8 ...
##  $ curbWeight      : int  3740 3055 3750 2169 1918 2935 3495 3950 2975 1944 ...
##  $ engineType      : Factor w/ 5 levels &amp;quot;dohc&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;ohc&amp;quot;,..: 5 3 3 3 3 3 3 5 3 3 ...
##  $ engineSize      : int  234 164 183 98 97 141 183 326 146 92 ...
##  $ bore            : num  3.46 3.31 3.58 3.19 3.15 3.78 3.58 3.54 3.62 2.97 ...
##  $ stroke          : num  3.1 3.19 3.64 3.03 3.29 3.15 3.64 2.76 3.5 3.23 ...
##  $ compressionRatio: num  8.3 9 21.5 9 9.4 9.5 21.5 11.5 9.3 9.4 ...
##  $ horsepower      : int  155 121 123 70 69 114 123 262 116 68 ...
##  $ peakRpm         : int  4750 4250 4350 4800 5200 5400 4350 5000 4800 5500 ...
##  $ cityMpg         : int  16 20 22 29 31 24 22 13 24 31 ...
##  $ highwayMpg      : int  18 25 25 34 37 28 25 17 30 38 ...
##  $ price           : int  34184 24565 28248 8058 6649 15985 28176 36000 17669 6189 ...

# take a quick look
hist(imp.train$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(imp.test$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will be predicting the price of imported automobiles in this example. While tree-based methods are scale-invariant with respect to predictor variables, this is not true for the response variable. Hence, let’s take a log-transformation on &lt;strong&gt;price&lt;/strong&gt; here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp.train$price &amp;lt;- log(imp.train$price)
imp.test$price &amp;lt;- log(imp.test$price)

# take a look again
hist(imp.train$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(imp.test$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-tree-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision tree&lt;/h3&gt;
&lt;p&gt;Done with the preparation, let’s begin with decision trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Construct decision tree model
tree.mod &amp;lt;- tree(price ~ ., data = imp.train)

# here&amp;#39;s how the model looks like
plot(tree.mod)
title(&amp;quot;Decision tree&amp;quot;)
text(tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s see if our decision tree requires pruning
cv.prune &amp;lt;- cv.tree(tree.mod, FUN = prune.tree)
plot(cv.prune$size, cv.prune$dev, pch = 20, col = &amp;quot;red&amp;quot;, type = &amp;quot;b&amp;quot;,
     main = &amp;quot;Cross validation to find optimal size of tree&amp;quot;,
     xlab = &amp;quot;Size of tree&amp;quot;, ylab = &amp;quot;Mean squared error&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# looks fine

# now let&amp;#39;s make some predictions
tree.pred &amp;lt;- predict(tree.mod,
                     subset(imp.test,select = -price), 
                     type = &amp;quot;vector&amp;quot;)

# Comparing our predictions with the test data:
plot(tree.pred, imp.test$price, main = &amp;quot;Decision tree: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1) # A prediction with zero error will lie on the y = x line&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# What is the MSE of our model?
print(tree.mse &amp;lt;- mean((tree.pred - imp.test$price) ** 2))
## [1] 0.04716916&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our decision tree model predicts the price of imported automobiles with a mean squared error of 0.0472. As with the previous section, we will make comparsions on model performances later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Next, bagging.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.mod&amp;lt;-randomForest(price ~ ., data = imp.train,
                     mtry = num.var - 1, # try all variables at each split, except the response variable
                     ntree = 300,
                     importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees
# here, the error is the mean squared error,
# not classification error
plot(bg.mod$mse, type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Bagging: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# variable importance
varImpPlot(bg.mod,
           main = &amp;quot;Bagging: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s make some predictions
bg.pred &amp;lt;- predict(bg.mod,
                   subset(imp.test,select = -price))

# Comparing our predictions with test data:
plot(bg.pred,imp.test$price, main = &amp;quot;Bagging: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE of bagged model
print(bg.mse &amp;lt;- mean((bg.pred - imp.test$price) ** 2))
## [1] 0.03004431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our bagging model predicts the price of imported automobiles with a mean squared error of 0.03.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random forest&lt;/h3&gt;
&lt;p&gt;Finally, the random forest model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.mod&amp;lt;-randomForest(price ~ ., data = imp.train,
                     mtry = floor((num.var - 1) / 3), # 7; only difference from bagging is here
                     ntree = 300,
                     importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees
plot(rf.mod$mse, type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Random forest: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# tuning the mtry hyperparameter:
# model may be rebuilt if desired
tuneRF(subset(imp.train, select = -price),
       imp.train$price,
       ntreetry = 100)
## mtry = 7  OOB error = 0.02543962 
## Searching left ...
## mtry = 4     OOB error = 0.03064481 
## -0.2046095 0.05 
## Searching right ...
## mtry = 14    OOB error = 0.02643948 
## -0.03930348 0.05
##    mtry   OOBError
## 4     4 0.03064481
## 7     7 0.02543962
## 14   14 0.02643948
title(&amp;quot;Random forest: Tuning the mtry hyperparameter&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

# variable importance
varImpPlot(rf.mod,
           main = &amp;quot;Random forest: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s make some predictions
rf.pred &amp;lt;- predict(rf.mod,
                   subset(imp.test, select = -price))

# Comparing our predictions with test data:
plot(rf.pred, imp.test$price, main = &amp;quot;Random forest: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE of RF model
print(rf.mse &amp;lt;- mean((rf.pred - imp.test$price) ** 2))
## [1] 0.03139744&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our random forest model incurs a mean squared error of 0.0314 for the prediction of imported automobile prices&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-performances-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization of performances&lt;/h3&gt;
&lt;p&gt;For comparison purposes, let’s also construct a ordinary least squares (linear regression) model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ols.mod &amp;lt;- lm(price ~ ., data = imp.train)

# predictions
ols.pred &amp;lt;- predict(ols.mod,
                   subset(imp.test, select = -price))

# comparisons with test data:
plot(ols.pred, imp.test$price, main = &amp;quot;OLS: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_ols1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE
print(ols.mse &amp;lt;- mean((ols.pred-imp.test$price) ** 2))
## [1] 0.03556617&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s compare their performances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Comparing MSEs of various models:
barplot(c(tree.mse,
          bg.mse,
          rf.mse,
          ols.mse),
        main = &amp;quot;Mean squared errors of various models&amp;quot;,
        names.arg = c(&amp;quot;Tree&amp;quot;, &amp;quot;Bagging&amp;quot;, &amp;quot;RF&amp;quot;, &amp;quot;OLS&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_viz1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our top performer here is the random forest model, followed by the bagging model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About thestatsguy.rbind.io</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;h2 id=&#34;about-this-blog&#34;&gt;About this blog&lt;/h2&gt;
&lt;p&gt;This blog is simply an outlet for me to write, and in the process, reflect and analyse, about things in life that I naturally leaned towards – R, statistics, machine learning, investing, personal finance, analyzing financial data, and stoicism, while writing behind a pseudonym.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Machine learning and statistics&lt;/strong&gt;: the blog was so named “the stats guy” largely because that became my nickname amongst some friends. Also, there’s an increasing emphasis on hacking and e.g. layer-stacking in deep learning, with too little statistical groundings, as above. The subtle disregard of statistics in data science is unhealthy to say the least, and so here I write about certain ideas and principles that are, for the most part, neglected in analyzing and modelling data. Finally, I figured it’s more liberating to write behind a pseudonym of sorts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finance, retail investing in Singapore&lt;/strong&gt;: there are plenty of materials out there regarding retail investing in US/UK, but not enough for those of us who live in Singapore. So I think aloud about these topics here as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_finance&#34;&gt;Statistical finance&lt;/a&gt;&lt;/strong&gt;: it’s sort of the overlap between my data science/statistics background and me managing my own money, so this is fun to me and comes naturally.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rat_race&#34;&gt;Corporate rat race&lt;/a&gt;&lt;/strong&gt;: in here, I write a little bit about being in the corporate rat race, how to protect yourself and survive, how to (try to) excel, and what&amp;rsquo;s the endgame for everyone (including myself).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Stoicism&#34;&gt;Stoicism&lt;/a&gt;&lt;/strong&gt;: or rather, &amp;ldquo;applied Stoicism&amp;quot;^[There is no such thing as &amp;ldquo;applied Stoicism&amp;rdquo; - I made it up only because it made sense to me. Like applied statistics.]. I spend some time exploring how Stoicism is helpful for ordinary people like me.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;about-me&#34;&gt;About me&lt;/h2&gt;
&lt;p&gt;I’m of course a Singaporean, living and working in Singapore, got my B.Sc and M.Sc from the National University of Singapore (NUS), and work as a data scientist in a Boston-based start-up. By all modern and millennial standards, especially that of in Singapore, I’m an extremely boring person and spend most of my waking (and highly routinized if I am not travelling for work) hours on the few things I enjoy the most, such as those listed above. So fortunately, my life is a little less boring than myself.&lt;/p&gt;
&lt;p&gt;By the way, neither of my B.Sc or M.Sc has anything remotely related to business, finance, or investments. Nor am I in any sorts of professional role that justifies me giving of any sorts of financial advice. That means you should take all my opinions as a pinch of salt and do your own research. I am, however, sufficiently trained in looking at numbers and cryptic computer languages (or at least, I dare to claim so), and my opinions are largely backed by some levels of analysis of available data.&lt;/p&gt;
&lt;p&gt;In any case, if you would like a chat, you can drop me a note at thestatsguy90[at]gmail[dot]com. If you are interested in the analyses I did or the code I wrote, you can drop by my &lt;a href=&#34;https://github.com/thestatsguy&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;boring-some-history&#34;&gt;[boring] Some history&lt;/h2&gt;
&lt;p&gt;This blog you are looking at right now (mid 2020) is the result of years of trying to settle down on a platform and write content. Ever since I joined the workforce in July 2015, I have always wanted to have a consolidated place to write, consistently and painlessly. If you scrutinise a little on the post dates, it&amp;rsquo;s quite obvious there are bouts of time over the years where I was able to diligently write, piece after piece, idea after idea. At the start it was just short writings (~2-3 mins of slow reading) with sporadic ideas, and over time the pieces became longer and (hopefully) more in-depth and interesting. In addition, what I paid attention to has changed over time as well.&lt;/p&gt;
&lt;h3 id=&#34;blogging-platform---now-and-then&#34;&gt;Blogging platform - now and then&lt;/h3&gt;
&lt;p&gt;This blog is put together using a variety of tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.netlify.com/&#34;&gt;Netlify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://support.rbind.io/about/&#34;&gt;rbind.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://disqus.com/&#34;&gt;Disqus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://analytics.google.com/&#34;&gt;Google Analytics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This stack is by far the most complex one that I have used, after being initially convinced by &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt;. The kind of ecosystem that &lt;a href=&#34;https://yihui.org/&#34;&gt;Xie Yihui&lt;/a&gt; and others have built is powerful and customizable, yet painless (relatively) and easy to operate^[The &lt;a href=&#34;https://xmin.yihui.org/&#34;&gt;Hugo theme&lt;/a&gt; of this blog is built by Xie Yihui as well, and it looks great.]. Proficiency in R helps as well.&lt;/p&gt;
&lt;p&gt;Before this, I have tried various platforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/new&#34;&gt;Google sites&lt;/a&gt; (mid 2014 to late 2015)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.blogger.com&#34;&gt;Blogger&lt;/a&gt; (twice! once in late 2016 to mid 2017 and again in late 2018)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pelicanthemes.com/&#34;&gt;Pelican&lt;/a&gt; + &lt;a href=&#34;https://jupyter.org/&#34;&gt;Jupyter&lt;/a&gt; + &lt;a href=&#34;https://pages.github.com&#34;&gt;GitHub Pages&lt;/a&gt; (early 2019)^[Check &lt;a href=&#34;https://www.dataquest.io/blog/how-to-setup-a-data-science-blog/&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;https://rasor.github.io/using-pelican-blog-on-github-pages.html&#34;&gt;this&lt;/a&gt; out if you are interested in this stack.]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wordpress.com&#34;&gt;WordPress&lt;/a&gt; (late 2018 to mid 2019)^[This is still alive &lt;a href=&#34;https://thestatsguy.home.blog/&#34;&gt;here&lt;/a&gt; by the way, with weeds growing here and there.]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, with thestatsguy.rbind.io technically being my 6th attempt, I am finally satisfied! With that, I also spent some time migrating some of my old writings onto here^[However, I still reference static links of images like &lt;a href=&#34;https://thestatsguyhome.files.wordpress.com/2018/12/SSB.jpg&#34;&gt;this one&lt;/a&gt; (look at the URL) in my WordPress blog from thestatsguy.rbind.io - something that I have been procrastinating to migrate and fix since it&amp;rsquo;s going to be a boring and time-consuming task. I may never bother to fix this.].&lt;/p&gt;
&lt;h3 id=&#34;even-more-boring-my-focus-for-the-past-10-years---2010-to-2020&#34;&gt;[even more boring] My focus for the past 10 years - 2010 to 2020&lt;/h3&gt;
&lt;p&gt;Over the years, my world view, and together with it, my focus and my interests have changed. For example, I used to think that climbing the corporate ladder is a lofty goal that is worthy of my time and energy. I spent time understanding and strategizing about my work, my work environment and my colleagues at a deeper level, so that I can get more done with less time and then use that productivity to value-add and increase my chances of promotion. These days, I think less of these things. I try to focus more on myself, my family, my development as an individual. Moreover, with life events unfolding one after another in the mid 2010s, Stoicism has been critical in helping me reflect on and dissect these events, and in the process, learn more about myself and the human mind^[The most important lesson or mental model that I learnt from Stoicism is probably the idea of clearly differentiating what that is within my control, and what that is not. “The chief task in life is simply this: to identify and separate matters so that I can say clearly to myself which are externals not under my control, and which have to do with the choices I actually control. Where then do I look for good and evil? Not to uncontrollable externals, but within myself to the choices that are my own&amp;hellip;” - &lt;a href=&#34;https://dailystoic.com/Epictetus/&#34;&gt;Epictetus&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Nonetheless, some interests didn&amp;rsquo;t change. I was and am still interested in financial markets and assets. I remember reading &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Quants&#34;&gt;The Quants&lt;/a&gt; by Scott Patterson during 2010 and was intrigued by the idea of math wizards gaming the stock market, looking for signals and earning big money (albeit everything came crashing down). This was when the global financial crisis was still fresh in people&amp;rsquo;s minds, but I was too young and ignorant to appreciate what a financial crisis was. This was also before I entered NUS. Fast forward to 2020, I still find the topic of financial markets alluring - though I must add that I probably would never want to formally get a job in this sector. Perhaps it&amp;rsquo;s alluring only when I admire it from the outside.&lt;/p&gt;
&lt;p&gt;During my NUS days was when data science took the world by storm, and so during the early part of 2010s (2011 to 2016), I spend the most of those years understanding machine learning, data science, data analytics - through my B.Sc degree as well as later, honing my skills on the job as a data science consultant. For a year or so, I also briefly dabbled with product management (of data analytics products).  However, after a while, my focus shifted more to statistics and my M.Sc in 2017, after realising that half the time in ML, most people don&amp;rsquo;t have the fundamentals to treat data in the right manner (like Kaggle). Throughout this time, R remained to be my language of choice for many tasks - my first contact with R was during an internship I did in 2012.&lt;/p&gt;
&lt;p&gt;At the same time, as a young professional, my humble net worth started to grow, with salary coming in month after month. I started to spend time understanding investing as a retail investor, to &amp;ldquo;make my money work for me&amp;rdquo;. Mid 2018 was the first time I shifted from simply reading and researching, to putting money to where my mind or mouth is. That&amp;rsquo;s when I started writing more on investing as well - and now, also coming back to think more about the use of statistics in finance.&lt;/p&gt;
&lt;p&gt;Looking back, I guess it turns out that these topics - R, statistics, machine learning, finance, investing, understanding paid work, employment and the corporate world, stoicism, and others^[Like &lt;a href=&#34;https://en.wikipedia.org/wiki/Epistemology&#34;&gt;epistemology&lt;/a&gt;.], have very much defined my 20s as a young adult. I enjoy studying and writing about these topics, and I hope that you, the reader will enjoy reading my take on them as well.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
