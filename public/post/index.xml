<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on The Stats Guy</title>
    <link>/post/</link>
    <description>Recent content in Posts on The Stats Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>FAQ about the Master of Science in Statistics programme in NUS</title>
      <link>/post/2020/08/30/faq-about-the-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/08/30/faq-about-the-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>

&lt;p&gt;It&amp;rsquo;s been a while since I last worked on this blog, but today (Sunday!) I have some time in my hands to write a little bit here.&lt;/p&gt;

&lt;p&gt;Prior to this, I have been receiving more questions about the MSc. Stats programme in NUS - the timing makes a lot of sense since it was just before the start of the new Academic Year in NUS (AY20/21).&lt;/p&gt;

&lt;p&gt;In this post, I have collated a number of questions that I have received, most rephrased to be more generic, as well as my responses to them. Moving forward, I may also update this post as I receive more questions.&lt;/p&gt;

&lt;p&gt;Before I continue, I would like to sincerely thank my readers for reaching out to me and asking these questions. I hope that my answers were helpful for you in your decision-making and I hope you excel in the programme. I also hope you don&amp;rsquo;t mind me sharing these generic questions to a wider audience.&lt;/p&gt;

&lt;p&gt;And for those of you starting the new AY in NUS - hope you have a good year ahead!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;on-my-background&#34;&gt;On my background&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q1. May I know what your educational/professional background is before the MSc?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A1. My degree was in biostatistics before the Msc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2. What field were you in before joining the programme?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A2. I was and still am in the data science field.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q3. What made you want to join the programme?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A3. I was a few years into the workforce and wanted to go back and study something. Stats happened to be the area that I am most interested in.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;on-the-programme-in-general&#34;&gt;On the programme in general&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q1. Does the programme require knowledge on programming languages for data science (R, Python)? Does it teach any programming languages for data science?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A1. There is no emphasis in programming in the Stats masters. Some modules may require you to code in R, but not many, and not much within those modules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2. Is the programme grueling?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A2. I found it manageable, but the start was a little challenging, getting used to studying again. And certain modules are more theoretical in nature, with some bit of theorem proving.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q3. Would you recommend the programme?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A3. Yes definitely. I found it useful for my work, and managed to learn what I wanted to.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q4 .Do you find the statistics content that you have learnt useful for your current job?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A4. I would say yes. I work in the data science industry and it&amp;rsquo;s always useful to fall back on fundamental statistics principles to think through certain issues and problems.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;on-admission&#34;&gt;On admission&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q1. Do you know the competitiveness of the program? Is it hard to get in for NUS grads?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A1. It really depends on your background prior to the MSc. Competitiveness in terms of getting in may not be difficult, but coping in the workload may be so, both for full time (5 msc modules) or part time (2-3 msc modules + a day job). Also, having an honours would help as you can be in Track 1 straightaway (the shorter track with 40MCs).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2. Do you know around when you got your results of the admissions?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A2. I got my offer letter via email sometime near late May during the year I applied.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;on-modules-and-workload&#34;&gt;On modules and workload&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q1. Do you have any advice on which module combination to take in each semester? (MOE subsidy for AY2020/21 requires students to complete the required modules within 2 years for part time track)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A1. I would recommend finishing the two core modules of 5201 and 5202 as soon as possible. It’s hard to plan ahead of time for the MSc in terms of modules because modules offered can differ significantly from academic year to academic year. It depends heavily on lecturer availability and some modules only appear once in 4 semesters I believe.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2. What is the difference between &amp;ldquo;Coursework&amp;rdquo; and non-coursework? Seems like some non-coursework modules are also offered at 7-10pm for part-time students.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A2. For the &amp;ldquo;Coursework&amp;rdquo; tag, it is used by the department to tag whether it&amp;rsquo;s meant more for the MSc Research students as opposed to 100% course work students. Technically the non-coursework modules are supposed to be harder - but not always the case. And also whether it&amp;rsquo;s possibly taught during working hours vs. 7-10pm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q3. For most of the statistics modules that you have taken so far, is it true that they are very theoretical, with majority of the tutorial questions based on proving theories rather than application questions?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A3. It heavily depends on the nature of the module, and more importantly the lecturer. E.g. the same topic on regression can be taught in both heavily theoretical and more applied methods, depending on the lecturer&amp;rsquo;s style. And some lecturers are &amp;ldquo;renowned&amp;rdquo; for being theoretical no matter what topic they are teaching. That said, since these are all stats modules, proving-type of questions are inevitable. I would recommend finding out more about the lecturer&amp;rsquo;s style.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q4. For the ten modules that you have taken, was the workload and difficulty level for those modules somewhat similar to ST5201?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A4. Yes, largely the same with minor variations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q5. On average, how many hours per week do you think you spent on your lectures and tutorials?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A5. I typically spend 1 Sunday (morning + until before dinner) and ~1 to 1.5 weeknights per week on catching up on lectures and working on tutorials, on top of the 7-10pm lectures.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q6. Were there a lot of group projects or assignments?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A6. Not for me, usually stats mods in NUS don&amp;rsquo;t have many group projects. I only had one with ST5227 applied data mining.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q7. How was the bell curve for the statistics modules? i.e. Were the exam papers rather simple but with steep bell curves or very difficult with shallower bell curves?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A7. Again this heavily depends on the lecturer again. The variation in style, content level, exam format etc in the MSc is larger than that in NUS undergrad courses, because there isn&amp;rsquo;t really a tight oversight from the Faculty or Registrar, as compared to undergrad courses. I think I have equally sat in both steep and shallow bell curves.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q8. Do you think reading lecture notes and doing tutorials alone is sufficient to score a B+ for most of the statistics modules?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A8. Doubt so. For the majority of students, B+ is a respectable 2nd Upper standard. Reading lecture notes and doing tuts can warrant a pass (C/C+/B-?) probably. I would say attending lectures, reading the recommended textbooks (just very briefly on some of the harder topics) and consistent revision are important. If you are a part-time student, consistency is even more important because we would only have some portions of our week allocated to the MSc, and on top of work and life. Also, the tutorials themselves can be very short (4-5, at most 6 questions?). It&amp;rsquo;s not enough to assess yourself to see whether you have a holistic understanding of the chapter.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q9. Of all the statistics modules that you have taken so far, is there any that you would not recommend to take?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A9. None I think. I enjoyed my modules. The hardest part for me was at the start, taking ST5201 during the first semester. It was relatively theoretical plus the fact that I was a little &amp;ldquo;rusty&amp;rdquo; in the theoretical stuff, plus the need to get used to studying again. After that, it got a lot better and more manageable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q10. Are you able to share some materials from the MSc?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A10. Sure, write to me at my gmail (see &lt;a href=&#34;https://thestatsguy.rbind.io/about/&#34;&gt;About&lt;/a&gt;) and we will see how I can help you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[wip] The intuition behind averaging</title>
      <link>/post/2020/06/19/wip-the-intuition-behind-averaging/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/06/19/wip-the-intuition-behind-averaging/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Everyone knows what the average is. To find the average of the bunch of numbers, simply add them all up and then divide the total by the number of numbers there are. Calculating the average of a series of numbers has always been a straightforward way in which we summarize many numbers: “on average, I eat about 3 to 4 apples per week.” This calculating and reporting of the average summarizes a larger amount of information into a single summary - the mean itself.&lt;/p&gt;
&lt;p&gt;Consider a series of say 9 numbers. Let’s call it &lt;code&gt;v&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v &amp;lt;- c(2, 6, 9, 4, 10, 3, 3, 7, 8)
length(v)
## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It makes sense to report the mean as a representative summary of a series of numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(v)
## [1] 5.777778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Knowing the mean of something gives us some intuitive sense of the range of numbers that we are dealing with. For example, the mean definitely has to be within the series’ minimum (2) and maximum (10). With this, we also tend to associate the mean to be the “middle” of a series of numbers, whatever the “middle&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;” means.&lt;/p&gt;
&lt;p&gt;In this post, I would like to illustrate a particular property of the mean that makes it a powerful single summary to describe a series of numbers, namely:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The mean minimizes the squared error.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At this point you may have some intuition on what’s about to follow, or it may not be immediately clear to you why this is important or what I am talking about. I will use a rather peculiar example to illustrate this property and its importance. Hopefully by the end of this example, it would be clearer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-peculiar-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A peculiar example&lt;/h3&gt;
&lt;p&gt;Consider the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;diagram&gt;&lt;/p&gt;
&lt;p&gt;In this diagram, there are a bunch of numbers and a single question mark. Behind the question, is also a number. The known numbers are the same as in our friend &lt;code&gt;v&lt;/code&gt; above.&lt;/p&gt;
&lt;p&gt;Our task is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make a guess on what that mystery number could be. And,&lt;/li&gt;
&lt;li&gt;If we can’t get it right, then reduce, as much as possible, the error we incur on our guess.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that there is no special ordering or patterns in these numbers or their location in their circle. The only thing we know is that these numbers belong to a larger group of numbers, or that they all belong in a group with some other numbers unseen to us at the moment. (If it helps, you can think of them as being numbers relevant to something in real life, like the number of guppies that some fish owners have in their homes, out of all fish owners).&lt;/p&gt;
&lt;p&gt;There are a few approaches to think about this strange and seemingly irrelevant problem:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Since 3 appeared twice, we should guess 3&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Since 3 appeared twice, we should not guess 3.&lt;/li&gt;
&lt;li&gt;Since we have no other information, any guess is as good as any other. For example, guessing 1,000,000 is the same as guessing 3 or 8 or any other number.&lt;/li&gt;
&lt;li&gt;Given that we know these numbers belong together in some fashion, while the actual number could be anything - what is a good guess that reduces our error?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Remember, the only things we know now are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These numbers belong to a larger group of other numbers.&lt;/li&gt;
&lt;li&gt;We want to minimize our error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s consider each of these approaches.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach 1 - guess 3&lt;/strong&gt;: frequentist, could work well. Keeping to the guppy example, this assumes many fish owners keep 3 guppies, which, based on the information that we have, is an assumption. However, we can’t really use this as a rule for guessing since there’s no guarantee that duplicate numbers will always appear.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approach 2 - don’t guess 3&lt;/strong&gt;: this assumes that we incidentally picked 3 twice, and the odds of 3 appearing again is low&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Then, with this, what should we guess? Kind of stuck.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approach 3 - it doesn’t matter, guess any number&lt;/strong&gt;: this is useless as we can’t make any intelligent guess of any sorts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approach 4 - what can reduce our error?&lt;/strong&gt;: firstly, what does “belong to the same group” mean? The most intuitive way of extrapolating from that is that we can at least guess that the mystery number should be close to the other numbers of the circle - i.e. we have no reason to think that it’s smaller than the smallest number, or larger than the largest number, and have some intuition to guess that the mystery number is somewhere within the smallest and the largest number. A reasonable intuition.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far so good? It seems like approaches 1 to 3 are not so helpful, and reducing error is our lead forward. At the least, approach 4 gives us some probable region of interest to guess, namely somewhere between the minimum 2 and maximium 10.&lt;/p&gt;
&lt;p&gt;What then, minimizes the error? Well we must first define the error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimizing-our-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Minimizing our error&lt;/h3&gt;
&lt;p&gt;We are looking for a guess that reduces our error to as low as possible, given what we got. In an intuitive sense, we can define error to look something like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[error = actual - guess\]&lt;/span&gt;
Fair? We define the error to be distance or difference between the actual value, and our guess. The small the error, the closer our guess is to the actual value, whatever it may be.&lt;/p&gt;
&lt;div id=&#34;two-sides-of-the-error&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Two sides of the error&lt;/h4&gt;
&lt;p&gt;Now consider our objective of minimizing the error. This means that we would like to have as low of an error as possible. Suppose we make two guesses: one incurred an error of &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;, while another incurred an error of &lt;span class=&#34;math inline&#34;&gt;\(-4\)&lt;/span&gt;. Numerically, &lt;span class=&#34;math inline&#34;&gt;\(-4\)&lt;/span&gt; is smaller than &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;, when in actual fact, both guesses and errors are equally far apart from the actual value. Therefore, to say that we would like “minimize” the error may not be as precise as we like. We would need a way of tweaking our error measurement so that if we try to minimize it, our approach does not favour an error of &lt;span class=&#34;math inline&#34;&gt;\(-10\)&lt;/span&gt; over an error of say &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Fortunately, there are simple ways to tweak our error measurement - here are two of them:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
sq.error = error^2 = (actual - guess)^2  \\
abs.error = |error| = |actual - guess|
\end{equation}
\]&lt;/span&gt;
The first way is simply to take the square of the error i.e. the &lt;strong&gt;squared error&lt;/strong&gt;. Taking the square resolves the issue of the &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; signs, in that &lt;span class=&#34;math inline&#34;&gt;\((-4)^2 = 4^2 = 16\)&lt;/span&gt;. We then try to minimize the squared error, since whatever that can minimize the squared error should also minimize the error&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. Likewise, taking the absolute value of the error, i.e. the &lt;strong&gt;absolute error&lt;/strong&gt; also resolves the issue of directions, in that &lt;span class=&#34;math inline&#34;&gt;\(|-4| = |4| = 4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To continue in this example, I will go ahead and pick the first method of squaring the error, and then come back to explain the key differences&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; between the 2 ways of modifying our error function.&lt;/p&gt;
&lt;p&gt;Following? OK, let’s continue. Our next step is to find something that can minimize the squared error.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-minimizes-the-squared-error---a-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What minimizes the squared error? - a simulation&lt;/h3&gt;
&lt;p&gt;To get a sense of this, let’s use some numerical simulation to get some intuition. Let’s bring our friend &lt;code&gt;v&lt;/code&gt; back again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v &amp;lt;- c(2, 6, 9, 4, 10, 3, 3, 7, 8)
length(v)
## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;v&lt;/code&gt; contains all the numbers in the diagram above, in no particular order, other than question mark. A simple way to get some intuition here is simply to iteratively regard each of the 9 numbers in &lt;code&gt;v&lt;/code&gt; as missing (i.e. a question mark), and use the remaining 8 numbers to make a guess, then validate our guess with the actual value. Confusing? Let me explain again, step-by-step:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;v = (2, 6, 9, 4, 10, 3, 3, 7, 8)
Step 1. Hide 2 and treat 2 as ?, use the rest of the numbers to guess, compare guess with actual value (2).
Step 2. Hide 6 and treat 6 as ?, use the rest to guess, compare guess with actual (6).
Step 3. Hide 9 and treat 9 as ?, use the rest to guess, compare guess with actual (9).
Step 4. ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By doing this, we get an interesting mechanic of iterating over different possibilities in order to learn something about our approach or objective of minimizing the squared error&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then for each step, let’s do many brute-force guesses, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Step 1. Hide 2 and treat 2 as ?, use the rest to guess, compare guess with actual value (2).
---&amp;gt;Step 1.1. Guess 1, compare guess (1) with actual value (2), calculate squared error
---&amp;gt;Step 1.2. Guess 2, compare guess (2) with actual value (2), calculate squared error
---&amp;gt;Step 1.3. Guess 3, compare guess (3) with actual value (2), calculate squared error
    ...
---&amp;gt;Step 1.10. Guess 10, compare guess (10) with actual value (2), calculate squared error

Step 2. Hide 6 and treat 6 as ?, use the rest to guess, compare guess with actual (6).
---&amp;gt;Step 2.1. ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s give it a shot and see what happens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(dplyr)
## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.3
## 
## Attaching package: &amp;#39;dplyr&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union
library(tidyr)
## Warning: package &amp;#39;tidyr&amp;#39; was built under R version 3.5.3
## 
## Attaching package: &amp;#39;tidyr&amp;#39;
## The following object is masked from &amp;#39;package:magrittr&amp;#39;:
## 
##     extract
set.seed(123)

v &amp;lt;- c(2, 6, 9, 4, 10, 3, 3, 7, 8)
mean(v)
## [1] 5.777778
length(v)
## [1] 9

# leave-one-out, let&amp;#39;s guess from 1 to 10 - reasonable?
# calculate error

simulation_set &amp;lt;- data.frame(leave_out = numeric(0),
                             guess = numeric(0),
                             error = numeric(0))

for(idx in seq_along(v)){
  leave_out &amp;lt;- v[idx]
  answer &amp;lt;- leave_out
  
  for(guess in 1:10){
    
    error &amp;lt;- guess - answer
    simulation_set &amp;lt;- rbind(simulation_set, data.frame(leave_out = leave_out, guess = guess, error = error))
  }
}

# calculate squared error
simulation_set$sq_error &amp;lt;- simulation_set$error**2
boxplot(simulation_set$sq_error ~ as.factor(simulation_set$guess), main = &amp;quot;Distribution of sq error for each guess (1 to 10)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-19-the-intuition-behind-averaging_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;v&lt;/code&gt; for vector in R.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Otherwise known as &amp;quot;&lt;a href=&#34;https://en.wikipedia.org/wiki/Central_tendency&#34;&gt;central tendency&lt;/a&gt;&amp;quot; in statistics.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Sort of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Frequentist_inference&#34;&gt;frequentist&lt;/a&gt; thinking.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Kind of like &lt;a href=&#34;https://stattrek.com/statistics/dictionary.aspx?definition=sampling_without_replacement&#34;&gt;sampling without replacement&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Because on both &lt;span class=&#34;math inline&#34;&gt;\([0,\infty]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\([-\infty,0]\)&lt;/span&gt; subdomains, the function &lt;span class=&#34;math inline&#34;&gt;\(f(x)=x^2\)&lt;/span&gt; is monotonic.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;In particular, you will see later that the mean minimizes the squared error, while the median minimizes the absolute error. Cool huh?&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;In machine learning, this is also known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation&#34;&gt;leave-one-out cross validation (LOOCV)&lt;/a&gt;. This and other types of model validation techniques is also one of the beautiful cornerstone in statistics - we observe what we have (data), and try to make do and make the best out of it. Just like life. Make do and make the best out of what you have, and you will lead a fruitful life.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A fuller review of my Master of Science in Statistics programme in NUS</title>
      <link>/post/2020/05/10/a-fuller-review-of-my-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/05/10/a-fuller-review-of-my-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_sparrows.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sparrows taking shelter from the afternoon shower at the Faculty of Science (December 2018)
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Back in January 2017, I first seriously considered taking up a postgraduate degree, as a means to improve myself and continue learning. Well, it wasn&amp;rsquo;t a long and hard decision, really, as I had and still have the freedom, the capacity and the means to study more.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Taking a part-time Masters this year in 2017&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2017/01/30&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am leaning towards taking a part-time Masters this year in 2017. Points of considerations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Masters or PhD?&lt;/li&gt;
&lt;li&gt;Full-time or part-time?&lt;/li&gt;
&lt;li&gt;(If masters) technical or non-technical?&lt;/li&gt;
&lt;li&gt;(If full-time) Overseas or local?&lt;/li&gt;
&lt;li&gt;(If local) NUS or NTU?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Full-time is personally a non-option for me, as I don’t see the financial sense in taking a sabbatical to pursue a full-time programme. That leaves full-time and overseas out.&lt;/p&gt;

&lt;p&gt;Based on what I know about a PhD programme, part-time PhD sounds like a nightmare. That leaves part-time Masters in Singapore as my option.&lt;/p&gt;

&lt;p&gt;Next question: technical or non-technical? Well I am leaning towards to doing something with technical content when studying - non-technical content can be picked up most of the time simply by being widely read and learning from work experiences. This means statistics or computing for me.&lt;/p&gt;

&lt;p&gt;And NUS is probably the better choice than NTU. SMU is not in my consideration.&lt;/p&gt;

&lt;p&gt;So for now, my choice is going to be M.Sc. Statistics from NUS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;Quoting myself in January 2017, &lt;a href=&#34;https://thestatsguy.rbind.io/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/&#34;&gt;here&lt;/a&gt;. Was I rigourous in my decision-making? Hmm&amp;hellip;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;So I decided to go with a Master of Science in Statistics programme in NUS, and starting my first semester in August 2017. I then took 5 semesters, all the way to December 2019, to complete the programme. Today, I have since happily graduated and have had a good experience with the programme.&lt;/p&gt;

&lt;p&gt;Around midway through the programme, I wrote a short review on the logistics and my experience of the programme so far.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;My Master of Science in Statistics programme in NUS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2019/02/09&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I have gotten quite a couple of questions regarding my current &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;MSc Statistics programme&lt;/a&gt; in NUS. Here are some broadstroke information about the programme and how I am approaching it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I&amp;rsquo;m doing the MSc by Coursework programme, which means a research thesis is not part of my curriculum. A MSc by Research option is available.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Under the Coursework programme, there is a Track 1 (40MC) programme and a Track 2 (80MC) one. Basically dependent on whether you have a Honours in your Bachelor&amp;rsquo;s degree. I&amp;rsquo;m doing the Track 1 programme - 40MC is equivalent to 10 modules. Under usual circumstances, it takes 2 full-time semesters to finish 10 modules, i.e. 1 academic year. Semesters run as per typical undergraduate semesters in Singapore.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;There&amp;rsquo;s also the part-time option, where one would take 4 to 5 semesters to finish the 10 modules - 5 semesters is basically 2 modules x 5 semesters = 10 modules.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I&amp;rsquo;m on the part-time programme. Personally, 3 modules on a part-time basis per semester is too much for me to handle - so I opt to finish my MSc in 5 semesters, or 2.5 academic years.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I&amp;rsquo;m currently in my 4th semester, so would be finishing the programme requirements by Dec 2019 and graduate during July 2020 (commencement).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For MSc Statistics, lectures typically run from 7pm to 10pm weeknights. Each module has 1 lecture per week, with the typical workload of tutorials, homework assignments, individual or group projects, subjected to respective lecturer&amp;rsquo;s discretion.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, the programme &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GDTuitionCurrent.pdf&#34;&gt;cost&lt;/a&gt; &lt;b&gt;$2,500 per semester for Singaporeans who are taking this MSc programme as their first higher qualification programme under the &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GD-Eligibility-Guidelines.pdf&#34;&gt;MOE Subsidy&lt;/a&gt;&lt;/b&gt;. Yes it&amp;rsquo;s pretty value for money if you ask me. This tuition fee amount is not unique to MSc Statistics, and is general to many other programmes in NUS, again provided if you belong to the above category.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My experience with the programme is a positive one so far.  Difficulty and commitment level is within my comfort zone, and I managed to learn quite a couple of new things. Modules that I have taken include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Applied Data Mining&lt;/li&gt;
&lt;li&gt;Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;Spatial Statistics&lt;/li&gt;
&lt;li&gt;Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;Experimental Design&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, in any case, it feels good to be a student again. Each time I go to my stats lecture after a long day of work, it &lt;em&gt;almost&lt;/em&gt; always feels therapeutic. Yea, almost.&lt;/p&gt;

&lt;p&gt;Finally, if you are looking to advance your data science street cred via a postgraduate degree, this is just one of many options, even within NUS or Singapore. Do your research wisely before committing to any!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;Quoting myself in Febuary 2019, &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/&#34;&gt;here&lt;/a&gt;.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Well, this post first appeared on my WordPress blog &lt;a href=&#34;https://thestatsguy.home.blog/2019/02/09/my-master-of-science-in-statistics-programme-in-nus/&#34;&gt;here&lt;/a&gt;, and I realised that this post on WordPress turned out to be one of the top results in a number of Google searches on this topic. The other search results are of course dominated by links to NUS and Faculty of Science itself.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_google_search.PNG&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Google search for &lt;a href=&#34;https://www.google.com/search?q=nus+msc+statistics+review&#34;&gt;&amp;ldquo;nus msc statistics review&amp;rdquo;&lt;/a&gt; - 4th result here (early May 2020)
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Turns out that I got a decent number of views on this post. Not bad considering that I don&amp;rsquo;t expect much (if any at all) traffic on my blog.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_msc_post_wordpress_stats.PNG&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Post views on WordPress (early May 2020)
&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-fuller-review&#34;&gt;A fuller review&lt;/h3&gt;

&lt;p&gt;Since there is &lt;strong&gt;&lt;em&gt;some&lt;/em&gt;&lt;/strong&gt; interest in this topic, I thought I would spend some time in this post to do a more complete review of the programme and my experience.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Admission and logistics&lt;/li&gt;
&lt;li&gt;Review of some of the stats modules I took

&lt;ul&gt;
&lt;li&gt;ST5201 Basic Statistical Theory&lt;/li&gt;
&lt;li&gt;ST5202 Applied Regression Analysis&lt;/li&gt;
&lt;li&gt;ST5225 Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;ST5218 Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;ST5211 Sampling From Finite Populations&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Some relevant Singaporean hacks

&lt;ul&gt;
&lt;li&gt;SkillsFuture Credit&lt;/li&gt;
&lt;li&gt;Post-Secondary Education Account (PSEA)&lt;/li&gt;
&lt;li&gt;Income Tax Course Fees Relief&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;How I went about my life while being a part-time student&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;admission-and-logistics&#34;&gt;Admission and logistics&lt;/h3&gt;

&lt;p&gt;On this, what I have written before &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/&#34;&gt;here&lt;/a&gt; pretty much summarized it, except that I just want to quickly refer you to this &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;link&lt;/a&gt; for the admission criteria and requirements.&lt;/p&gt;

&lt;h3 id=&#34;review-of-some-of-the-stats-modules-i-took&#34;&gt;Review of some of the stats modules I took&lt;/h3&gt;

&lt;h4 id=&#34;st5201-basic-statistical-theory&#34;&gt;ST5201 Basic Statistical Theory&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_5201.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sample content in Basic Statistical Theory - Estimator Consistency
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;ST5201 Basic Statistical Theory, later renamed to Statistical Foundations of Data Science&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:I-believe-the-co&#34;&gt;&lt;a href=&#34;#fn:I-believe-the-co&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, is one of the two core modules for the MSc. Recommended to take during the very first semester of the programme, this module is the pre-requisite to several other MSc modules&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Though-later-on&#34;&gt;&lt;a href=&#34;#fn:Though-later-on&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and covers basic statistics and probability theory - &amp;ldquo;basic&amp;rdquo; as in fundamental and theoretical, not easy.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Exploratory data analysis including heat map and concentration map&lt;/li&gt;
&lt;li&gt;Random variables&lt;/li&gt;
&lt;li&gt;Joint distributions&lt;/li&gt;
&lt;li&gt;Expected values&lt;/li&gt;
&lt;li&gt;Limit theorems.&lt;/li&gt;
&lt;li&gt;Estimation of parameters including maximum likelihood estimation, Bayesian approach to parameter estimation&lt;/li&gt;
&lt;li&gt;Testing hypotheses and confidence intervals, bootstrap method of finding confidence interval, generalized likelihood ratio statistics&lt;/li&gt;
&lt;li&gt;Summarizing data: measures of location and dispersion, estimating variability using Bootstrap method, empirical cumulative distribution function, survival function, kernel probability density estimate&lt;/li&gt;
&lt;li&gt;Basic ideas of predictive analytics using multiple linear and logistic regressions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It was quite a challenging module for me as I never had a formal background in theoretical statistics via my BSc. The only way I could counteract this was to spend more time and energy in making up for my not-so-strong theoretical background. It was also my first semester in the MSc so this module was the one that set my expectations for the subsequent semesters, in terms of the amount of work needed per module. I was taught by Dr Choi Yunjin.&lt;/p&gt;

&lt;h4 id=&#34;st5202-applied-regression-analysis&#34;&gt;ST5202 Applied Regression Analysis&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_5202.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sample content in Applied Regression Analysis - Bias-variance tradeoff
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;ST5202 Applied Regression Analysis is the second of the 2 core modules in the programme. Unlike 5201, content in 5202 was more platable to me, with these regression modelling techniques being more applied than theoretical. If you are largely familiar with regression analysis then this module is mainly a refresher more than anything else. Like 5201, 5202 is a pre-req to many other modules in the programme. I was also taught by Dr Choi Yunjin for this module.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Multiple regression&lt;/li&gt;
&lt;li&gt;Model diagnostics, remedial measures&lt;/li&gt;
&lt;li&gt;Variable selection techniques&lt;/li&gt;
&lt;li&gt;Non-least squares estimation&lt;/li&gt;
&lt;li&gt;Nonlinear models&lt;/li&gt;
&lt;li&gt;One and two factor analysis of variance, analysis of covariance&lt;/li&gt;
&lt;li&gt;Linear model as special case of generalized linear model&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;st5225-statistical-analysis-of-networks&#34;&gt;ST5225 Statistical Analysis of Networks&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_networks.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sample content in Statistical Analysis of Networks - Exponential Random Graph Models
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;ST5225 Statistical Analysis of Networks was taught by Dr Wang Wanjie. Quite an interesting module that is a little different from the other stats modules. Typically, network / graph analysis are covered more by a computer science course than a statistics course.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Graph structures, adjacency matrix&lt;/li&gt;
&lt;li&gt;Graph sampling&lt;/li&gt;
&lt;li&gt;Centrality, cohesion, density, cliques, clustering&lt;/li&gt;
&lt;li&gt;Graph partitions&lt;/li&gt;
&lt;li&gt;Matching markets&lt;/li&gt;
&lt;li&gt;The World Wide Web, PageRank&lt;/li&gt;
&lt;li&gt;Graph models, random graph, stochastic block model, exponential random graph model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another interesting point for me on this module was that the lectures took place on Saturdays 1pm to 3pm, which turned out to be a good timing for lectures&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:What-I-did-was-t&#34;&gt;&lt;a href=&#34;#fn:What-I-did-was-t&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. I don&amp;rsquo;t believe this module is offered regularly.&lt;/p&gt;

&lt;h4 id=&#34;st5218-advanced-statistical-methods-in-finance&#34;&gt;ST5218 Advanced Statistical Methods in Finance&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_finance.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sample content in Advanced Statistical Methods in Finance - Capital Asset Pricing Model
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Of all the MSc modules that I took, this one stands out to be my favorite and resonates with me the most. Guess that’s mainly because I like topic, as well as of the fact that I was heavily experimenting with investing on my own during that time. I also found that using finance as the backdrop or context to study certain statistical concepts, such as copula or factor analysis, to be more engaging than perhaps studying these topics in vaccum.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Statistical distributions: Value-at-Risk (VaR)&lt;/li&gt;
&lt;li&gt;Linear regression: Capital Asset Pricing Model (CAPM)&lt;/li&gt;
&lt;li&gt;Factor analysis: Arbitrage Pricing Theory&lt;/li&gt;
&lt;li&gt;Time series analysis: price forecast, volatility modelling&lt;/li&gt;
&lt;li&gt;Copulae: tail dependence of asset prices&lt;/li&gt;
&lt;li&gt;Estimation of covariance matrix and optimization: Markowitz’s portfolio theory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many other topics that could just as well fit into the theme of the module, but unfortunately 13 weeks isn&amp;rsquo;t a very long time. I was taught by Prof Xia Yingcun and he is a great lecturer who painstakingly explains each and every little detail and concept so that it&amp;rsquo;s clear for his students&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Other-than-copul&#34;&gt;&lt;a href=&#34;#fn:Other-than-copul&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. I highly recommend this module if you have a chance to take it.&lt;/p&gt;

&lt;h4 id=&#34;st5211-sampling-from-finite-populations&#34;&gt;ST5211 Sampling From Finite Populations&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_sampling.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Sample content in Sampling from Finite Populations - Regression Estimation of population parameters
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Finally, ST5211 Sampling From Finite Populations is my very last module during the MSc. I was taught by Prof Zhou Wang, who also painstakingly explains every detail to his students.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple random sampling&lt;/li&gt;
&lt;li&gt;Stratified sampling&lt;/li&gt;
&lt;li&gt;Ratio and regression estimation&lt;/li&gt;
&lt;li&gt;Sampling with unequal probabilities&lt;/li&gt;
&lt;li&gt;Systematic sampling&lt;/li&gt;
&lt;li&gt;Single stage cluster sampling&lt;/li&gt;
&lt;li&gt;Two-stage cluster sampling&lt;/li&gt;
&lt;li&gt;Design-based versus model-based inference&lt;/li&gt;
&lt;li&gt;Small domain estimation&lt;/li&gt;
&lt;li&gt;Nonresponse and other nonsampling errors&lt;/li&gt;
&lt;li&gt;Survey quality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I had a lot of fun with this module as it was the only module I had left during my final semester, so it was more enjoyable than it was hard work. And the content in this module is genuinely interesting and applicable in real life problems.&lt;/p&gt;

&lt;h3 id=&#34;some-relevant-singaporean-hacks&#34;&gt;Some relevant Singaporean hacks&lt;/h3&gt;

&lt;h4 id=&#34;skillsfuture-credit&#34;&gt;SkillsFuture Credit&lt;/h4&gt;

&lt;p&gt;This course is eligible for claim from your &lt;a href=&#34;https://www.skillsfuture.sg/Credit&#34;&gt;SkillsFuture Credit (SFC)&lt;/a&gt; - if you haven&amp;rsquo;t yet use any SFC before, then you should have $500 worth of opening credits to boot. To claim for SFC, log in into your SkillsFuture account and go to this &lt;a href=&#34;https://www.myskillsfuture.sg/content/portal/en/training-exchange/course-directory/course-detail.html?courseReferenceNumber=NUS-200604346E-01-1006ST1CWK&#34;&gt;page&lt;/a&gt; and click on &amp;ldquo;Claim SkillsFuture Credit&amp;rdquo;. Of course you can only claim for any one semester so feel free to claim it as early as possible.&lt;/p&gt;

&lt;p&gt;You have to do this before the semester for which you want to claim for SFC, and &lt;strong&gt;before (not after)&lt;/strong&gt; your Student Bill is finalized. What happens is that once approved, SkillsFuture will credit your $500 SFC credit directly to NUS, which will then appear in your Student Bill. You then pay the balance for your tuition fees.&lt;/p&gt;

&lt;h4 id=&#34;post-secondary-education-account-psea&#34;&gt;Post-Secondary Education Account (PSEA)&lt;/h4&gt;

&lt;p&gt;As a Singaporean student, the PSEA account is created when you turn 16, and balance from your Edusave account will be transferred to the PSEA account. Like the Edusave account, the PSEA can be used for education purposes. In addition, with the PSEA account, you get an interest of 2.5% per annum, and this account will be held until you are 30 years old - after which the balance will be transferred into your CPF-OA&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Don-t-ask-me-why&#34;&gt;&lt;a href=&#34;#fn:Don-t-ask-me-why&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;To use the PSEA for your MSc, go to this NUS Student Service &lt;a href=&#34;https://www.askstudentservice.nus.edu.sg/app/answers/detail/a_id/2425/related/1&#34;&gt;page&lt;/a&gt; and complete the Standing Order (SO) form, then submit it accordingly.&lt;/p&gt;

&lt;h4 id=&#34;income-tax-course-fees-relief&#34;&gt;Income Tax Course Fees Relief&lt;/h4&gt;

&lt;p&gt;Finally, this course is also eligible for Course Fees Relief for your Income Tax, up to a maximum of $5,500 per year. This &lt;a href=&#34;https://www.iras.gov.sg/IRASHome/Individuals/Locals/Working-Out-Your-Taxes/Deductions-for-Individuals/Course-Fees-Relief/&#34;&gt;page&lt;/a&gt; on the IRAS website spells out all the necessary details. Like most other reliefs, you can claim the Course Fees Relief by submitting it in your annual tax assessment - so that means claiming 2 semesters at a time. Of course, this only applies if you are an employed part-time student.&lt;/p&gt;

&lt;h3 id=&#34;how-i-went-about-my-life-while-being-a-part-time-student&#34;&gt;How I went about my life while being a part-time student&lt;/h3&gt;

&lt;p&gt;My time as a part-time student (2.5 years in total&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Throughout-the-2&#34;&gt;&lt;a href=&#34;#fn:Throughout-the-2&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;) basically flew by, simply because of the packed schedules and constant back and forth between work and school. Since most lectures happened between 7pm to 10pm on weeknights, on lecture nights I would (gladly) leave work on time or early and make my way to school for dinner and lecture. I tried very much to not skip any lectures regardless of lecture recordings, but this proved to be occasionally impossible. And there were nights where I was simply too exhausted to go to school and sit for 3 hours to absorb content. While most of the time I would take MRT/bus to school, sometimes I would splurge a little and take a Grab. It was always good to reach school earlier, so that I can take my time with my dinner and enjoy the cheap Science canteen food and a cup of coffee.&lt;/p&gt;

&lt;p&gt;On most non-lecture nights I typically don&amp;rsquo;t touch any of my schoolwork - but I would dedicate one day of my weekend (usually the Sunday) to catch up on lectures and work on tutorials and assignments. During these days, I would spend the day in Utown and sort of &amp;ldquo;blend in&amp;rdquo; with the other undergraduates. If it&amp;rsquo;s not at Utown, then I would either spend the day in the Medical library or the Science library. Either way, there is still plenty of cheap food and coffee in school to replenish myself throughout my mugging.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_utown.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Utown (July 2017)
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Of course, sometimes one day simply isn&amp;rsquo;t enough so it would spill over to the other weeknights from time to time. Even as a part-time student, Recess Week was always great as it means no need to travel to school, no new content, and more time to catch up, and of course prepare for the mid-term exam or assignment.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-10_exam_schedule.PNG&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Final exam schedule (May 2018)
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;When it comes to the final exams, my protocol has always been to take roughly about 1.5 weeks of paid or study leave to prepare for the 2 final exams I have per semester. This was always a good break from work&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Studying-to-prep&#34;&gt;&lt;a href=&#34;#fn:Studying-to-prep&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;, as I typically don&amp;rsquo;t spend that much time away from work&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Yes-I-am-a-littl&#34;&gt;&lt;a href=&#34;#fn:Yes-I-am-a-littl&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. I might also take one extra day of leave after the last paper to just relax and &amp;ldquo;celebrate&amp;rdquo; the fact that I finished yet another semester, before going back to work.&lt;/p&gt;

&lt;p&gt;In all, it was a rewarding experience and I am very glad that I took the plunge to commit to the MSc for the 5 semesters. It was great being a student again, having blocks of time during weeknights and weekends focusing on nothing else but the content on my lecture notes and assignments. I guess for those of you who have been in the workforce for a while now and would like a change of pace or a break in stagnancy, going back to school is definitely an option, be it full-time or part-time. In any case, I hope this post was as fun for you to read as it was for me to write. Thanks for reading!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:I-believe-the-co&#34;&gt;I believe the content in this module became more applied and less theoretical after the renaming. I took it before the renaming. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:I-believe-the-co&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Though-later-on&#34;&gt;Though later on I realise that in the MSc programme, fulfilling pre-requisites is understandably loosely followed. It&amp;rsquo;s OK to take 5201 beyond your first semester. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Though-later-on&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:What-I-did-was-t&#34;&gt;What I did was that I would spend the whole Saturday morning to rest and get ready and then have lunch in school. After the lecture, I would then head over to VivoCity for dinner, coffee, and then do a bit more of work or studying. It was rather therapeutic. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:What-I-did-was-t&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Other-than-copul&#34;&gt;Other than &lt;a href=&#34;https://en.wikipedia.org/wiki/Copula_(probability_theory)&#34;&gt;copulae&lt;/a&gt;. I had a tough time understanding and appreciating the concept of a copula, and happened to find this &lt;a href=&#34;https://twiecki.io/blog/2018/05/03/copulas/&#34;&gt;blog post&lt;/a&gt; do an expert job at demystifying it. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Other-than-copul&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Don-t-ask-me-why&#34;&gt;Don&amp;rsquo;t ask me why this Edusave -&amp;gt; PSEA -&amp;gt; CPF-OA transferring exists. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Don-t-ask-me-why&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Throughout-the-2&#34;&gt;Throughout the 2.5 years, I had in fact switched jobs twice (another story for another time). Guess this didn&amp;rsquo;t really affect anything for my studying, other than going to school from different workplaces, and figuring out different printer settings in 3 offices to print my lecture notes. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Throughout-the-2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Studying-to-prep&#34;&gt;Studying to prepare for final exams is fun, but writing those &amp;ldquo;cheatsheets&amp;rdquo; definitely is not. They were a bane. If you don&amp;rsquo;t know what &amp;ldquo;cheatsheets&amp;rdquo; are, good for you. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Studying-to-prep&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Yes-I-am-a-littl&#34;&gt;Yes, I am a little bit of a workaholic. Just a bit. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Yes-I-am-a-littl&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Declining interest rates amidst Covid-19 - and a hidden star</title>
      <link>/post/2020/05/07/declining-interest-rates-amidst-covid-19-and-a-hidden-star/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/05/07/declining-interest-rates-amidst-covid-19-and-a-hidden-star/</guid>
      <description>

&lt;p&gt;As Covid-19 continues to wreck havoc in many countries around the world, including the US, massive sell-offs have taken place in Wall Street and other markets.&lt;/p&gt;

&lt;p&gt;In a move to protect the US economy from the effects of the pandemic, the US Federal Reserve has cut interest rates twice during March 2020 - &lt;a href=&#34;https://www.channelnewsasia.com/news/business/in-an-emergency-move-us-federal-reserve-cuts-interest-rates-to-12496552&#34;&gt;first in early March&lt;/a&gt;, cutting rates by 50 basis points to a targeted range of 1% to 1.25%, then &lt;a href=&#34;https://www.straitstimes.com/business/economy/us-federal-reserve-cuts-interest-rates-to-near-zero-coordinates-with-other-central&#34;&gt;another cut in mid March&lt;/a&gt; to a targeted range of 0% to 0.25%. At the same time, a &lt;a href=&#34;https://www.cnbc.com/2020/03/15/federal-reserve-cuts-rates-to-zero-and-launches-massive-700-billion-quantitative-easing-program.html&#34;&gt;quantitative easing&lt;/a&gt; (QE) programme was also launched, entailing US$700 billion worth of asset purchases comprising US Treasury bonds and mortgage-backed securities, i.e. the Fed buys these securities from the market, and basically inject liquidity back into the market.&lt;/p&gt;

&lt;p&gt;The last time the Fed has taken steps similiar to this, &lt;a href=&#34;https://www.nytimes.com/2019/07/31/business/economy/federal-reserve-interest-rate-cut.html&#34;&gt;was during the 08 Global Financial Crisis&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;impact-on-singapore-and-ordinary-singaporeans&#34;&gt;Impact on Singapore and ordinary Singaporeans&lt;/h3&gt;

&lt;p&gt;Inevitably, interest rates in Singapore (SIBOR, SOR) have always been closely tied to US Fed rates, and hence these rates have been falling as well. For retail investors and lay Singaporeans like myself, there are several effects.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Falling mortgage rates - housing loans will be cheaper, and refinancing may be viable if it&amp;rsquo;s an option for you.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.straitstimes.com/business/companies-markets/some-singapore-retail-investors-using-cheap-cash-to-load-up-on-stocks&#34;&gt;Or, you can even go on leverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Local banks and the STI suffer - our favorite DBS, OCBC, and UOB will have their Net Interest Margin (NIM) decrease, &lt;a href=&#34;https://www.businesstimes.com.sg/companies-markets/analysts-downgrade-singapore-banks-after-us-fed-rate-cut&#34;&gt;affecting their earnings&lt;/a&gt;. And with &lt;a href=&#34;https://sginvestors.io/market/sgx-share-price-performance/straits-times-index-constituents&#34;&gt;~35% of the STI&lt;/a&gt; is made up of these banks (at time of writing), the STI will drop as well. Of course the banks are not the only reason why the STI suffers.&lt;/li&gt;
&lt;li&gt;Deposit account rates fall - likewise, our favorite DBS Multiplier, OCBC 360, UOB One, and the likes will see a &lt;a href=&#34;https://www.straitstimes.com/business/banking/banks-here-cut-deposit-rates-in-line-with-global-markets&#34;&gt;cut in interest rates&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Singapore Government Securities rates fall - additionally, SGS bonds and SSBs will also see a decrease in yield.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;my-three-main-cash-stashes-uob-one-singapore-savings-bond-and-cimb-fastsaver&#34;&gt;My three main cash stashes: UOB One, Singapore Savings Bond, and CIMB FastSaver&lt;/h3&gt;

&lt;h4 id=&#34;uob-one&#34;&gt;UOB One&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://www.uob.com.sg/personal/save/chequeing/one-account.page&#34;&gt;UOB One&lt;/a&gt; has been my workhorse account for salary crediting, GIRO, credit card spend etc for several months now. The effective rate of the UOB One for 75K used to be 2.436%, if you meet the bonus interest requirements. Effective 1st May 2020, here is what it looks like:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-07%20uob%20one.png&#34; width=&#34;100%&#34;&gt;
Bonus interest structure for the UOB One account, effective 1st May 2020.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The effective interest rates is now 1.796% for 75K. On the upside, UOB has retained the existing qualifying criteria so that there are no disruptions to our savings routine. Other deposit accounts have also cut their interest rates accordingly.&lt;/p&gt;

&lt;h4 id=&#34;singapore-savings-bond&#34;&gt;Singapore Savings Bond&lt;/h4&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-07%20BT%205th%20May%202020%20SSB%20FD%20mortgage%20rates%201.PNG&#34; width=&#34;100%&#34;&gt;
Article from The Business Times 5th May 2020.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;SSB rates have fallen significantly over the past months, since early 2019. The June 2020 issue of SSB has a first year yield of 0.57% (10-year yield average of 1.05%) - the lowest it has ever been since the launch of SSBs in 2015.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://investmentmoats.com/wp-content/uploads/2020/05/20200506-Singapore-Savings-Bond-1.png&#34; width=&#34;100%&#34;&gt;
SSB first year and 10-year yields. &lt;a href=&#34;https://investmentmoats.com/saving-and-investing-my-money/singapore-savings-bonds-ssb-june-2020/&#34;&gt;Source&lt;/a&gt;.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Of course, SSBs are issued and guaranteed by the Singapore government. Nonetheless, the current state of SSBs, with a first year yield of 0.57% and 10-year average of 1.05%, is a shadow of its former self.&lt;/p&gt;

&lt;h4 id=&#34;cimb-fastsaver-the-hidden-star&#34;&gt;CIMB FastSaver - the hidden star&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cimbbank.com.sg/en/personal/products/accounts/savings-accounts/cimb-fastsaver-account.html&#34;&gt;CIMB FastSaver&lt;/a&gt; is my third cash stash. Unlike the typical high-interest workhorse accounts like DBS Multiplier or OCBC 360, the CIMB FastSaver does not have any special requirements of salary crediting, credit card spend, bill payments etc in order to be granted an interest rate of 1% (for first 50K). Considering there is no special requirements, 1% is definitely on the high side, especially in comparison to SSB and current FD rates:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-07%20BT%205th%20May%202020%20SSB%20FD%20mortgage%20rates%202.PNG&#34; width=&#34;60%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Article from The Business Times 5th May 2020.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Compare that with the CIMB FastSaver interest rates, as a deposit account:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/images/2020-05-07%20cimb%20fastsaver.PNG&#34; width=&#34;100%&#34;&gt;
CIMB FastSaver interest structure.
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;As of the time of writing this, there doesn&amp;rsquo;t seem to be any indications from CIMB to adjust the interest rate of the FastSaver account. I sure hope I didn&amp;rsquo;t jinxed it and it stays this way! Instead of buying any more SSBs, I will place spilled over cash from my UOB One into CIMB FastSaver for now - but I guess I won&amp;rsquo;t be surprised if there is going to be an adjustment to the CIMB FastSaver rates, putting out the light from the hidden star.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A gentle introduction to the modern portfolio theory</title>
      <link>/post/2020/04/10/a-gentle-introduction-to-the-modern-portfolio-theory/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/04/10/a-gentle-introduction-to-the-modern-portfolio-theory/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-formulation&#34;&gt;Problem formulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#portfolio-weights&#34;&gt;Portfolio weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#portfolio-expectation-and-variance&#34;&gt;Portfolio expectation and variance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-math-behind-diversification&#34;&gt;The math behind diversification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#two-uncorrelated-assets&#34;&gt;Two uncorrelated assets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-correlated-assets&#34;&gt;Two correlated assets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#starting-with-two-assets&#34;&gt;Starting with two assets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#efficient-portfolios&#34;&gt;Efficient Portfolios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-variance-portfolio&#34;&gt;Minimum variance portfolio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#risk-free-assets&#34;&gt;Risk-free assets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#one-risky-asset-and-one-risk-free-asset&#34;&gt;One “risky” asset and one risk-free asset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-risky-assets-and-one-risk-free-asset&#34;&gt;Two “risky” assets and one risk-free asset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I would like to talk about modern portfolio theory (MPT), a key topic in finance. The first couple of lines in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Modern_portfolio_theory&#34;&gt;MPT Wikipedia article&lt;/a&gt; explains it very well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(MPT) is a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. It is a formalization and extension of diversification in investing, the idea that owning different kinds of financial assets is less risky than owning only one type. Its key insight is that an asset’s risk and return should not be assessed by itself, but by how it contributes to a portfolio’s overall risk and return. It uses the variance of asset prices as a proxy for risk.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The objective of the MPT is to simultaneously maximise the returns of a portfolio, while at the same time, minimise the risk of the portfolio. Generally, we all know that higher returns come with higher risk, and therefore the dual goals of maximising returns while minimising risk are at odds with each other. After all, why would an investor place himself in a riskier position without any due reward or &lt;strong&gt;risk premium&lt;/strong&gt;? It wouldn’t make sense. Note that in this and many statistical finance frameworks, risk is often measured by means of standard deviation of returns, either that of the asset or the portfolio&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a “gentle introduction” to MPT, in this post, all you need is the basic understanding of mean, variance, and covariance. I will do the rest by repeatedly applying these three statistics in the context of a portfolio - nothing more, nothing less!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Risk premium&lt;/em&gt;&lt;/strong&gt; refers to the difference between the expected return between a risky asset, like a stock, and that of something that basically has no risk, like cash, Treasury Bills or Singapore Savings Bonds. Without any risk premium, there’s no reason to invest in anything other than risk-free assets.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem formulation&lt;/h2&gt;
&lt;p&gt;Suppose there are &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; assets with &lt;span class=&#34;math inline&#34;&gt;\(R_i, i = 1, ..., N\)&lt;/span&gt; denoting the random variables that represent their respective returns in a given time period, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R} = (R_1, ..., R_N)^{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Their returns and risks are respectively&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
r_i = ER_i \\\
\sigma_i = Var(R_i)\\\
cov(\mathbf{R}) = \Sigma = (\sigma_{ij})_{1 \le i,j \le N}
\end{array}
\]&lt;/span&gt;
We can then form a portfolio consisting of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; assets such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
R_{pf} = w_1R_1 + ... + w_NR_N = w^T\mathbf{R},
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(R_{pf}\)&lt;/span&gt; is the return of the portfolio and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; is the weight of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset in the portfolio, with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i = 1}^{N} w_i = 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In other words, the portfolio is the linear combination of the various assets under consideration. Of course, the question that becomes: how to choose &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w} = (w_1, ..., w_N)^T\)&lt;/span&gt; such that expected returns are maximised while minimising risk?&lt;/p&gt;
&lt;div id=&#34;portfolio-weights&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Portfolio weights&lt;/h3&gt;
&lt;p&gt;A technical point to bring out here is that while &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i = 1}^{N} w_i = 1\)&lt;/span&gt;, there is no constraint on individual &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; to be non-negative. In fact, a negative &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; implies a short position in the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset, i.e. &lt;strong&gt;short selling&lt;/strong&gt;. This is in constrast to a &lt;em&gt;long&lt;/em&gt; position, i.e. buying the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th asset. In this post, we will only consider positive &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; only.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Holding a &lt;strong&gt;&lt;em&gt;long&lt;/em&gt;&lt;/strong&gt; position in a particular asset simply means buying that particular asset. On the other hand, &lt;strong&gt;&lt;em&gt;short selling&lt;/em&gt;&lt;/strong&gt; is where one sells an asset without owning it in the first place. The asset, e.g. a stock, is borrowed from a broker or another customer of the broker. At a later point in time, a stock must then be bought back from the market and then returned to the lender. This closes the short position, and the idea is that if one is able to sell the borrowed stock at a higher price and return it at a lower price, a profit is made.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;portfolio-expectation-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Portfolio expectation and variance&lt;/h3&gt;
&lt;p&gt;With this set-up, the expectation and variance of the portfolio would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ER_{pf} = \sum_iw_iER_i = \mathbf{w}^TE\mathbf{R}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = \sum_iw_i^2\sigma_i^2 + \sum_i\sum_{j\neq i}w_iw_j\sigma_{ij}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ij}\)&lt;/span&gt; is the covariance between the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th asset.&lt;/p&gt;
&lt;p&gt;Alternatively,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = \sum_i\sum_jw_iw_j\sigma_{ij} = \mathbf{w}_T\Sigma \mathbf{w}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-math-behind-diversification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The math behind diversification&lt;/h2&gt;
&lt;p&gt;We all know “not to put our eggs in one basket”, and have a diversified portfolio. Intutitively, we know that if we were to put all our money on a single stock, then we have placed a large bet on that one company. While we could instantly make it big just by having our one stock making it big, &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_aversion&#34;&gt;loss aversion&lt;/a&gt; would dictate that we would rather minimise the risk of losing it all in a single stock.&lt;/p&gt;
&lt;p&gt;In this sense, what diversification does to a portfolio is that it minimises risk of the portfolio. While the mean return of a portfolio depends on the mean returns of the individual assets and their respective weights, the risk of the portfolio depends on both the risk of the individual assets, as well as each asset’s relationships with the others, in terms of correlations.&lt;/p&gt;
&lt;p&gt;This means that placing a right mix of weights and assets would reduce the risk of the portfolio - a fundamental idea in portfolio theory. Diversification, by way of investing in multiple assets, reduces risks.&lt;/p&gt;
&lt;p&gt;Let’s consider two instructive examples.&lt;/p&gt;
&lt;div id=&#34;two-uncorrelated-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two uncorrelated assets&lt;/h3&gt;
&lt;p&gt;Suppose we have 2 assets with returns &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;, with the same mean and variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{}
ER_1 = ER_2 = \mu \\\
Var(R_1) = Var(R_2) = \sigma^2
\end{array}
\]&lt;/span&gt;
Also, &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt; are uncorrelated, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Investing 100% in either &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt; would yield exactly the same return and the same risk. Now consider a portfiolio with some weight on both assets. Let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; be the weight for &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; - then we would have &lt;span class=&#34;math inline&#34;&gt;\(1-w\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. In this portfolio, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ER_{pf} = wER_1 + (1-w)ER_2 = w\mu + (1-w)\mu = \mu\]&lt;/span&gt;
With this, we know that the return of this portfolio does not depend on &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, given that the assets are uncorrelated, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = w^2Var(R_1) + (1-w)^2Var(R_2) + cov(R_1, R_2)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(cov(R_1, R_2) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(R_{pf}) = (w^2 + (1-w)^2)\sigma^2\]&lt;/span&gt;
When &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, we would have &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) = \sigma^2\)&lt;/span&gt;. However, for any other value of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;w&amp;lt;1\)&lt;/span&gt;, we would have &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) &amp;lt; \sigma^2\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-correlated-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two correlated assets&lt;/h3&gt;
&lt;p&gt;Let’s consider a similar scenario, but this time, &lt;span class=&#34;math inline&#34;&gt;\(cov(R_1, R_2) \neq 0\)&lt;/span&gt;. This also means that &lt;span class=&#34;math inline&#34;&gt;\(-1&amp;lt; \rho_{12} &amp;lt; 1\)&lt;/span&gt;. Consider a portfolio where &lt;span class=&#34;math inline&#34;&gt;\(w = \frac{1}{2}\)&lt;/span&gt;, i.e. placing equal weight on both assets &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. We would then have &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf} = \frac{1}{2}ER_1 + \frac{1}{2}ER_2 = \mu\)&lt;/span&gt;. More importantly,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(R_{pf}) = \frac{1}{4}Var(R_1) + \frac{1}{4}Var(R_2) + 2\frac{1}{2}\frac{1}{2}Cov(R_1, R_2) \\\
= \frac{1}{4}\sigma^2 + \frac{1}{4}\sigma^2 + \frac{1}{2}\rho_{12}\sigma^2 \\\
= \frac{1}{2}(1+\rho_{12})\sigma^2
\]&lt;/span&gt;
Therefore, for any value of &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(-1 &amp;lt; \rho_{12} &amp;lt; 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) &amp;lt; \sigma^2\)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-with-two-assets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starting with two assets&lt;/h2&gt;
&lt;p&gt;Again, the objective of treating portfolio selection as a statistical problem is to select individual &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; is large and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; is small. As with our two instructive examples above, we can start solving this problem by first considering &lt;span class=&#34;math inline&#34;&gt;\(N = 2\)&lt;/span&gt;, that is, choosing the weights &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; between two assets only. Inevitably, we would have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{c}
ER_{pf} = \sum_iw_iER_i = w_1ER_1 + w_2ER_2 \\\
Var(R_{pf}) = \sum_iw_i^2\sigma_i^2 + \sum_i\sum_{j\neq i}w_iw_jcov(R_1, R_2) = w_1^2\sigma_1^2 + w_2^2\sigma_2^2 + w_1w_2\rho_{12}\sigma_1\sigma_2 \\\
w_1+w_2 = 1, w_2 = 1 - w_1
\end{array}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt; is the correlation between &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. Again, we can let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; be the weight for &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; - then we would have &lt;span class=&#34;math inline&#34;&gt;\(1-w\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(R_2\)&lt;/span&gt;. Simplifying,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
ER_{pf} = wER_1 + (1-w)ER_2 = w\mu_1 + (1-w)\mu_2 \\\
Var(R_{pf}) = w^2Var(R_1) + (1-w)^2Var(R_2) + 2w_1w_2cov(R_1, R_2) \\\
= w^2\sigma_1^2+ (1-w)^2\sigma_2^2 + 2w(1-w)\rho_{12}\sigma_1\sigma_2
\]&lt;/span&gt;
Notice that in this set-up, &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt; are considered to be known and constant&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. We are concerned with choosing a value for &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; is maximised, while &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; is minimised, i.e. we treat both &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; as univariate functions of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s visualise this with a numerical example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# expected returns, standard deviation, and correlation of two assets
mu1 &amp;lt;- 0.2
mu2 &amp;lt;- 0.1
sigma1 &amp;lt;- 0.1
sigma2 &amp;lt;- 0.05
rho12 &amp;lt;- 0.25

# calculate return and risks based on w
cal_pf_return &amp;lt;- function(w){
  return(w*mu1 + (1-w)*mu2)
}
cal_pf_risk &amp;lt;- function(w){
  return(w^2*sigma1^2 + (1-w)^2*sigma2^2 + 2*w*(1-w)*rho12*sigma1*sigma2)
}

# weights span from -1 to 1; this considers short positions as well for illustration
weights &amp;lt;- seq(-1, 1, by = 0.01)

returns &amp;lt;- NULL
risks &amp;lt;- NULL

for(w in weights){
  returns &amp;lt;- c(returns, cal_pf_return(w))
  risks &amp;lt;- c(risks, cal_pf_risk(w))
}

plot(risks, returns,
     xlab = &amp;quot;Portfolio risk&amp;quot;,
     ylab = &amp;quot;Portfolio return&amp;quot;,
     main = &amp;quot;Risk-return relationship across weights&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-10-a-gentle-introduction-to-the-modern-portfolio-theory_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;efficient-portfolios&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Efficient Portfolios&lt;/h3&gt;
&lt;p&gt;Take a look at the plot above. Notice that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For every possible value of return, there is one corresponding value of risk.&lt;/li&gt;
&lt;li&gt;However, for every possible value of risk, there are two corresponding values of returns.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Naturally, for a given amount of risk that we would like to take on, we would want a higher return than a lower one. Therefore, any combination of risk-return that belongs to the top half of the plot would be portfolios that we would want, as compared to the lower half. In particular, each combination in the top half of the plot are considered as &lt;strong&gt;efficient portfolios&lt;/strong&gt;. At this point, we might also want to know which is the portfolio or weight that has the lowest risks. This portfolio is also known as the &lt;strong&gt;minimum variance portfolio&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Efficient portfolios&lt;/em&gt;&lt;/strong&gt; lie on on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Efficient_frontier&#34;&gt;&lt;strong&gt;efficient frontier&lt;/strong&gt;&lt;/a&gt;, the top half of our plot. By definition, the efficient frontier consists of portfolios such that for each of these portfolios and their respective returns, there exists no other portfolios that carries a lower portfolio risk for a given return (hence “efficient”&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;). The &lt;strong&gt;minimum variance portfolio&lt;/strong&gt; is one special case of an efficient portfolio - no other portfolios carries a lower portfolio risk than itself, and is the leftmost point of our plot.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min_risk_w &amp;lt;- weights[which(risks == min(risks))]
min_risk_return &amp;lt;- mean(cal_pf_return(min_risk_w))

efficient_idx &amp;lt;- which(returns &amp;gt;= min_risk_return)
inefficient_idx &amp;lt;- which(returns &amp;lt; min_risk_return)

plot(risks[efficient_idx], returns[efficient_idx],
     xlab = &amp;quot;Portfolio risk&amp;quot;,
     ylab = &amp;quot;Portfolio return&amp;quot;,
     main = &amp;quot;The Efficient Frontier&amp;quot;,
     ylim = c(0,0.2), xlim = c(0.002, 0.01))

lines(risks[inefficient_idx], returns[inefficient_idx], lty = &amp;quot;dotted&amp;quot;)

abline(h = min_risk_return)
text(x = 0.003, y = min_risk_return-0.01, &amp;quot;min variance portfolio&amp;quot;)
text(x = 0.006, y = 0.15, &amp;quot;efficient frontier&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-10-a-gentle-introduction-to-the-modern-portfolio-theory_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-variance-portfolio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Minimum variance portfolio&lt;/h3&gt;
&lt;p&gt;In the example above, we arrived at the minimum variance portfolio by doing some simple calculations. A more robust method would be arrive at the minimum variance portfolio analytically by treating the problem as a optimization problem, minimizing &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt;. This can be done by simply solving some basic calculus.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(R_{pf}) = w^2\sigma_1^2+ (1-w)^2\sigma_2^2 + 2w(1-w)\rho_{12}\sigma_1\sigma_2 \\
\frac{dVar(R_{pf})}{dw} = 2w\sigma_1^2 - 2(1-w)\sigma_2^2 + 2(1-2w)\rho_{12}\sigma_1\sigma_2 \\
\]&lt;/span&gt;
Also, since&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{d^2Var(R_{pf})}{dw^2} = 2\sigma_1^2 + 2\sigma_2^2 - 4\rho_{12}\sigma_1\sigma_2
\geqslant 2\sigma_1^2 + 2\sigma_2^2 - 4\sigma_1\sigma_2 = 2(\sigma_1 - \sigma_2)^2 \geqslant 0 
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} &amp;lt; 1\)&lt;/span&gt;. We would have &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2Var(R_{pf})}{dw^2} &amp;gt; 0\)&lt;/span&gt;, i.e. solving &lt;span class=&#34;math inline&#34;&gt;\(\frac{dVar(R_{pf})}{dw} = 0\)&lt;/span&gt; minimizes &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt;, giving the minimium variance portfolio.&lt;/p&gt;
&lt;p&gt;Solving,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
w \equiv w_{minvar} = \frac{\sigma_2^2 - \rho_{12}\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 - 2\rho_{12}\sigma_1\sigma_2}
\]&lt;/span&gt;
and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1 - w_{minvar} = \frac{\sigma_1^2 - \rho_{12}\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 - 2\rho_{12}\sigma_1\sigma_2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;risk-free-assets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Risk-free assets&lt;/h2&gt;
&lt;p&gt;Now let’s make this a little more interesting than just 2 assets. Since it’s always possible to not invest all our capital in these assets, and in fact leave a portion of capital in something that is &lt;strong&gt;risk-free&lt;/strong&gt;, such as cash, we could include risk-free assets as part of our framework as well. In particular, we could have &lt;span class=&#34;math inline&#34;&gt;\(w_1 + w_2 &amp;lt; 1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(1 - w_1 - w_2\)&lt;/span&gt; being the portion not invested, but instead diverted to risk-free assets. Doing this directly reduces portfolio variance, and of course reduces portfolio return.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Terminology&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As pointed out above, &lt;strong&gt;risk-free assets&lt;/strong&gt; are simply assets that are generally deemed as 100% safe&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, such as cash in a deposit account&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;, Treasury Bills, or Singapore Savings Bonds. Typically, the three-month &lt;a href=&#34;https://www.treasury.gov/resource-center/data-chart-center/interest-rates/pages/textview.aspx?data=yield&#34;&gt;US Treasury Bills&lt;/a&gt; are used as proxies for risk-free assets, and their yield or return are considered accordingly. In context of Singapore, we could consider the yield of SSBs as risk-free returns.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(R_{free}\)&lt;/span&gt; be the random variable representing the return of a risk-free asset. Then we have &lt;span class=&#34;math inline&#34;&gt;\(ER_{free} = \mu_{free}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{free}) = 0\)&lt;/span&gt;&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. Naturally, &lt;span class=&#34;math inline&#34;&gt;\(ER_{free}\)&lt;/span&gt; would be small and contributes a small return to portfolio. On the other hand, directly adjusting the &lt;span class=&#34;math inline&#34;&gt;\(1 - w_1 - w_2\)&lt;/span&gt; provides a convenient of controlling portfolio variance.&lt;/p&gt;
&lt;p&gt;Also, note that since &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{free}) = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\forall i cov(R_{free}, R_i) = 0\)&lt;/span&gt;, i.e. the risk-free is not correlated with any other “risky” assets&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;one-risky-asset-and-one-risk-free-asset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One “risky” asset and one risk-free asset&lt;/h3&gt;
&lt;p&gt;Now let’s consider combining a “risky” asset with a risk-free one. Let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; be the weight invested in the risky asset, with its return being &lt;span class=&#34;math inline&#34;&gt;\(ER\)&lt;/span&gt; and variance being &lt;span class=&#34;math inline&#34;&gt;\(Var(R)\)&lt;/span&gt;. The portfolio would look like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
ER_{pf} = wER + (1-w)R_{free} \implies \mu_{pf} = w\mu_R + (1-w)\mu_{free} \\
Var(R_{pf}) = w^2Var(R) \implies \sigma_{pf} = w\sigma_R
\]&lt;/span&gt;
In particular, observe that the portfolio variance is now a function of &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, i.e. solving &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf}) = w^2Var(R)\)&lt;/span&gt; gives &lt;span class=&#34;math inline&#34;&gt;\(w = \frac{\sigma_{pf}}{\sigma_R}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{pf}\)&lt;/span&gt; is the standard deviation of the portfolio and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_R\)&lt;/span&gt; is the standard deviation of the risky asset.&lt;/p&gt;
&lt;p&gt;This gives the following relationship between the portfolio return &lt;span class=&#34;math inline&#34;&gt;\(\mu_{pf}\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{pf}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu_{pf} = \mu_{free} + \frac{\mu_R - \mu_{free}}{\sigma_R}\sigma_{pf}
\]&lt;/span&gt;
Again, let’s visualize this relationship with a numerical example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# expected returns of both &amp;quot;risky&amp;quot; and risk-free asset, variance
mu_R    &amp;lt;- 0.2
mu_free &amp;lt;- 0.04
sigma_R &amp;lt;- 0.1

# calculate return and risks based on w
cal_pf_return &amp;lt;- function(w){
  return(w*mu_R + (1-w)*mu_free)
}
cal_pf_sd &amp;lt;- function(w){ # standard deviation, not variance
  return(w*sigma_R)
}

weights &amp;lt;- seq(0, 1, by = 0.01)

returns &amp;lt;- NULL
risks &amp;lt;- NULL

for(w in weights){
  returns &amp;lt;- c(returns, cal_pf_return(w))
  risks &amp;lt;- c(risks, cal_pf_sd(w))
}

plot(risks, returns,
     xlab = &amp;quot;Portfolio sd&amp;quot;,
     ylab = &amp;quot;Portfolio return&amp;quot;,
     main = &amp;quot;Risk-return relationship across weights&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-10-a-gentle-introduction-to-the-modern-portfolio-theory_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-risky-assets-and-one-risk-free-asset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two “risky” assets and one risk-free asset&lt;/h3&gt;
&lt;p&gt;Now, let’s go one step further, and look at having two “risky” assets with one risk-free asset. What does the math look like? Again, Let &lt;span class=&#34;math inline&#34;&gt;\(w_1\)&lt;/span&gt; be the weight invested in the first risky asset, &lt;span class=&#34;math inline&#34;&gt;\(w_2\)&lt;/span&gt; in the second risky asset and &lt;span class=&#34;math inline&#34;&gt;\(w_{free}\)&lt;/span&gt; in the risk-free asset, with respective returns being &lt;span class=&#34;math inline&#34;&gt;\(ER_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(ER_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(ER_{free}\)&lt;/span&gt;, and variances being &lt;span class=&#34;math inline&#34;&gt;\(Var(R_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var(R_2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since there are now two risky assets, we also need to consider their correlation, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12}\)&lt;/span&gt;. The portfolio would look like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
ER_{pf} = w_1ER_1 + w_2ER_2 + (1-w_1-w_2)R_{free} \implies \mu_{pf} = w_1\mu_{R_1} + w_2\mu_{R_2} + (1-w_1-w_2)\mu_{free} \\
Var(R_{pf}) = w_1^2Var(R_1) + w_2^2Var(R_2) + 2w_1w_2cov(R_1, R_2) = w_1^2\sigma_1^2 + w_2^2\sigma_2^2 + 2w_1w_2\rho_{12}\sigma_1\sigma_2 \\
w_{free} = 1-w_1-w_2
\]&lt;/span&gt;
In particular, note that we can write &lt;span class=&#34;math inline&#34;&gt;\(ER_{pf}\)&lt;/span&gt; in the following manner as well:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
ER_{pf} = (1-w_{free})(\bar{w_1}R_1 + \bar{w_2}R_2) + w_{free}R_{free} \\
\bar{w_1} = \frac{w_1}{1-w_{free}}, \bar{w_2} = \frac{w_2}{1-w_{free}} \implies \bar{w_1} + \bar{w_2} = 1
\]&lt;/span&gt;
In this sense, we can now view portfolio selection as a two-stage problem. First, we construct a portfolio based on risky assets only (e.g. chooosing &lt;span class=&#34;math inline&#34;&gt;\(\bar{w_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{w_2}\)&lt;/span&gt; only). Then, we include the risk-free asset by choosing &lt;span class=&#34;math inline&#34;&gt;\(w_{free}\)&lt;/span&gt;. From the perspective, we gain an interesting tool in controlling for the risk we take on in our portfolio, by the inclusion of the risk-free asset - since its inclusion always decreases the portfolio risk, regardless of the risky assets we take on.&lt;/p&gt;
&lt;p&gt;Also note that once the first stage is completed, we are effectively dealing with one risky asset and one risk-free asset again, with the “single risky asset” being a combination of 2 risky assets, with &lt;span class=&#34;math inline&#34;&gt;\(\bar{w_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{w_2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, the scenario of using two risky assets and one risk-free asset simplifies that of using one risky asset and one risk-free asset! This results also generalises to &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; assets.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I have attempted to give a brief and hopefully gentle enough introduction to the Modern Portfolio Theory, using only statistics like mean, variance, and covariance. We looked at the math behind diversification, computed the Efficient Frontier, and generalised the problem from &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt; assets to &lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;2\)&lt;/span&gt; assets.&lt;/p&gt;
&lt;p&gt;If you are interested to go a bit further, you might want to find out more about the &lt;a href=&#34;https://pgpfm.wordpress.com/tag/tangency-portfolio/&#34;&gt;Tangency Portfolio&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sharpe_ratio&#34;&gt;Sharpe Ratio&lt;/a&gt; - both further extends our analysis and this post would have given you sufficient understanding for further exploration.&lt;/p&gt;
&lt;p&gt;That’s all from me today, thank you for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In my personal opinion, using standard deviation as a measure of risk is a little iffy, for several reasons. For example, risk could be better represented as a probability of huge losses, rather than the sd of expected returns - but let’s stick to this for now.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In particular, choosing &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; minimizes &lt;span class=&#34;math inline&#34;&gt;\(Var(R_{pf})\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma}{\sqrt{2}}\)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;It’s commonly said that stocks and bonds are considered to be negatively correlated. Assuming this is true, consider the two subdomains &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} &amp;lt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho_{12} &amp;gt; 0\)&lt;/span&gt;. We have &lt;span class=&#34;math inline&#34;&gt;\(\forall\rho_{12} &amp;lt; 0, Var(R_{pf}) &amp;lt; \frac{1}{2}\sigma^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\forall\rho_{12} &amp;gt; 0, Var(R_{pf}) &amp;gt; \frac{1}{2}\sigma^2\)&lt;/span&gt;, i.e. portfolio risk reduces when you have two negatively correlated assets in the portfolio - an intuitive result and justification for proper diversification and &lt;a href=&#34;https://www.investopedia.com/terms/s/strategicassetallocation.asp&#34;&gt;strategic asset allocation&lt;/a&gt;. This of course begs the question on whether stocks and bonds are indeed negatively correlated, in good economic times and in bad, and correlated by how much. We can explore this topic deeper in the future. Also, the math here is similar to that of &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/&#34;&gt;ensemble modelling&lt;/a&gt; in machine learning.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Of course the challenge in real life is to estimate them using historical data.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The same way an estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is &lt;a href=&#34;https://en.wikipedia.org/wiki/Efficient_estimator&#34;&gt;efficient&lt;/a&gt; if &lt;span class=&#34;math inline&#34;&gt;\(Var(\hat{\theta})\)&lt;/span&gt; is minimized.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Of course there is no such thing in real life.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;In Singapore, up to SGD75K is insured by &lt;a href=&#34;https://www.sdic.org.sg/&#34;&gt;SDIC&lt;/a&gt;.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;We could in fact use a constant instead of a random variable.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Again, this is not always true in real life. For example, we have a coronavirus pandemic decimating the return of both “risky” and risk-free assets.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>[short] Statistical methods in finance series</title>
      <link>/post/2020/03/19/short-statistical-methods-in-finance-series/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/03/19/short-statistical-methods-in-finance-series/</guid>
      <description>

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://miro.medium.com/max/1280/1*LZnClq6o0kEU88-BZnkvCw.jpeg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a sleepless night, so I decided to write a little. This will be the first part of a series that I would like to write about, circling around the use of statistical methods in finance. Most of the content that I will be exploring would originate from one of my NUS M.Sc Statistics modules, Advanced Statistical Methods in Finance. I was taught by Prof Xia Yingcun when I did this module.&lt;/p&gt;

&lt;p&gt;Of all the modules that I did during my M.Sc, I always found this one to resonate with me the most - guess that&amp;rsquo;s mainly because I like topic, as well as of the fact that I was heavily experimenting with investing on my own during that time. I also found that using finance as the backdrop or context to study certain statistical concepts, such as copula or factor analysis, to be more engaging than perhaps studying these topics in vaccum.&lt;/p&gt;

&lt;h1 id=&#34;statistical-methods-in-finance&#34;&gt;Statistical methods in finance&lt;/h1&gt;

&lt;p&gt;There are a number of statistical methods that can be directly mapped to their applications in finance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Statistical distributions: Value-at-Risk (VaR)&lt;/li&gt;
&lt;li&gt;Linear regression: Capital Asset Pricing Model (CAPM)&lt;/li&gt;
&lt;li&gt;Factor analysis: Arbitrage Pricing Theory&lt;/li&gt;
&lt;li&gt;Logit/probit models: credit scoring/rating&lt;/li&gt;
&lt;li&gt;Time series analysis: price forecast, volatility modelling&lt;/li&gt;
&lt;li&gt;Nonlinear regression: term structures of interest rates&lt;/li&gt;
&lt;li&gt;Monte-Carlo simulations: pricing of assets&lt;/li&gt;
&lt;li&gt;Copulae: tail dependence of asset prices&lt;/li&gt;
&lt;li&gt;Estimation of covariance matrix and optimization: Markowitz&amp;rsquo;s portfolio theory&lt;/li&gt;
&lt;li&gt;and many more&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this series, I would like to tackle some of these topics, by combining statistical theory, finance theory, and the use of R where relevant.&lt;/p&gt;

&lt;h1 id=&#34;broader-question-can-statistics-be-used-to-make-a-profit&#34;&gt;Broader question: can statistics be used to make a profit?&lt;/h1&gt;

&lt;p&gt;Since there are folks who decided to spend the time and energy to study the application of statistical methods in capital markets, surely it&amp;rsquo;s a worthwhile effort and making money is possible? Well, yes and no. It&amp;rsquo;s well known in statistical finance is that financial data has extremely low signal-to-noise (SNR) ratio. This means that for a model that predicts price, even if the model is correctly specified, due to the low SNR, it&amp;rsquo;s unlikely for the predictions to be useful. Most folks would hence consider the study of price volatility to be more important than price itself. If we are able to have an informed point of view about future volatility (perhaps due to the fact that volatility autocorrelation is strong =&amp;gt; volatility may be predicted in a GARCH/ARCH model), we would be able to act accordingly. We will talk about this along the way in this series.&lt;/p&gt;

&lt;h1 id=&#34;stylized-fact-in-statistical-finance&#34;&gt;Stylized fact in statistical finance&lt;/h1&gt;

&lt;p&gt;I previously (well, a year ago) wrote about &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/04/04/short-stylized-facts-in-statistical-finance/&#34;&gt;stylized facts&lt;/a&gt; in statistical finance, where I briefly listed without explanations various &amp;ldquo;facts&amp;rdquo;. Stylized facts are commonly used in statistical finance to address and summarize phenomenon directly observed from historical data, and explainable with a certain level of theoretical consistency and logic. There are a number of well-known ones, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Financial data has extremely low signal-to-noise ratio.&lt;/li&gt;
&lt;li&gt;Price changes are less volatile in bull markets and more volatile bear markets.&lt;/li&gt;
&lt;li&gt;Volatility clustering is typically observed in financial data; that is, large changes tend to be followed by large changes of either sign, and small changes tend to be followed by small changes of either sign.&lt;/li&gt;
&lt;li&gt;and others&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this series, we will necessarily also delve a little deeper into some of these stylized facts, by considering both empirical (historical data) and theoretical perspectives.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s it for a brief introduction to series - looking forward to do deep dives into various topics!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fundamental Analysis 101: A basic overview of the fundamental analysis of stocks</title>
      <link>/post/2019/06/08/fundamental-analysis-101-a-basic-overview-of-the-fundamental-analysis-of-stocks/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/06/08/fundamental-analysis-101-a-basic-overview-of-the-fundamental-analysis-of-stocks/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://frugalinvesting.files.wordpress.com/2018/07/market-analysis-1024x682.jpg?w=900&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;In this post, I would like to give an overview of the principles and concepts in the fundamental analysis of stocks.&lt;/p&gt;

&lt;h2&gt;What is fundamental analysis?&lt;/h2&gt;

&lt;p&gt;Well, fundamental analysis (FA) is the analysis of the fundamentals of a company. The fundamentals of the company are trying to capture essentially the health of the company, in the present and future. The objective of FA is to discern a point of view on the current and future prospects of a company – the actionability from this point of view is of course whether to buy (or short-sell) the stock. If our FA suggests that the company is likely to perform well in the future, we would like a piece of the action.&lt;/p&gt;

&lt;p&gt;In this sense, there is an inherent notion of valuation – based on our view of the stock, we could, for example, conclude that the stock is currently under-valued and is likely to grow to at least a fair valuation. Therefore, we buy. Nonetheless, we don’t know when our under-valued stock would grow to our fair valuation. It could take days, months or years.&lt;/p&gt;

&lt;h2&gt;The efficient market hypothesis, technical analysis, and fundamental analysis&lt;/h2&gt;

&lt;p&gt;Any current under- or over-valuation of a stock implies that the market is not valuing the stock “correctly”, and is hence inefficient. One of the dominant criticisms against FA is the efficient market hypothesis (EMH). EMH states that at any point in time, the market is efficient and security prices are exactly where they should be – every security is valued correctly, taking into account all aspects of the security and the company. For example, the resultant price plunge from a JNJ lawsuit or a Boeing plane crash is exactly reflective of the inherent value of JNJ or Boeing.&lt;/p&gt;

&lt;p&gt;Corollary to the EMH is as follows – it would be impossible to derive any form of “correct” valuation or insights based on e.g. the fundamentals of JNJ or Boeing, no matter how we analyse the fundamentals. Research has shown that the strong condition of the EMH is largely true (reference), and there are occasions where inefficiencies occur.&lt;/p&gt;

&lt;p&gt;On the other hand, proponents of technical analysis (TA) argue that price movements are driven by signals in the historical prices themselves. TA relies on e.g. trading on momentum of the stock, and similar to the EMH, suggests that the JNJ lawsuit or Boeing plane crash has already been priced into the stock, and stock prices constantly moves to capture these news and signals.&lt;/p&gt;

&lt;p&gt;Based on my current understanding on securities and prices, FA makes sense to me. As someone with scientific and statistical background, it is challenging for me to convince myself that TA is not some variant of data dredging or confirmation bias. But of course, you can always take what you will and maintain healthy skepticism.&lt;/p&gt;

&lt;h2&gt;The objective of fundamental analysis&lt;/h2&gt;

&lt;p&gt;As we mentioned above, the objective of FA is to discern a point of view on the current and future prospects of a company. The actionability from this point of view is of course whether to buy (or short-sell) the stock. More specifically, what are we interested in from an exercise of FA is to answer questions such as the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is the company actually making money?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can the company continue to make money?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can the company beat its competition in time to come?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Is the company is able to fulfil its debt obligations?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As simple as they are crafted, these questions are difficult to tackle and sometimes give ambiguous answers, leaving the analyst to provide a subjective opinion. Personally, this is one reason why FA appeals to me – the fact that the same set of numbers can lead to different analysis and stories meant that there’s more to the performance of a company than financials, much like data scientists would use data to tell a story and derive insights.&lt;/p&gt;

&lt;h2&gt;The ancient dichotomy – Quantitative and qualitative fundamentals&lt;/h2&gt;

&lt;p&gt;There’s no need to further elaborate the difference between quantitative and qualitative fundamentals. While revenue, sales, and costs can be objectively measured, notions like brand loyalty, intellectual product, and management competence are hard to place a price tag on. Following table illustrates examples what we would like to look for in both aspects of FA.&lt;/p&gt;

&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Example quantitative factors&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Example qualitative factors&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;b&gt;Balance sheet&lt;/b&gt;&lt;br&gt;    Assets&lt;br&gt;    Liabilities&lt;br&gt;    Equity&lt;br&gt;&lt;b&gt;Income statement&lt;/b&gt;&lt;br&gt;    Revenue&lt;br&gt;    Cost of goods sold&lt;br&gt;    Selling, general and admin expenses&lt;br&gt;&lt;b&gt;Cash flow statement&lt;/b&gt;&lt;br&gt;    Operations&lt;br&gt;    Financing&lt;br&gt;    Investing &lt;/td&gt;&lt;td&gt;Business model&lt;br&gt;Industry outlook&lt;br&gt;Competitive landscape&lt;br&gt;Regulation&lt;br&gt;Management competence&lt;br&gt;Governance    &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;Let’s take a look at each of them, starting from the qualitative factors.&lt;/p&gt;

&lt;h3&gt;Qualitative factors&lt;/h3&gt;

&lt;h4&gt;Business model&lt;/h4&gt;

&lt;p&gt;What exactly does a company do? The nature of a company’s business can vary from simplistic to downright complex and opaque. Something like McDonalds is not difficult to look at, while it is challenging to comprehensively evaluate Amazon or Google. The whole idea of understanding a company’s business model is consider whether its profits are sustainable in the short and long term, or a fluke. In addition, in this digital era, is the company and industry that we are looking at prone to disruption (think Kodak), or are the barriers to entry significant? It’s wiser and much easier to invest with a piece of mind on companies that we understand.&lt;/p&gt;

&lt;h4&gt;Industry outlook&lt;/h4&gt;

&lt;p&gt;Likewise, is the industry a weary and inefficient one, or something that investment dollars are pouring into? Is it on the fringe of disruption or well-protected? Is the industry growing or shrinking?&lt;/p&gt;

&lt;h4&gt;Competitive landscape&lt;/h4&gt;

&lt;p&gt;How do the competitors in the same space match up? What is their respective market share? Is it a duopoly like Boeing and Airbus, or are there plenty of strong players? Are there any up and coming small players that could disrupt? Or are there any external, non-traditional players looking to enter the market? Are the R&amp;amp;D initiatives bearing fruits that can shake things up? Or are customers simply to loyal to certain brands for anything to change at all? Knowing the competitive landscape helps in making that informed decision.&lt;/p&gt;

&lt;h4&gt;Regulation&lt;/h4&gt;

&lt;p&gt;Industries that are heavily regulated by authorities typically spend significant amounts of money lobbying, or have to be reactive to policy changes. Something like Medicare for all would make a dent on the bottomlines of healthcare companies.  Therefore, understanding the relationship between the company and the government helps in making an informed forecast.&lt;/p&gt;

&lt;h4&gt;Management competence&lt;/h4&gt;

&lt;p&gt;A solid business model won’t fly if the management is incompetent, leading to poor execution. For a retail investor, there is no truly good or robust way to assessing management competence. We would be able to fling a few million dollars around and demand attendance from the C-suite. The best would be to rely on Wall Street analyst opinions as well as to attend quarterly earnings calls with the CEO and/or CFO and judge for yourself. Are they open to tough line of questioning or are they dodging, spinning, or giving non-answers? We could also consider their historical records. In all, not easy to judge.&lt;/p&gt;

&lt;h4&gt;Governance&lt;/h4&gt;

&lt;p&gt;Finally, checks and balances are required to ensure that a company is compliant with laws and regulations. Financial transparency allows shareholders and investors information that they should rightly have access to. Conflicts of interest in management structure should be nonexistent and shareholders’ rights and interests should be upheld and represented.&lt;/p&gt;

&lt;h3&gt;Quantitative factors&lt;/h3&gt;

&lt;p&gt;The quantitative fundamentals of a company can all be found in its financial statements, which are quarterly, semi-annually or annually compiled and published. There are three major financial statements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Balance sheet&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Income statement&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cash flow statement&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before we elaborate on each of these, note that these statements and other variants are filed and published with the respective bourses. For stocks listed on the SGX, annual reports and related documents can be found &lt;a href=&#34;https://www2.sgx.com/securities/annual-reports-related-documents&#34;&gt;here&lt;/a&gt;. On the other hand, stocks listed in the US following &lt;a href=&#34;https://www.sec.gov/&#34;&gt;SEC&lt;/a&gt; regulations and periodically file reports known as 10-K and 10-Q. The 10-K is an annual filing that publishes the company&#39;s performance over the previous fiscal year. It consists of the balance sheet, income statement, cash flow statement, as well as other types of information, including that of operations, future plans, headwinds, etc. The 10-Q,&amp;nbsp; on the other hand, is the quarterly version of the 10-K, and is not audited, unlike the 10-K. There would typically be three 10-Q filings and one 10-K filing in a year, and they are vital sources of information and research for any serious stock picker.&lt;/p&gt;

&lt;h4&gt;Balance sheet&lt;/h4&gt;

&lt;p&gt;The balance sheet is a record of the company’s assets, liabilities, and equities at a given point in time. Assets are the all resources in the company, inclusive of free cash, inventory, real estate, machinery etc. The relationship between assets, liabilities, and equities are given in the fundamental accounting equation:&lt;/p&gt;

&lt;p&gt;Assets = Liabilities + Shareholders’ equity&lt;/p&gt;

&lt;p&gt;The right hand side of the equation represents the way in which these assets are financed, either by debt, i.e. liabilities, or by equity, i.e. all the value that has been contributed to the company by shareholders.&lt;/p&gt;

&lt;p&gt;The balance sheet is essentially a snapshot of the company at point in time, like an X-ray or MRI. While important, investors oftentimes overlook the balance sheet as revenue or earnings, which require a notion of time elapsed e.g. quarterly earnings, are not found in the balance sheet. On the other hand, the balance sheet informs investors about the amount of debt that the company has, how much cash and cash-like assets it possesses and so on – like how we take a blood test to look at various biomarkers like blood glucose or red blood cell count.&lt;/p&gt;

&lt;p&gt;There are two categories of assets – current and non-current assets. Current assets are largely liquid, perhaps with a latency of up to 12 months or less. Examples of current assets include cash, inventories, and account receivables, of course with cash being the most liquid.&lt;/p&gt;

&lt;p&gt;Typically, we can think of large amounts of free cash in a company as being an attractive investment. Simply put, there is an option value in cash for companies to grow in strategic directions, or provide some cushion during tough times – the proverbial “saving for the rainy day”. Another way to think about free cash is that the company is so caught up in earning money that its management has yet to figure a way to put the free cash into good use. Yet another way to see this is that its management is not far-sighted enough to know how to invest the cash. You can see that a small stockpile of cash can be argued accordingly in both ways as well.&lt;/p&gt;

&lt;p&gt;Inventories are goods that yet to be sold, while account receivables are outstanding payments due to the company by its customers. Inventories can be thought of as “held up” revenue, in that the goods have already been produced, but no revenue comes in because they are not sold yet. Like free cash, inventory levels can be analyzed accordingly. For example, inventory turnover (sales / average inventory) indicates how fast goods are moving through the system to customers. We would want inventories to remain at a steady level.&lt;/p&gt;

&lt;p&gt;Account receivables are credit issued to customers for their purchases. This allows the company to book top-line revenue and stimulate sales. Inevitably, too long of a credit is bad for the company.&lt;/p&gt;

&lt;p&gt;Non-current assets are essentially assets that cannot be liquidated in a jiffy – these includes real estate, factories etc. Even though non-current assets are counted in the balance sheet as assets, in reality they are not exactly resources that the company can rally in times of difficulty for any quick maneuvers.&lt;/p&gt;

&lt;p&gt;Liabilities are essentially debt, and can also be categorized under current liabilities and non-current liabilities, with the time horizon of current liabilities being say a year or less. For example, when a company issues a corporate bond of 10 years in tenure, this would be a non-current liability.&lt;/p&gt;

&lt;p&gt;Debt is critical for a company to succeed, when it manages to leverage appropriately. Of course, too much debt is unhealthy. The quick ratio, (current assets - inventories) / current liabilities, is a quick indicator – a quick ratio of more than 1 implies that short term debt obligations can be taken care of.&lt;/p&gt;

&lt;p&gt;Equity is the difference between assets and liabilities, represents what the shareholders own in the company. There are two key items in equity, namely the paid-in capital and retained earnings. Paid-in capital is the amount of money shareholders paid in the first place during public offering to own a share in the company. Retained earnings represent all the earnings made by the company, that instead of being paid out as dividend to shareholders, are retained to further the growth and operations of the company.&lt;/p&gt;

&lt;h4&gt;Income statement&lt;/h4&gt;

&lt;p&gt;The income statement is a little less boring than the balance sheet, because it directly shows the revenue numbers of the company over a period of time. Numbers like revenue, earnings, earnings per share (EPS) illustrates the performance of the company. Revenue, specifically revenue growth is often a strong investing signal as it can show potential of the company and the market as well as the company’s positioning in the market.&lt;/p&gt;

&lt;p&gt;To get from revenue to earnings, we need to consider expenses. There are typically two key types of expenses, namely cost of goods sold (COGS) and selling, general, and administrative expenses (SG&amp;amp;A) – both are rather self-explanatory. With these, we can then get to gross margin (net sales – COGS) and operating income (gross income – operating expenses, i.e. excluding expenses related to e.g. taxes and interest).&lt;/p&gt;

&lt;h4&gt;Cash flow statement&lt;/h4&gt;

&lt;p&gt;While the cash flow statement sounds similar to the income statement, they are very different beasts for a simple reason – while the income statement captures revenue and expense transaction, if there’s no actual cash flow, no records will be made on the cash flow statement. This manner of recording in the income statement is known as accrual accounting. Cash flow records in the cash flow statement could come from operations, financing, and investing.&lt;/p&gt;

&lt;p&gt;As we discussed previously, cash is an important component in the health of the company. Cash means flexibility and agility. One of the key considerations in looking at a company’s cash flow statement is to assess its ability to generate free cash flow, or FCF. Think of it as all our monthly expenses being accounted for from our salary – the rest is free for us to save, invest, or spend on a little indulgence. The same logic applies to a company. A company with plenty of FCF can embark on various ways to make use of the cash, perhaps in R&amp;amp;D or paying some of them back to shareholders.&lt;/p&gt;

&lt;p&gt;A closely related topic in cash flow, or specifically, discounted cash flow, is that of valuation, which I would like to touch on more in-depth in a future post.&lt;/p&gt;

&lt;p&gt;Summing up, another key topic in fundamental analysis is the use of valuation ratio, such as earnings per share, or EPS, or price-to-earnings or P/E ratio, to evaluate and summarize a company – another topic that justifies a separate in-depth exploration.&lt;/p&gt;

&lt;p&gt;That’s all for now! Hope you have gained something from this brief introduction. Thanks for reading 😊&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Medtech 101: An overview of the medical device industry</title>
      <link>/post/2019/05/27/medtech-101-an-overview-of-the-medical-device-industry/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/27/medtech-101-an-overview-of-the-medical-device-industry/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://www.medtecheurope.org/wp-content/uploads/2018/10/shutterstock_691546471-1049x590.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In this post, I will be doing a 101 dissection of the medical device industry. It’s a somewhat not-so-popular industry from the perspective of retail investing, but I have had some prior experience in this space, so it’s a tad easier for me to conduct some levels of analysis on companies in this space.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;I will work through the following pointers in this piece,
and hopefully at the end of it, allow the reader to have some level of appreciation
for the industry as a whole.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Introduction to the medical device industry&lt;/li&gt;&lt;li&gt;The medical devices themselves&lt;/li&gt;&lt;li&gt;Key features of the industry&lt;/li&gt;&lt;li&gt;Being a retail investor in this space&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Unless otherwise stated, all facts and information are obtained from my desktop research from BMI Research and Fitch Solutions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;Introduction to the medical device industry&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Medical devices refer to a variety of mostly hardware instruments used in the diagnosis, treatment or prevention of a multitude of diseases. These instruments range from surgical gloves and masks, to advanced devices like drug-eluting stents (DES). Technologically simple products are competed in the market via price and volume, just like any other household or everyday products.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://www.mddionline.com/sites/default/files/images/Boston-Sci-Synergy-Stent.jpg&#34; width=&#34;60%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Boston Scientific Synery Drug Eluting Stent system. &lt;a href=&#34;https://www.mddionline.com/boston-scis-synergy-stent-worth-price&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;What we are more interested in is the opposite end of this spectrum. Products like DES or other implantable medical devices (IMD) are technologically advanced and provide differentiated ways in treating complex diseases like heart arrhythmia and chronic arthritis. They are costly devices and are rarely paid for by individual patients – that is, their usage is co-paid or reimbursed by insurers or government healthcare schemes.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;By definition, a medical device is different from a drug in that the successful use of the device is not dependent on the absorption or metabolism of the device. In this sense, a wheelchair is a medical device in the same way as a pacemaker.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;The industry at a glance&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Following table illustrates the size of the medical device industry (Source: Fitch Solutions, &lt;a href=&#34;https://store.fitchsolutions.com/global-medical-devices-report&#34;&gt;Global Medical Devices Report Q2 2019&lt;/a&gt;).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;
  &amp;nbsp;
  &lt;/td&gt;&lt;td&gt;&lt;strong&gt;2018   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;2019 forecast   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;2020 forecast   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;2021 forecast   &lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Total sales USDmn   &lt;/td&gt;&lt;td&gt;387,393.61   &lt;/td&gt;&lt;td&gt;408,173.28   &lt;/td&gt;&lt;td&gt;
  434,477.48
  &lt;/td&gt;&lt;td&gt;461,548.71   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Totals sales %y/y   &lt;/td&gt;&lt;td&gt;7.6   &lt;/td&gt;&lt;td&gt;5.4   &lt;/td&gt;&lt;td&gt;6.4   &lt;/td&gt;&lt;td&gt;6.2   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Total sales 5-yr CAGR   &lt;/td&gt;&lt;td&gt;5.9   &lt;/td&gt;&lt;td&gt;6.0   &lt;/td&gt;&lt;td&gt;5.8   &lt;/td&gt;&lt;td&gt;5.7   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Totals sales per capita USD   &lt;/td&gt;&lt;td&gt;61.6   &lt;/td&gt;&lt;td&gt;64.3   &lt;/td&gt;&lt;td&gt;67.9   &lt;/td&gt;&lt;td&gt;71.6   &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Following is an infographic published by Technavio.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://www.technavio.com/image/medical%20device%20market.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Technavio Global Medical Devices Report 2018-2022. &lt;a href=&#34;https://www.technavio.com/report/global-medical-devices-market-analysis-share-2018&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;According to both the Fitch Solutions Global Medical Devices Report Q2 2019 as well as the Technavio Global Medical Devices Report 2018 -2022, the industry is forecasted to grow at a CAGR of about 5% or more, up to a forecasted market size of 460 billion by 2021.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;The US Market&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The US is both a dominant player and consumer in the medical device industry. On top of having the world’s highest health spending as percentage GDP and per capita, the US medical device market accounts for over 40% of the global market and has the world’s highest per capita medical device expenditure. The US is also home to many of the world’s largest medical device companies. The following table illustrates the top 12 players in the industry, which is dominated by the Americans. Most of these companies have a presence in California, Massachusetts, and &lt;a href=&#34;https://www.medicaldesignandoutsourcing.com/minnesota-2-0-major-u-s-medical-device-cluster/&#34;&gt;Minnesota (the so-called “Medical Alley”)&lt;/a&gt; (Source: &lt;a href=&#34;https://www.mpo-mag.com/issues/2018-07-01/view_features/the-2018-top-30-global-medical-device-companies&#34;&gt;Medical Product Outsourcing 2018 Top 30 Global Medical Device Companies&lt;/a&gt;).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Rank   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Company   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Country   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Medical device revenue USDbn   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Market cap USDbn   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Price-to-Sales   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Ticker symbol   &lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1   &lt;/td&gt;&lt;td&gt;Medtronic   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;29.7   &lt;/td&gt;&lt;td&gt;125   &lt;/td&gt;&lt;td&gt;4.21   &lt;/td&gt;&lt;td&gt;MDT   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2   &lt;/td&gt;&lt;td&gt;Johnson &amp;amp; Johnson   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;26.6   &lt;/td&gt;&lt;td&gt;369*   &lt;/td&gt;&lt;td&gt;-*   &lt;/td&gt;&lt;td&gt;JNJ   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3   &lt;/td&gt;&lt;td&gt;GE Healthcare   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;19.1   &lt;/td&gt;&lt;td&gt;82.4   &lt;/td&gt;&lt;td&gt;4.31   &lt;/td&gt;&lt;td&gt;GE**   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4   &lt;/td&gt;&lt;td&gt;Royal Philips   &lt;/td&gt;&lt;td&gt;The Netherlands   &lt;/td&gt;&lt;td&gt;16.3   &lt;/td&gt;&lt;td&gt;37.5   &lt;/td&gt;&lt;td&gt;2.30   &lt;/td&gt;&lt;td&gt;PHIA   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5   &lt;/td&gt;&lt;td&gt;Siemens Healthineers   &lt;/td&gt;&lt;td&gt;Germany   &lt;/td&gt;&lt;td&gt;16.3   &lt;/td&gt;&lt;td&gt;40.4   &lt;/td&gt;&lt;td&gt;2.48   &lt;/td&gt;&lt;td&gt;SHL   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6   &lt;/td&gt;&lt;td&gt;Abbott Laboratories   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;16.2   &lt;/td&gt;&lt;td&gt;136   &lt;/td&gt;&lt;td&gt;8.40   &lt;/td&gt;&lt;td&gt;ABT   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7   &lt;/td&gt;&lt;td&gt;Cardinal Health   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;13.5   &lt;/td&gt;&lt;td&gt;13.7   &lt;/td&gt;&lt;td&gt;1.01   &lt;/td&gt;&lt;td&gt;CAH   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8   &lt;/td&gt;&lt;td&gt;Stryker   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;12.4   &lt;/td&gt;&lt;td&gt;69.1   &lt;/td&gt;&lt;td&gt;5.57   &lt;/td&gt;&lt;td&gt;SYK   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9   &lt;/td&gt;&lt;td&gt;Becton Dickinson   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;12.1   &lt;/td&gt;&lt;td&gt;63.4   &lt;/td&gt;&lt;td&gt;5.24   &lt;/td&gt;&lt;td&gt;BDX   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;10   &lt;/td&gt;&lt;td&gt;Baxter   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;10.6   &lt;/td&gt;&lt;td&gt;38.4   &lt;/td&gt;&lt;td&gt;3.62   &lt;/td&gt;&lt;td&gt;BAX   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;11   &lt;/td&gt;&lt;td&gt;Boston Scientific   &lt;/td&gt;&lt;td&gt;US   &lt;/td&gt;&lt;td&gt;9.0   &lt;/td&gt;&lt;td&gt;53.5   &lt;/td&gt;&lt;td&gt;5.94   &lt;/td&gt;&lt;td&gt;BSX   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;12   &lt;/td&gt;&lt;td&gt;Essilor   &lt;/td&gt;&lt;td&gt;France   &lt;/td&gt;&lt;td&gt;9.0   &lt;/td&gt;&lt;td&gt;52.0   &lt;/td&gt;&lt;td&gt;5.78   &lt;/td&gt;&lt;td&gt;EL   &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;* Johnson &amp;amp; Johnson’s businesses extend well beyond that of medical devices.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;** GE Healthcare remains to be a healthcare unit within GE. While &lt;a href=&#34;https://www.cnbc.com/2018/12/19/ge-reportedly-confidentially-files-for-ipo-of-health-care-unit.html&#34;&gt;GE has filed for IPO to spin off GE Healthcare&lt;/a&gt; during December 2018, &lt;a href=&#34;https://www.cnbc.com/2019/02/25/ge-withdraws-plans-for-healthcare-ipo-after-danaher-deal-bloomberg.html&#34;&gt;a 2019 IPO “looks unlikely after Danaher deal”&lt;/a&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;The medical devices themselves&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Classification of devices&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In the US, the primary regulatory body that governs the entire medical device industry is the Food and Drug Administration, or FDA. By FDA standards, medical devices are classified into three classes, namely Class I, II and III. A device falls into one of these classes by virtue of their risk level to patient, Class III being the riskiest. The following table illustrates examples of these classes. Most if not all major countries follow similar standards, e.g. Class I to IV in Japan, &lt;a href=&#34;https://www.hsa.gov.sg/content/hsa/en/Health_Products_Regulation/Consumer_Information/Consumer_Guides/Medical_Devices.html&#34;&gt;Class A to D in Singapore&lt;/a&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;
  &amp;nbsp;
  &lt;/td&gt;&lt;td&gt;&lt;strong&gt;Level of risk   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Examples   &lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Class I   &lt;/td&gt;&lt;td&gt;Low   &lt;/td&gt;&lt;td&gt;Surgical gloves, scalpers, wheelchairs   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Class II   &lt;/td&gt;&lt;td&gt;Moderate   &lt;/td&gt;&lt;td&gt;Powered wheelchairs, infusion pumps, surgical drapes   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Class III   &lt;/td&gt;&lt;td&gt;High   &lt;/td&gt;&lt;td&gt;Heart valves, silicone breast implants, stents and balloons   &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Segmentation by device type and diseases&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Medical device companies typically tackle conditions in the following areas of specialty, like this example from &lt;a href=&#34;https://global.medtronic.com/xg-en/patients/conditions.html&#34;&gt;Medtronic&lt;/a&gt;, with example conditions in each of them. Going into each specialty would be too much here.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;
  &amp;nbsp;
  &lt;/td&gt;&lt;td&gt;&lt;strong&gt;Example condition   &lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Device/therapy   &lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Airway and lungs   &lt;/td&gt;&lt;td&gt;Pulmonary valve disease   &lt;/td&gt;&lt;td&gt;Transcatheter pulmonary valve (TPV) Therapy   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Brain   &lt;/td&gt;&lt;td&gt;Epilepsy&amp;nbsp;   &lt;/td&gt;&lt;td&gt;Deep brain simulation   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Diabetes   &lt;/td&gt;&lt;td&gt;Type 2 diabetes   &lt;/td&gt;&lt;td&gt;Continuous glucose monitoring (CGM)   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Digestive and gastrointestinal   &lt;/td&gt;&lt;td&gt;Crohn’s Disease   &lt;/td&gt;&lt;td&gt;Disease visualization   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ear, Nose, Throat (ENT)   &lt;/td&gt;&lt;td&gt;Thyriod conditions   &lt;/td&gt;&lt;td&gt;Minimally invasive video-assisted   thyroidectomy   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Heart and Vascular   &lt;/td&gt;&lt;td&gt;Bradycardia, Tachycardia   &lt;/td&gt;&lt;td&gt;Pacemakers   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Pain Management   &lt;/td&gt;&lt;td&gt;Chronic pain   &lt;/td&gt;&lt;td&gt;Spinal cord stimulation   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Spine and Orthopedic   &lt;/td&gt;&lt;td&gt;Cervical herniated discs   &lt;/td&gt;&lt;td&gt;Cervical disc replacement   &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Urinary   &lt;/td&gt;&lt;td&gt;Overactive bladder   &lt;/td&gt;&lt;td&gt;Bladder control therapy   &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;Key features of the industry&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;The role of regulatory bodies&lt;/em&gt;. The FDA in the US plays a key role in regulating the medical device industry. For starters, in order for companies to market their devices, these devices must go through one of two FDA approval processes, namely the pre-market approval (PMA) or the pre-market notifications (510k), where the latter is less stringent than the former. Most devices go through the 510k process (3-6 months), while more advanced or novel devices go through the PMA process (~ 270 days) (&lt;a href=&#34;http://www.medpac.gov/docs/default-source/reports/jun17_ch7.pdf&#34;&gt;Source&lt;/a&gt;). The FDA is also continuously involved in the post-market surveillance of devices, by reviewing studies done be researchers or the companies themselves to evaluate device effectiveness and efficacies. If required, the FDA issues product recalls to stop all device marketing and usage in the market.&lt;/li&gt;&lt;li&gt;&lt;em&gt;Low price transparency&lt;/em&gt;. While list prices of products are typically known, due to the complexity of reimbursements on a procedure (rather than a device) basis, patients often have next to zero inkling on how much their devices truly cost. Likewise, physicians, hospitals, and e.g. Medicare all have limited knowledge on actual device transaction price. Only the medical device companies know how much they are selling these devices for. Talk about corporate capitalism.&lt;/li&gt;&lt;li&gt;&lt;em&gt;Relationship between medical devices companies and physicians&lt;/em&gt;. Medical devices companies have often developed deep ties with physicians (not hospitals) themselves, in a setting where physicians often make purchasing decisions on devices, e.g. preferring an equivalent product from one company to another, due to e.g. familiarity or inertia to change. There are other ways in which these companies interact directly with physicians (FDA) (&lt;a href=&#34;http://www.medpac.gov/docs/default-source/reports/jun17_ch7.pdf&#34;&gt;Source&lt;/a&gt;). These includes:&lt;ul&gt;&lt;li&gt;royalty payments to physicians who help develop medical devices;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;consulting fees to physicians for providing feedback about the performance and design of a company’s devices;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;funding for physicians to conduct research;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;funding for medical education activities; and,&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;for physicians who use IMDs, regular interactions with the manufacturer’s sales representatives, who are often present at the physician’s invitation in the operating room during procedures and may help the physician make a final decision about which devices to use.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;High barrier of entry even amongst the tech giants&lt;/em&gt;. While the likes of Google Health, Apple Watch, or &lt;a href=&#34;https://thenextweb.com/artificial-intelligence/2018/10/15/amazons-new-patent-will-allow-alexa-to-detect-your-illness/&#34;&gt;Alexa detecting a cold or cough&lt;/a&gt; have attempted to break into the healthcare space, success in developing and marketing medical devices requires significant scientific expertise and market knowledge. Combine that with navigating the red tape and bureaucracy, something like Netflix producing and distributing films probably won’t happen for quite some time.&lt;/li&gt;&lt;li&gt;&lt;em&gt;Big eat small&lt;/em&gt;. Start-up companies that develop promising new products are often acquired by one of the large medical devices companies. These acquisitions are beneficial to both sides of the equation: small companies can find it challenging to market their products, while major device companies have established distribution networks and relationships with hospitals and other providers. Large companies can also provide additional resources to further develop and improve new medical devices. An acquisition also allows the venture capital firms that supported the start-up company to withdraw their funding and realize a profit. For the large companies, acquisitions provide another way to conduct research and development and can either complement or substitute for the company’s internal efforts. Large companies can also use acquisitions to branch out into new therapeutic areas or bolster existing product lines (&lt;a href=&#34;http://www.medpac.gov/docs/default-source/reports/jun17_ch7.pdf&#34;&gt;Source&lt;/a&gt;).&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;Being a retail investor in the medical device industry&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Short of being an entrepreneur, a private equity, a venture capital or even a medical researcher, investing in the medical device industry would mean investing in stocks or investing in index ETFs. Suppose due diligence is done and fundamental analysis validates that a company is suitably priced in the market. It would be reasonable then to approach stock picking in this industry based on events such as the following:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Research breakthroughs, new product launch, acquisition of new, non-cannibalizing technology&lt;/li&gt;&lt;li&gt;Lawsuits, FDA product recalls, ethical scandals&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Being aware of who’s who in specific areas is critical in assessing changes in the competitive landscape. For example, Boston Scientific continues to dominate in the left atrial appendage closure (LAAC) market with Watchman, while Johnson &amp;amp; Johnson has fallen from grace in the DES market, giving up space to Abbott Labs, Boston Scientific, and Medtronic.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Alternatively, the best bet on the space as a lay retail investor is probably the &lt;a href=&#34;https://www.ishares.com/us/products/239516/ishares-us-medical-devices-etf&#34;&gt;iShares US Medical Devices ETF&lt;/a&gt; (IHI), listed on NYSE. IHI tracks the Dow Jones U.S. Select Medical Equipment Index, with its current top holdings including Abbott Labs, Medtronic, and Strkyer. At an expense ratio of 43 basis points, it’s a fairly reasonable ETF to hold on to for broad medical device exposure.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;And that&amp;rsquo;s all for this 101! Hope you have gained a little something from it. Thank you for reading :)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>REITs 101: Introduction to real estate investment trusts</title>
      <link>/post/2019/05/20/reits-101-introduction-to-real-estate-investment-trusts/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/20/reits-101-introduction-to-real-estate-investment-trusts/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://i.etsystatic.com/6111470/r/il/35f144/1417264877/il_fullxfull.1417264877_awhp.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;This post is a 101 dissection of real estate investment trusts, REITs or Reits in short. As the full content of this topic is pretty long and I will try to make this 101 comprehensive, touching on the following:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;What are Reits&lt;/li&gt;&lt;li&gt;What makes Reits different from other investment vehicles&lt;/li&gt;&lt;li&gt;Why invest in Reits&lt;/li&gt;&lt;li&gt;What are the different types of Reits&lt;/li&gt;&lt;li&gt;How to evaluate a Reit&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;As you can see, there&amp;rsquo;s plenty to talk about even in a 101 piece, so let&amp;rsquo;s get straight to it.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;1. What are Reits?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Reits, by virtue of their long form name, are essentially instruments that allow investors to get exposure to the real estate market, be it locally, regionally, or globally. The easiest way to give an explanation of what is a Reit is to take on the perspective of the Reit manager. As the Reit manager, to start an investment trust on real estate, you would first need to secure financing from, say a bank, to finance your portfolio of properties. That&amp;rsquo;s one portion of the financing. The other portion comes from listing the Reit in an exchange through an IPO, and get investors to invest in your Reit as a unitholder.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;From there, your entire portfolio of properties are financed from these two parties and your Reit is established. Afterwhich, you would do whatever that is required to rent these properties out on a lease, and start to collect rental income.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Reits as a passive income stream&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;And here&amp;rsquo;s where plenty of retails investors, especially those in Singapore, have came to love Reits - Reits are obligated to pay out a minimum of 90% of its income as distributions (i.e. dividends) to enjoy tax exemptions. Suffice to say that the math works out for the Reit manager to indeed to pay at least 90%. This obligation is often viewed favorably by investors as a means to achieve consistent passive income stream - it has been suggested that Reits can be used as a &lt;a href=&#34;https://www.todayonline.com/singapore/choosing-steady-retirement-income&#34;&gt;supplement or even alternative to a retiree&amp;rsquo;s annuity&lt;/a&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;The different parties within a Reit&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Following illustrates clearly how each party in a Reit interacts with another. Let&amp;rsquo;s take a brief look at each of them.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://www.moneysense.gov.sg/-/media/moneysense/images/content-graphics-infographics/a-typical-reit-structure.jpg?la=en&amp;amp;hash=C618E33305CB50C523C8A226DB39D2684F532A1A&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
A typical Reit structure. &lt;a href=&#34;https://www.moneysense.gov.sg/articles/2018/10/understanding-real-estate-investment-trusts-reits&#34;&gt;Source&lt;/a&gt;.
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;The Trustee: the trustee, as the name suggests, is responsible for the holding of the Reit units for investors. The trustee may also ensure compliance on the Reit on behalf of unitholders, and in gist, act in all manners in the interest of unitholders.&lt;/li&gt;&lt;li&gt;The Reit manager: the Reit manager is the one or the team that manages the strategic oversight of the Reit and its investment strategies. A Reit manager, may for instance, decide to expand out of Singapore real estate and venture into say, the Australian market.&lt;/li&gt;&lt;li&gt;The Property manager: on the other hand, the property manager is in charge of the actual management of the properties, e.g. running marketing campaigns or strategizing to maximise rental income.&lt;/li&gt;&lt;li&gt;The Sponsor: the sponsor is basically the &amp;ldquo;big boss&amp;rdquo; in the Reit. They are usually big developers like &lt;a href=&#34;https://www.capitaland.com/sg/en.html&#34;&gt;CapitaLand&lt;/a&gt; or &lt;a href=&#34;http://www.mapletree.com.sg/&#34;&gt;Mapletree&lt;/a&gt;. They may be the ones who initially developed the properties and assets in the Reit. A sponsor, especially a larger one, would typically run and manage several Reits at once with different Reit manager teams - e.g. CapitaLand with &lt;a href=&#34;https://sg.finance.yahoo.com/quote/c38u.si&#34;&gt;CapitaLand Mall Trust&lt;/a&gt;, &lt;a href=&#34;https://sg.finance.yahoo.com/quote/C61U.SI&#34;&gt;CapitaLand Commercial Trust&lt;/a&gt;, &lt;a href=&#34;https://sg.finance.yahoo.com/quote/A68U.SI/&#34;&gt;Ascott Residence Trust&lt;/a&gt;, &lt;a href=&#34;https://sg.finance.yahoo.com/quote/AU8U.SI&#34;&gt;CapitaLand Retail China Trust&lt;/a&gt;, and &lt;a href=&#34;https://sg.finance.yahoo.com/quote/5180.KL/&#34;&gt;CapitaLand Malaysia Trust&lt;/a&gt; (listed on the Kuala Lumpur Stock Exchange).&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;2. Why makes Reits different from other investment vehicles?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Fundamentally, Reits have no relations with stocks, bonds or commodities, only real estate. Therefore Reits are often thought of as an alternative asset class.&lt;/li&gt;&lt;li&gt;Public companies operate in accordance to typical business regulations set out by e.g. SEC in the US, and different companies operate under different guidelines for different industries, e.g. a medical devices company with the FDA. On the other hand, Reits are more strictly regulated and more homogeneous. In Singapore, &lt;ul&gt;&lt;li&gt;Reits cannot undertake any development activities (e.g. &lt;a href=&#34;https://www.reitsweek.com/2017/11/asset-enhancement-initiative.html&#34;&gt;asset enhancement initiatives&lt;/a&gt; or AEI) directly or indirectly, when the value of these activities are in excess of 10% of the Reit&amp;rsquo;s portfolio value.&lt;/li&gt;&lt;li&gt;Reits can have a maximum leverage (i.e. gearing ratio) of 45%.&lt;/li&gt;&lt;li&gt;As mentioned above, Reits are obligated to pay out a minimum of 90% of its income as distributions (i.e. dividends) to enjoy tax exemptions.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Unlike Reits, these or equivalent measures on leverage and distributions are not applicable in public stocks or companies.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;What about Reits vs physical property?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Well that&amp;rsquo;s all good, but what about having real estate exposure by simply owning a property? How does that match up? In Singapore,&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Investing in a Reit requires a brokerage account, a CDP account, and the transaction fees in buying and selling. There is no capital gain tax nor dividend tax in Singapore. For a physical property, there&amp;rsquo;s the following to think about, on top of time and money spent in e.g. sourcing for tenants:&lt;ul&gt;&lt;li&gt;3-4% Buyer Stamp Duty&lt;/li&gt;&lt;li&gt;12-25% Additional Buyer Stamp Duty&lt;/li&gt;&lt;li&gt;legal fees&lt;/li&gt;&lt;li&gt;property tax of ~10% Annual Value&lt;/li&gt;&lt;li&gt;rental income tax&lt;/li&gt;&lt;li&gt;maintenance cost&lt;/li&gt;&lt;li&gt;insurance&lt;/li&gt;&lt;li&gt;mortgage&lt;/li&gt;&lt;li&gt;4-12% Seller Stamp Duty&lt;/li&gt;&lt;li&gt;agent commission&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Physical properties are more illquid that the exchange-traded Reit unit&lt;/li&gt;&lt;li&gt;Due to the price of physical properties, it is unlikely for a retail investor to have say more than 5 or 10 properties, thus incurring concentration risk in location. A well diversified Reit would have no such issues.&lt;/li&gt;&lt;li&gt;Yield of a Reit is typically in the 4-8% range, while it could be challenging to replicate or better this yield from a physical property.&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;(Good to know) Reits versus Business trusts, stapled securities&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Not all the counters you see on SGX that seem like Reits, are actually Reits. For example, Ascendas Reit (&lt;a href=&#34;https://sg.finance.yahoo.com/quote/A17U.SI/&#34;&gt;A17U.SI&lt;/a&gt;) is a Reit, while Ascendas Htrust (&lt;a href=&#34;https://sg.finance.yahoo.com/quote/Q1P.SI&#34;&gt;Q1P.SI&lt;/a&gt;) is a &lt;strong&gt;stapled security&lt;/strong&gt;, i.e. one unit of Q1P constitutes buying into the Ascendas Hospitality Real Estate Trust plus the Ascendas Hospitality Business Trust. Think of it as stapling two pieces of paper together.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;From a regulatory standpoint, there are significant differences between a Reit and a business trust. This &lt;a href=&#34;https://www.reitas.sg/singapore-reits/differences-between-s-reits-business-trusts-in-singapore/&#34;&gt;REITAS page&lt;/a&gt; (&lt;a href=&#34;https://www.reitas.sg/&#34;&gt;REITAS&lt;/a&gt; is short for REIT Association of Singapore) is clear on the differences. For retail investors, it would be good to be cognizant of the following (lifted from the same page):&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Reit&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Business Trust&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Distribution&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Reits must distribute at least 90% of their specified income to their unitholders to enjoy tax transparency under the Income Tax Act.&lt;/td&gt;&lt;td&gt;There is no statutory requirement to distribute a certain percentage of its income, but business trusts may pledge to distribute a certain percentage of income to their unitholders. &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Gearing&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;The Code on Collective Investment Schemes sets a 45% cap on gearing.&lt;/td&gt;&lt;td&gt;There is no statutory gearing limit, but business trusts may commit to a self-imposed borrowing limit.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In a stapled security, all Reits regulation (e.g. the Income Tax Act and the Code on Collective Investment Schemes) apply to the Reit portion only. The business trust is nothing but a company, regulated under typical public company regulations.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;3. Why invest in Reits?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Reits allow for property exposure without the significant hassle of property ownership.&lt;/li&gt;&lt;li&gt;Reits are most commonly thought of as a source of passive income stream, and rightfully so. As we pointed out earlier in this 101, Reits are obligated to pay out a minimum of 90% of its income as distributions to enjoy tax exemptions. Suffice to say that the math works out for the Reit manager to indeed pay out at least 90%. This obligation is viewed as a plus point for retail investors as a consistent passive income. This is in addition to the fact that most Reits distribute on a quarterly basis.&lt;/li&gt;&lt;li&gt;Because the relatively high yields (~6-8%), Reits can play a part in a retiree&amp;rsquo;s holdings, where passive income and/or principal drawback kicks in. Instead of selling of units, Reits function as a perpetual income stream, of course provided that you have accumulated enough units during your career years.&lt;/li&gt;&lt;li&gt;Unlike evaluating companies coming from a variety of industries, from e-commerce to shipping, Reits are relatively homogeneous and easier to understand. The notion of buying and managing a property, then collect rental income is much easier to grasp than e.g. how Medicare and Medicaid policy changes will affect the outlook of pharmaceutical companies. This relative ease makes it more straightforward to evaluate Reits, and act on your decision.&lt;/li&gt;&lt;li&gt;In addition to high yields, Reits also avail themselves to capital gains and losses. Reits have been performing well since the second half of 2018 and continues to do so. Make of that what you will.&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;4. What are the different types of Reits?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Reits are typically thought of as coming under these five categories:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Retail Reits&lt;/strong&gt; are those comprising of shopping malls, focusing on the retail sector. Example Reit: &lt;a href=&#34;https://sg.finance.yahoo.com/quote/c38u.si/&#34;&gt;CapitaMall Trust&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Commercial Reits&lt;/strong&gt; go for commercial properties, such as office buildings. Example Reit: &lt;a href=&#34;https://sg.finance.yahoo.com/quote/t82u.si/&#34;&gt;Suntect Reit&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Industrial Reits&lt;/strong&gt; consist of properties like factories, warehouses and other specialized facilities. Example Reit: &lt;a href=&#34;https://sg.finance.yahoo.com/quote/me8u.si/&#34;&gt;Mapletree Industrial Trust&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Hospitality Reits&lt;/strong&gt; are the ones with hotels and serviced apartments. Example Reit: &lt;a href=&#34;https://sg.finance.yahoo.com/quote/q1p.si/&#34;&gt;Ascendas hTrust&lt;/a&gt; (a stapled security).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Healthcare Reits&lt;/strong&gt; are, well, hospitals and other healthcare facilities like medical centers or nursing homes. Example Reit: &lt;a href=&#34;https://sg.finance.yahoo.com/quote/AW9U.si/&#34;&gt;First Reit&lt;/a&gt;.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;As you can imagine, each category of Reits would flourish or suffer under whichever prevailing economic conditions. The clearest example would be that hospitality Reits would typically suffer during economic downturns, where consumers are less likely to travel (discretionary spending). High unemployment numbers could mean lower occupancy in commercial Reits, while a boost from the government to transform the country into a medical tourism hub would do well for healthcare Reits. As such, these broad economic and policy changes plays significant role in Reit evaluation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;There is also a bunch of Reit-ETFs, such as the &lt;a href=&#34;https://sg.finance.yahoo.com/quote/CLR.SI&#34;&gt;Lion-Phillip S-Reit ETF&lt;/a&gt;, which I personally don&amp;rsquo;t fancy - it&amp;rsquo;s kind of &amp;ldquo;fee-on-top-of-fee&amp;rdquo;. &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/01/13/a-guide-to-dollar-cost-averaging-in-singapore/&#34;&gt;Using regular saving plans products like the POEMS Shares Builder Plans&lt;/a&gt; can allow us to assemble &lt;a href=&#34;https://www.poems.com.sg/rsp/#counters&#34;&gt;a diverse portfolio of Reits&lt;/a&gt; without the ETF expense ratio.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;5. How to evaluate a Reit&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Due to their homogeneity, there are a number of standardized ways of Reit evaluation, with the use of a number of metrics that capture different aspects of a Reit. I wouldn&amp;rsquo;t be going too in-depth here as I would like to explore this topic in greater detail in the future.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Here are some ways in which Reits are evaluated:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Distribution per Unit (DPU)&lt;/strong&gt;. The DPU is simply the distributed amount to unitholders during e.g. quarterly distributions. For example, the most recent distribution from &lt;a href=&#34;https://www.dividends.sg/view/C38U&#34;&gt;CapitaMall Trust&lt;/a&gt; (C38U) in May 2019 was 0.0444 per unit, which is a quarterly yield (see 2. below) and yield-to-date of 1.83%.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Distribution Yield&lt;/strong&gt;. Slightly different from DPU is the yield. The yield is simply your dividends divided by your principal, i.e. instead of having units as a denominator in DPU, we are talking here about the price of the Reit. More important than maximizing yield (the so-called &lt;a href=&#34;https://www.fool.com/retirement/2018/01/28/what-is-a-dividend-trap.aspx&#34;&gt;dividend trap&lt;/a&gt;) is to look for stable and growing distributions. An increasing yield over time could mean that the Reit is stable, and is able to attract tenants. For example, the yield of C38U grew from 3.44% in 2013 to 7.68% in 2018 full year.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Price-to-NAV&lt;/strong&gt;. The price-to-NAV ratio, also known as book value, compares the market valuation of the Reit to the value of the Reit in its books. NAV stands for net asset value, which captures the amount of money, in principle, that the Reit would have if the Reit manager liquidates all assets and returns the money back to investors. We would consider any price-to-NAV ratio of less than 1 to signal undervaluation by the market. Following are some examples (as of May 2019):&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://sg.finance.yahoo.com/quote/t82U.SI/key-statistics?p=t82U.SI&#34;&gt;Suntec Reit&lt;/a&gt; - 0.87&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://sg.finance.yahoo.com/quote/k71U.SI/key-statistics?p=k71U.SI&#34;&gt;Keppel Reit&lt;/a&gt; - 0.85&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://sg.finance.yahoo.com/quote/ME8U.SI/key-statistics?p=ME8U.SI&#34;&gt;Mapletree Industrial Trust&lt;/a&gt; - 1.39&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Gearing Ratio&lt;/strong&gt;. Also known as leverage, the gearing ratio is the ratio between the amount of debt divided by the Reit&amp;rsquo;s total assets. Recall we talked about the maximum gearing ratio of a Reit in Singapore has to be less than 45%. Generally, we are seeing an average of &amp;lt; 40% amongst our S-Reits.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Weighted Lease Average Expiry (WALE)&lt;/strong&gt;. As the name suggests, WALE is simply the mean remaining lease duration, weighted by the rental income, in the Reit&amp;rsquo;s portfolio. Needless to say, a WALE of 10 years is better than that of 5 years. WALE typically differs depending on the type of Reit - for example, industrial Reits tend to have longer leases than a retail Reit.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Capitalization Rate&lt;/strong&gt;. Also know as property yield, the cap rate simply captures the income derived from the properties (net operating income) to the value of the properties.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;And there&amp;rsquo;s more. Some of these metrics, like WALE, are obviously Reit-specific, while others apply to most if not all listed company stocks. I will go over more of these metrics, as well as viewing Reits under a fundamental analysis lens, in a later, post-101 series of posts.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;That&amp;rsquo;s all for now - thank you for reading! :)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] An uncommon approach in tackling class imbalance</title>
      <link>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/05/11/short-an-uncommon-approach-in-tackling-class-imbalance/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/05/f5126-1h01epkhncswjxdtyxzuq6g.jpeg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph {&#34;align&#34;:&#34;left&#34;} --&gt;

&lt;p&gt;&lt;p class=&#34;has-text-align-left&#34;&gt;In supervised learning, one challenged faced by data scientists is classification class imbalance, where in a binary classification problem, instances in one class severely outnumbers instances in the other. This poses a problem as model performances may be misleading: a naive example would be to always predict negative in a 10% positive-90% negative dataset - accuracy would then be 90%, but the model would be utterly useless.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The typical approaches in alleviating class imbalance include using robust metrics such as the ROC-AUC, or performing downsampling of majority class or upsampling of minority class (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis#SMOTE&#34;&gt;SMOTE&lt;/a&gt;). Of course, downsampling of majority class is often frowned upon as precious datapoints are being discarded.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In many situations, these approaches work reasonably well. However, in contexts in which there is an inherent asymmetry between false positives and false negatives, these approaches are less than ideal. For example,&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;In cyber security, the inability to detect an intrusion into networks (false negative) incurs a different cost as compared to a false alarm (false positive).&lt;/li&gt;&lt;li&gt;In human resources, the inability to detect impending attrition of a high-potential employee (false negative) incurs a different cost as compared to incorrectly detecting said attrition (false positive).&lt;/li&gt;&lt;li&gt;In a clinical setting, the inability to detect post-surgical complications (false negative) incurs a different cost as compared to incorrectly detecting a complication (false positive). &lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In most practical contexts, false positives and negatives incur different waste and costs. In addition, given that a prediction error is going to occur, there is often a &lt;strong&gt;preferred &lt;/strong&gt;outcome or error. For instance, a healthcare practitioner would likely rather to not miss out on a post-surgical complication, than save on manpower and resources with poor prognosis.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Assuming that is the case, a simple but less commonplace approach to tackling class imbalance is to design a utility function &lt;em&gt;U(m)&lt;/em&gt; that captures the inherent asymmetry of prediction outcomes, and use &lt;em&gt;U(m) &lt;/em&gt;as the loss function in the ML training process for model &lt;em&gt;m&lt;/em&gt;. Such utility functions are often used in econometrics to capture choices and preferences. Following illustrates an instructive but naive example of &lt;em&gt;U(m)&lt;/em&gt;:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;
  &lt;strong&gt;Prediction Outcome&lt;/strong&gt;
  &lt;/td&gt;&lt;td&gt;
  &lt;strong&gt;Utility Score&lt;/strong&gt;
  &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
  True positive
  &lt;/td&gt;&lt;td&gt;
  1
  &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
  True negative
  &lt;/td&gt;&lt;td&gt;
  1
  &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
  False positive
  &lt;/td&gt;&lt;td&gt;
  -5
  &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
  False negative
  &lt;/td&gt;&lt;td&gt;
  -50
  &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;em&gt;U(m)&lt;/em&gt; would then be:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:preformatted --&gt;

&lt;p&gt;&lt;pre class=&#34;wp-block-preformatted&#34;&gt;U(m) = TP(n&lt;sub&gt;1&lt;/sub&gt;) + TN(n&lt;sub&gt;2&lt;/sub&gt;) - 5FP(n&lt;sub&gt;3&lt;/sub&gt;) - 50FN(n&lt;sub&gt;4&lt;/sub&gt;)&lt;/pre&gt;
&lt;!-- /wp:preformatted --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;where &lt;em&gt;n&lt;sub&gt;i &lt;/sub&gt;&amp;nbsp;&lt;/em&gt;are the respective case counts of each prediction outcome. &lt;em&gt;U(m) &lt;/em&gt;can then be used as the loss function in tuning individual ML models, heavily penalizing false negatives.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The utility scores of each prediction outcome is a function of the expert judgement of clinicians and practitioners, in evaluating relative costs and tradeoffs between each outcome. They are also largely context-driven and should ideally differ between surgical complications, diseases, or even hospitals and departments. Finally, loss functions are general to supervised learning algorithms, i.e. &lt;em&gt;U(m)&lt;/em&gt; can be experimented with and built into various ML models.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Of course, the challenge then, in using utility functions in machine learning, is in the design of the function - how to best to capture the inherent asymmetry and tradeoffs, and evaluate and summarize relative costs. This has to be done together with domain experts as they are the ones who can providing the expert opinion and judgement, in articulating the tradeoffs in utility. While network intrusion and employee attrition can be quantified in dollar value, the loss of a human life is definitely not so straightforward.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Stylized facts in statistical finance</title>
      <link>/post/2019/04/04/short-stylized-facts-in-statistical-finance/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/04/04/short-stylized-facts-in-statistical-finance/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://miro.medium.com/max/1280/1*LZnClq6o0kEU88-BZnkvCw.jpeg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;In statistical finance (as well as in certain areas in social science), &lt;a href=&#34;https://en.wikipedia.org/wiki/Stylized_fact&#34;&gt;stylized facts&lt;/a&gt; are empirical observations summarized in a theory-like form, and are generally representative under broad circumstances.&lt;/p&gt;

&lt;p&gt;Stylized facts are commonly used in statistical finance to address and summarize phenomenon directly observed from historical data, and explainable with a certain level of theoretical consistency and logic (&#34;... this makes sense to me&#34;). Stylized facts may often guide investigations and explorations into certain topics of interest, functioning perhaps as assumptions to be either built upon or proven otherwise. Specifically, invalidating a stylized fact could be considered insightful, much like using data analysis and machine learning to invalidate certain entrenched assumptions held by business users.&lt;/p&gt;

&lt;p&gt;In this first statistical finance post, I would like to list out a couple of stylized facts (SF), without explanation, that are observed and used in finance, for future referencing as well as for my own explorations.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(SF1)&lt;/strong&gt; Financial data has extremely low signal-to-noise ratio.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF2)&lt;/strong&gt; Price changes are less volatile in bull markets and more volatile bear markets.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF3)&lt;/strong&gt; The autocorrelations of the returns of an asset is weak, hence univariate prediction of returns is not possible. However, volatility autocorrelation is strong, and volatility can be reasonably predicted (using e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity&#34;&gt;GARCH&lt;/a&gt; models).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF4)&lt;/strong&gt; Financial data usually have more extreme events or values than expected (fat tails, &lt;a href=&#34;https://en.wikipedia.org/wiki/Black_swan_theory&#34;&gt;black swans&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF5)&lt;/strong&gt; Tail dependence between different assets are high, making &lt;a href=&#34;https://en.wikipedia.org/wiki/Copula_(probability_theory)&#34;&gt;copula&lt;/a&gt; fitting worthwhile.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF6)&lt;/strong&gt; Past price changes are negatively correlated with future volatility.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF7)&lt;/strong&gt; Aggregate stock market returns (e.g. an index) display negative skewness; individual stock returns display positive skewness.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF8)&lt;/strong&gt; In the long run, stocks in a given market will be positively correlated.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(SF9)&lt;/strong&gt; Volatility clustering is typically observed in financial data; that is, large changes tend to be followed by large changes of either sign, and small changes tend to be followed by small changes of either sign.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;There are probably more and more advanced ones that I have yet to come across, so I will probably add on to the list over time. In any case, I will start with these SFs and other techniques in the meantime, in my exploration of statistical finance. I also be using these SF indices (SF1 etc.) as references in future posts to reduce verbose.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reviewing means of stashing cash with impending recession in sight</title>
      <link>/post/2019/03/28/reviewing-means-of-stashing-cash-with-impending-recession-in-sight/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/28/reviewing-means-of-stashing-cash-with-impending-recession-in-sight/</guid>
      <description>&lt;!-- wp:image --&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img src=&#34;https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ikhFGzC1peMs/v4/pidjEfPlU1QWZop3vfGKsrX.ke8XuWirGYh1PKgEw44kE/775x-1.png&#34; alt=&#34;&#34;/&gt;&lt;figcaption&gt;Inversion of US Treasury yield spread. &lt;a href=&#34;https://www.bloomberg.com/news/articles/2019-03-22/u-s-treasury-yield-curve-inverts-for-first-time-since-2007&#34;&gt;Source&lt;/a&gt;. &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/center&gt;
&lt;!-- /wp:image --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;With the &lt;a href=&#34;https://www.bloomberg.com/news/articles/2019-03-22/u-s-treasury-yield-curve-inverts-for-first-time-since-2007&#34;&gt;US Treasury yield curve inverting for the first time since 2007&lt;/a&gt;, we have our first reliable signal from the market that a recession is around the corner, perhaps &lt;a href=&#34;https://www.bloomberg.com/news/articles/2019-02-25/most-economists-see-u-s-recession-by-2021-nabe-survey-shows&#34;&gt;by 2021&lt;/a&gt; or earlier.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;From a simple retail investing standpoint, an impending recession means that we want to stash away cash, and perhaps load some of it into our brokerage accounts when the fire sale swings by. Load and ready.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;My current means of stashing cash - DBS Multiplier and Singapore Savings Bonds&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The &lt;a href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34;&gt;DBS Multiplier&lt;/a&gt; account is of course &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/&#34;&gt;one of my workhorse accounts&lt;/a&gt; for monthly working capital. The &lt;a href=&#34;http://www.sgs.gov.sg/savingsbonds/About-SSB/Overview.aspx&#34;&gt;Singapore Savings Bonds&lt;/a&gt; (SSB) have been a crucial part of my funds management, and form part of my monthly savings and source of passive income (with a &lt;a href=&#34;https://kpo-and-czm.blogspot.com/2018/04/dbs-multiplier-ssbs-joint-account-higher-interest.html&#34;&gt;SSB bond ladder&lt;/a&gt;).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;I view SSBs as an intermediate form of liquidity with moderate yields, with cash in my DBS Multiplier account being more liquid and my dollar cost averaging (DCA) investments being less liquid (may inevitably realize losses if cash is required). On the other hand, SSBs have pro-rated interest payout and a one-month redemption latency with no penalty - functioning well as intermediate liquidity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;(In addition, MAS recently (February 2019) &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/12/21/short-good-news-for-fans-of-singapore-savings-bonds/&#34;&gt;announced that SSBs can be purchased using SRS funds, via the respective SRS operators&lt;/a&gt;. This meant that the new risk-free rate in our &lt;a href=&#34;https://www.mof.gov.sg/MOF-For/Individuals/Supplementary-Retirement-Scheme-SRS&#34;&gt;SRS&lt;/a&gt; accounts becomes the SSB yields, making the use of SRS for tax breaks that much more attractive.)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;A reasonable point of view on future SSB interest rates trends&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:image {&#34;align&#34;:&#34;center&#34;,&#34;id&#34;:300,&#34;width&#34;:697,&#34;height&#34;:424} --&gt;

&lt;p&gt;&lt;div class=&#34;wp-block-image&#34;&gt;&lt;figure class=&#34;aligncenter is-resized&#34;&gt;&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/03/image-3.png&#34; alt=&#34;&#34; class=&#34;wp-image-300&#34; width=&#34;697&#34; height=&#34;424&#34;/&gt;&lt;figcaption&gt;Trending of historical SSB per annum interest rates&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;
&lt;!-- /wp:image --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;This month&amp;rsquo;s (purchase March 2019 for April 2019 issue) average 10-year yield is sitting at 2.16%. If you paid attention to SSB yields in the recent months, there seem to be an imminent downward trend, down from a peak of 2.57% in the December 2019 issue.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Furthermore, with the &lt;a href=&#34;https://www.bloomberg.com/news/articles/2019-03-20/fed-sees-no-2019-hikes-plans-september-end-to-asset-drawdown&#34;&gt;US Federal Reserve making an announcement in earlier this month there will likely be no more interest hikes in 2019&lt;/a&gt;, it does look to me that the SSB interest rates will be remaining at current levels of about 2.15% to 2.20%, give or take. For context, DBS Multiplier is able to give me between 2.20% to 2.30% on 50K of funds. The appeal of SSBs is certainly going to pale in the months to come.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;What&amp;rsquo;s actionable for me?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;When these pieces of information are assembled, in addition to my anticipation of some big-item purchases within the mid-term horizon, it seems worthy of my time and effort to do review my monthly funds management. Specifically, after discounting for expenses and payments, in between keeping 50K in DBS Multiplier and monthly purchases of SSB, there can be one additional location for me to stash cash - in decreasing liquidity:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;(salary minus expenses and payments)&lt;/li&gt;&lt;li&gt;(cash) Cash in DBS Multiplier, up to bonus interest cap&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(cash) Additional cash stash&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;(SSB) Monthly purchase of SSB&lt;/li&gt;&lt;li&gt;(investments) Monthly DCA investments via RSPs&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;Where to stash cash?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In this context, I figured the best way to fulfill my need is &lt;em&gt;another high-yield savings account that does not rely on salary, GIRO payments, credit card spending or any of such requirements&lt;/em&gt;, as these are generally already taken up by my DBS Multiplier account. Fixed deposits and the like won&amp;rsquo;t cut it, due to low relative yields and even lower liquidity. With that mind, a quick internet search gave the following shortlist:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://www.posb.com.sg/personal/deposits/savings-accounts/saye&#34;&gt;POSB Save-As-You-Earn (SAYE) account&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://www.cimbbank.com.sg/en/personal/products/accounts/savings-accounts/cimb-fastsaver-account.html&#34;&gt;CIMB FastSaver account&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://www.citibank.com.sg/gcb/deposits/mxgn-savacc.htm&#34;&gt;Citi MaxiGain Savings account&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;And following is a straightforward comparison:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Account&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Interest rate&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;My take for my needs&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;POSB SAYE&lt;/td&gt;&lt;td&gt;2%&lt;/td&gt;&lt;td&gt;Save fixed amount every month, no touching for 2 years&lt;/td&gt;&lt;td&gt;2 years is ok, though I prefer more flexibility for the step-up amount. Fixing the amont does not appeal to me.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CIMB FastSaver&lt;/td&gt;&lt;td&gt;1%&lt;/td&gt;&lt;td&gt;Flat, no conditions no nothing&lt;/td&gt;&lt;td&gt;Too low - though &amp;ldquo;no conditions&amp;rdquo; seems terrific as a workhorse account post-retirement. Haven&amp;rsquo;t research on this yet.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Citi MaxiGain&lt;/td&gt;&lt;td&gt;~2.2%, depending on 1-month SIBOR&lt;/td&gt;&lt;td&gt;70K to enjoy base interest rate of 70% of 1-month SIBOR (currently at &lt;a href=&#34;https://abs.org.sg/rates-sibor&#34;&gt;1.83%&lt;/a&gt;);&lt;br&gt;Step-up bonus of 0.1% per month, up to 1.2%&lt;br&gt;&lt;br&gt;(70K is also meets the criteria for &lt;a href=&#34;https://www.citibank.com.sg/citi-priority/index.htm&#34;&gt;Citi Priority&lt;/a&gt; banking.)&lt;/td&gt;&lt;td&gt;Seems right, aligned with my needs. 70K is reasonable for the base rate, high potential rates, step-up amount is flexible unlike SAYE. Citi Priority as a bonus.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;Citi MaxiGain Savings Account&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;It is pretty clear to me what I should go for. A bit more details on the Citi MaxiGain Savings account (quoted from the &lt;a href=&#34;https://www.citibank.com.sg/gcb/deposits/mxgn-savacc.htm&#34;&gt;MaxiGain page&lt;/a&gt; FAQ - emphasis my own):&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;The Bank shall pay you a base interest rate (&amp;ldquo;Base Interest Rate&amp;rdquo;) at 70% of the 1 month Singapore Dollar Singapore Interbank Offer Rate (&amp;ldquo;1 month SIBOR&amp;rdquo;) on your daily balance, up to the first S$150,000 in your MaxiGain account. For example, if 1 month SIBOR is 1.43% p.a., the Base Interest Rate shall be 1% p.a.&lt;/li&gt;&lt;li&gt;You need to maintain a daily end of day balance of at least $70,000 in your MaxiGain account to earn (the base) interest. Interest at the Base Interest Rate will accrue daily, if you maintain at least S$70,000 in your MaxiGain account, based on the daily end of day balance and will be paid on the last day of the month.&lt;/li&gt;&lt;li&gt;Your balances capped at S$150,000 shall accrue bonus interest at a rate (&amp;ldquo;Bonus Interest Rate&amp;rdquo;) that steps up each month, from 0.10% p.a. to a maximum of 1.20% p.a., &lt;strong&gt;if the lowest balance in your MaxiGain account in a month is equal to or greater than the lowest balance in the previous month&lt;/strong&gt;.&lt;/li&gt;&lt;li&gt;The Counter increases by 1 each time the Bonus Interest Rate steps up. &lt;strong&gt;The Bonus Interest Rate steps up in a month if the lowest balance in that month is equal to or greater than the lowest balance in the preceding month&lt;/strong&gt;. Bonus interest is computed based on the preceding month&amp;rsquo;s lowest balance and number of calendar days in the preceding month. &lt;strong&gt;The lowest amount of funds in your MaxiGain account at any point in time in a month shall be the &amp;ldquo;lowest balance&amp;rdquo; of that month&lt;/strong&gt;. &lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Seems to me it is easy to manage the monthly account balance to ensure that the Counter increases by 1 every month, up to 12 times. Certainly easier than the SAYE account which is less flexible.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;With that, the final picture should look like this - again in decreasing liquidity:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;(salary minus expenses and payments)&lt;/li&gt;&lt;li&gt;(cash) Cash in DBS Multiplier, up to bonus interest cap&lt;/li&gt;&lt;li&gt;&lt;strong&gt;(cash) Monthly deposit into Citi MaxiGains&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;(SSB) Monthly purchase of SSB&lt;/li&gt;&lt;li&gt;(investments) Monthly DCA investments via RSPs&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Time to get off the laptop and drop by a Citibank branch!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seven tips for working on analytics delivery projects</title>
      <link>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/17/seven-tips-for-working-on-analytics-delivery-projects/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/03/4c7f8-creativitygap.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Following are seven tips / tricks / hacks that I came to learnt (some of them the hard way) and compiled as a data scientist / delivery consultant / data science consultant. In brief, they are:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;You to Yourself&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Develop a strategy&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Keep a delivery journal&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan your daily activities&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Frontload your projects&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;You to Others&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Show mediocre output to no one&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Follow up on everything&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Have 30 seconds responses to every possible question from the customer you can think of&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before I elaborate, let me clarify that by &amp;ldquo;customers&amp;rdquo;, I mean anyone who is related to the project, most possibly only with the exception of yourself, your teammates, and your project manager. If you work as a consultant, the idea of a customer is obvious. If you work in an in-house analytics outfit, then your customer is someone who will use your final output; could be your boss, the business owners, the IT department and engineers, the end-users etc.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;YOU TO YOURSELF&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;DEVELOP A STRATEGY&lt;/b&gt; - At the start of the project, develop a strategy or a plan. Take a piece of paper and a pen, write down how you want to tackle the problem. &lt;b&gt;Writing it down is key&lt;/b&gt;. Consider everything - how data flows from point A and B, what models do you want to try/you think might work, what data pre-proc do you think you need/might be necessary, what is the ideal set of results/output for you - down to the data structures (R: data frame, list, vector, matrix; python: pandas dataframe, dictionaries, lists; pyspark: broadcasts, accumulators, local vs. distributed etc.), what packages do you think you will require (version numbers/compatibility?), what visualizations do you want to see, what would the ideal plot look like, what assumptions are you making, how much time do you think you need for each task, how big are the intermediate results, is the cluster/HDFS sized correctly, what difficulties do you think you will face. Draw flowcharts and diagrams, draw your pipelines. &lt;b&gt;EVERYTHING&lt;/b&gt;. Again, &lt;b&gt;writing them down is key&lt;/b&gt;. I strongly recommend using pen and paper for this, or a notebook. Don&amp;rsquo;t be afraid to take 1 or 2 hours on this. Be thorough. After you are done, take a picture of it with your phone and save it somewhere. Unless you are extremely clear right at the beginning what you want to do, this should probably be one of many drafts.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;KEEP A DELIVERY JOURNAL&lt;/b&gt; - Keep a delivery journal. Document everything - What happened today, problems faced, how long did a certain procedure take, your modelling strategy, your thoughts, your gut feelings, meeting notes, what went wrong, what went right, insights, mistakes… everything. Think of it as a diary. Do it on a daily basis. Show this journal to no one but yourself. Write as the day progresses, don’t wait until the end of the day. If you find it hard to do this, try to jot down in concise but substantive points, and expound on them at the end of your day. Also, make reference to the strategy you developed. Did anything change for the better or the worse after today? Personally, I keep a Evernote window open while I work and write in it as the day progresses.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;PLAN YOUR DAILY ACTIVITIES&lt;/b&gt; - Plan your daily activities. At the end of your day, plan what do you want to achieve at the end of the next manday. Write them down. Don’t do this in the customer’s office - do this after you had your dinner, took a shower. I find myself writing more accurate projections of my following manday when I write this at home or in the hotel room, i.e. outside of the office. I do this using Evernote as well.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;FRONTLOAD YOUR PROJECT&lt;/b&gt; - To frontload your project means to do as much of the work as possible at the start of the project - preferably in the first week. I always had this idea subconsciously, even when I was still in university, but never verbalized it until I read &lt;a href=&#34;https://www.bookdepository.com/McKinsey-Edge-Success-Principles-from-Worlds-Most-Powerful-Consulting-Firm-Shu-Hattori/9781259588686&#34;&gt;The McKinsey Edge by Shu Hattori&lt;/a&gt;. Basically, if you say this to yourself: “This is the first week of the project, so I should just take it easy”, &lt;b&gt;YOU ARE DEAD WRONG&lt;/b&gt;. If you have this sentiment during the first 5 mandays of your project, you are not doing the right thing. The first manweek is critical. Use it for the following:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Information (get up to speed with project kickoff materials, timeline, exact format of deliverables etc.)&lt;/li&gt;
&lt;li&gt;Clarifications (clarifications with your customers - this is the most important)&lt;/li&gt;
&lt;li&gt;Strategy (delivery or dev strategy, as above)&lt;/li&gt;
&lt;li&gt;Workflow (software, tools, access credentials)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, frontloading your project does not mean you jump right into developing the scripts and codebase and writing as much as possible. This is counter-productive, and most likely your codebase will turn out to be utterly useless by the second or third manweek. Instead, use the time to get the above issues out of the way, so that once you get into the dev rhythm, you don&amp;rsquo;t have to stop and mind about these pesky nonsense that will hurt your productivity.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;YOU TO OTHERS&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;b&gt;SHOW MEDIOCRE OUTPUT TO NO ONE - AKA you always need a story to tell&lt;/b&gt; - As it turns out, impressions matter even in a delivery project, with defined deliverables and outcomes. When you meet your customers for the first time during presales or sign-off or the like, you get sized up. The next and the most crucial juncture in which you are sized up again is perhaps at your first intermediate output or milestone, whether it&amp;rsquo;s a MVP dashboard or some intermediate flat table of results or a deck depicting your first iteration of modelling using CRISP-DM. Never show mediocre output to your customers, even if you prefaced it with &amp;ldquo;This is just some intermediary results&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you ponder about this point for a little, you might realise that for a modelling project, this might be difficult to accomplish. What if after putting in 2 or 3 manweeks of modelling effort, your ROC-AUC is still stuck at 0.65? In this case, there are several things you can think about and show to your customers, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Some EDA plot or statistic or metric to illustrate the quality of the data, or lack thereof&lt;/li&gt;
&lt;li&gt;Likewise, some plot or statistic or metric to show that one or more assumptions made in the project is not true, but only ostensible or perhaps even outright false. (In the latter case, you should go hammer your presales guy who scoped and sized this project. If you are the one who scoped it, you deserve it.)&lt;/li&gt;
&lt;li&gt;An alternative approach, not limited to changing performance metrics, including additional data or features, targetting or neglecting a specific subsample of the data for subsequent efforts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basically, you need a &lt;b&gt;story&lt;/b&gt;. If your immediate output looks great, great, you have a story to tell. But if your immediate output is less than ideal, then you need to craft a story on how to improve things going forward.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;FOLLOW UP ON EVERYTHING&lt;/b&gt; - This ties in with frontloading your project - follow up on every single doubt you have in your mind. This is important because you will meet customers who know that you, as the delivery consultant, lack a certain piece of critical information, but didn&amp;rsquo;t share it with you anyway - simply because you didn&amp;rsquo;t ask. I had to learn this this hard way. And it&amp;rsquo;s a sure-lose situation for you because there is no good answer to their question &amp;ldquo;Why didn&amp;rsquo;t you ask me?&amp;rdquo;. However, do be careful when you follow up on questions with your customers. Make sure your question is thought-out and well-researched. Remember, impressions count.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;HAVE A 30-SECOND RESPONSE TO EVERY POSSIBLE QUESTION from the customer you can think of&lt;/b&gt; - This is another one that I picked up from the McKinsey Edge, and I really like this a lot. It&amp;rsquo;s simple to implement, yet so impactful and well thought-out. Inevitably, you can&amp;rsquo;t have responses to every single question there is - just make sure you have the responses to the key and obvious questions. For example,&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Why do you use this feature over the other?&lt;/li&gt;
&lt;li&gt;Why do you log-transform this feature?&lt;/li&gt;
&lt;li&gt;Why are you using &lt;programming language A&gt; over &lt;programming language B&gt;?&lt;/li&gt;
&lt;li&gt;Why are you using &lt;machine learning model A&gt; over &lt;machine learning model B&gt;?&lt;/li&gt;
&lt;li&gt;How do you interpret these results?&lt;/li&gt;
&lt;li&gt;How can I use these results?&lt;/li&gt;
&lt;li&gt;Etc etc etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The objective is to not babble like a fool for 2 or 3 mins and have zero concrete ideas or responses put across. No one has the time or patience to listen to your uninsightful babbling. We all know this one person in our workplace who keeps talking continuously but nothing substantial is actually put forth. Therefore, make sure you convey your idea across as concisely and succinctly as possible. 30 seconds is just a heuristic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Review: To Tune or Not to Tune the Number of Trees in Random Forest</title>
      <link>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/24/paper-review-to-tune-or-not-to-tune-the-number-of-trees-in-random-forest/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/02/download2-1.png&#34; width=&#34;100%&#34;&gt;
Plotting different performance metrics against the number of trees in random forest. &lt;a href=&#34;https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I came across the following paper during my Masters coursework that addresses a practical issue in the use of the random forest model, and in general, any other bootstrap aggregating ensembles:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/papers/v18/17-269.html&#34;&gt;Probst, P. &amp;amp; Boulestix, A-L. (2018). To Tune or Not to Tune the Number of Trees in Random Forest. Journal of Machine Learning Research, 18(181), 1-18. &lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;1. Motivation&lt;/h2&gt;

&lt;p&gt;This is an interesting paper as it directly addresses a fundamental question on whether the number of base learners in a bagging ensemble should be tuned. In the case of random forest (RF), the number of trees &lt;em&gt;T&lt;/em&gt; is often regarded as a hyperparameter, in the sense that either too high or too low of a value would yield sub-par model performance. Tuning of &lt;em&gt;T&lt;/em&gt; is typically done by plotting the one or multiple chosen out-of-bag (OOB) metrics, such as the error rate, as a function of &lt;em&gt;T&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/02/download.png&#34; width=&#34;100%&#34;&gt;
Different OOB metrics (error rate, Brier score, log loss, AUC) as a function of &lt;em&gt;T&lt;/em&gt;. &lt;a href=&#34;https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;In the case above, there are clear indications of convergence in &lt;em&gt;T&lt;/em&gt;, and any further increase in &lt;em&gt;T&lt;/em&gt; brings either marginal or zero improvements in model performance. However, that&#39;s not always the case, prompting the treatment of &lt;em&gt;T&lt;/em&gt; as a model hyperparameter:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/02/download2.png&#34; width=&#34;100%&#34;&gt;
Non-convergence of &lt;em&gt;T&lt;/em&gt;. &lt;a href=&#34;https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2&gt;2. Objectives of paper&lt;/h2&gt;

&lt;p&gt;Based on this motivation, the authors move step by the step to give this issue a structured treatment along the following objectives:&lt;/p&gt;

&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table class=&#34;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Quoted from abstract&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Layman explanations&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;(i) Provide theoretical results showing that the expected error rate may be a non-monotonous function of &lt;em&gt;T&lt;/em&gt;, and explaining under which circumstances this happens&lt;/td&gt;&lt;td&gt;Show that the error rate of RF may increase or decrease as &lt;em&gt;T&lt;/em&gt; increases (non-monotonous), depending on a certain phenomenon that could be observed from the testing dataset.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;(ii) Provide theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the log loss (for classification) and the MSE (for regression)&lt;/td&gt;&lt;td&gt;Show that such non-monotonicity cannot be observed for performance metrics such as the Brier score, log loss and MSE, even though it can be observed for the error rate - sorry for the double negative, take a moment to digest that before you get confused further.&lt;br&gt;&lt;br&gt;(It is also shown in the paper that the ROC-AUC follows such non-monotonicity.)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;(iii) Illustrate the extent of the problem through an application to a large number (306) datasets&lt;/td&gt;&lt;td&gt;Validate findings via empirical experimentations using public ML datasets&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;(iv) Argue in favor of setting &lt;em&gt;T&lt;/em&gt; to a computationally feasible large number as long as classical error measures based on average loss are considered&lt;/td&gt;&lt;td&gt;Conclude that &lt;em&gt;T&lt;/em&gt; should not be tuned, but set to a feasibly large number.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h2&gt;3. Structure of paper&lt;/h2&gt;

&lt;ol&gt;&lt;li&gt;Motivation and literature review&lt;/li&gt;&lt;li&gt;Build up RF and various performance metrics for theoretical context&lt;/li&gt;&lt;li&gt;Theoretical treatment (mainly binary classification, results for multiclass classification and regression are extrapolated and inferred)&lt;/li&gt;&lt;li&gt;Empirical validation&lt;/li&gt;&lt;li&gt;Conclusions and extensions&lt;/li&gt;&lt;/ol&gt;

&lt;h2&gt;4. Five takeaways from the authors&lt;/h2&gt;

&lt;p&gt;Without boring you as the reader on the specifics of the theoretical and empirical treatments to the problem, here are the key takeaways from the authors (with my translations):&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Non-monotonicity (as a function of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;T&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;) can only be observed for the error rate and the ROC-AUC&lt;/strong&gt;, but not for typical performance metrics such as the Brier score, log loss, or MSE.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;*The behaviour of the error rate as a function of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;T&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; (error rate curve) is largely dependent on the empirical OOB distributions of the prediction errors ε&lt;/strong&gt;&lt;sub&gt;&lt;strong&gt;i&lt;/strong&gt;&lt;/sub&gt; - this is the most important point in this paper for me, to be elaborated further.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;For regression, non-monotonicity can be observed from median-based (as opposed to mean-based) performance metrics&lt;/strong&gt;. This makes sense since typical recursive partitioning in the base learners goes for squared error minimization.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The biggest OOB performance gain comes from the first 250 trees; going from 250 to 2000 trees yields minimal performance gains&lt;/strong&gt;. This is true for binary classification, multiclass classification, and regression.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;More trees are better&lt;/strong&gt; - non-monotonicity is only observed under specific conditions, with specific performance metrics. Even under non-monotonicity, the difference between the converged metric and its global extreme is minimal. Therefore, &lt;strong&gt;set &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;T&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; to a computationally feasible large number&lt;/strong&gt;.&lt;/li&gt;&lt;/ol&gt;

&lt;h2&gt;5. Empirical OOB distributions of prediction errors&lt;/h2&gt;

&lt;p&gt;One of the key takeaways from this paper is that the behaviour of the error rate as a function of &lt;em&gt;T&lt;/em&gt; (error rate curve) is largely dependent on the empirical OOB distributions of the prediction errors ε&lt;sub&gt;i&lt;/sub&gt;. In the paper, it was shown theoretically that the convergence rate of the error rate curve is only dependent on the distribution of ε&lt;sub&gt;i&lt;/sub&gt;. In particular,&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;In a largely accurate (many observations with ε&lt;sub&gt;i&lt;/sub&gt;  ≈ 0, few observations with ε&lt;sub&gt;i&lt;/sub&gt;  ≈ 1) ensemble, observations with ε&lt;sub&gt;i&lt;/sub&gt; &amp;gt; 0.5 will be compensated by observations with ε&lt;sub&gt;i&lt;/sub&gt; &amp;lt; 0.5 , in such a way that the error rate curve is monotonously decreasing.&lt;/li&gt;&lt;li&gt;However, in ensembles with many ε&lt;sub&gt;i&lt;/sub&gt;  ≈ 0 and a few ε&lt;sub&gt;i&lt;/sub&gt; ≥ 0.5 that are close to 0.5, a problem arises. By the authors&#39; computations, due to these fringe cases (model is uncertain &lt;strong&gt;∴&lt;/strong&gt; ε&lt;sub&gt;i&lt;/sub&gt;  ≈ 0.5), the error rate curve falls down quickly, then grows again slowly to convergence. The following should make a convincing case:&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/02/untitled.png&#34; width=&#34;100%&#34;&gt;
OOB error rate curves for three datasets from OpenML. &lt;a href=&#34;http://jmlr.org/papers/v18/17-269.html&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2019/02/untitled2.png&#34; width=&#34;100%&#34;&gt;
OOB ε&lt;sub&gt;i&lt;/sub&gt; distributions from RF models on the same three datasets. &lt;a href=&#34;http://jmlr.org/papers/v18/17-269.html&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;These are rather high impact findings as &lt;strong&gt;in practice, the empirical OOB distributions of ε&lt;/strong&gt;&lt;sub&gt;&lt;strong&gt;i&lt;/strong&gt;&lt;/sub&gt;&lt;strong&gt; are often not reviewed, especially in the setting of RF or any other ensemble&amp;nbsp;modeling&amp;nbsp;exercise&lt;/strong&gt;. (On the other hand, if your training is in statistics, then linear regression diagnostics should be very familiar. Just sayin&#39;.)&lt;/p&gt;

&lt;h2&gt;6. Practical implications from this paper&lt;/h2&gt;

&lt;p&gt;Finally, all that talk for what we can do in practice:&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;As part of model maintenance, check on the distribution of ε&lt;sub&gt;i&lt;/sub&gt;. When concept drift kicks in in the future, your model could become doubly misspecified - one count from training-testing data disparity and one count on the selection of &lt;em&gt;T&lt;/em&gt;. Also notice that the distribution of ε&lt;sub&gt;i&amp;nbsp;&lt;/sub&gt;is a function of e.g. size of the dataset (both &lt;em&gt;n&lt;/em&gt; and &lt;em&gt;p&lt;/em&gt;). This should be intuitive.&lt;/li&gt;&lt;li&gt;Understand the behavior of your selected performance metric as a function of &lt;em&gt;T&lt;/em&gt;. To give a naive example, optimizing ROC-AUC vs. optimizing log loss with respect to &lt;em&gt;T&lt;/em&gt; should now be very different to you.&lt;/li&gt;&lt;li&gt;Finally, under normal or slightly perturbed conditions (again on &lt;br&gt;distribution of ε&lt;sub&gt;i)&lt;/sub&gt; , a large &lt;em&gt;T&lt;/em&gt; is still better.&lt;/li&gt;&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Do Less, Better</title>
      <link>/post/2019/02/23/do-less-better/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/23/do-less-better/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://dailystoic.com/wp-content/uploads/2018/10/ds_Podcast.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:image --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img src=&#34;&#34; alt=&#34;&#34;/&gt;&lt;figcaption&gt;The Daily Stoic podcast&lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- /wp:image --&gt;&lt;/p&gt;

&lt;!-- wp:quote --&gt;

&lt;p&gt;&lt;blockquote class=&#34;wp-block-quote&#34;&gt;&lt;p&gt;Here’s the simple recipe for improvement and for happiness. It comes from Marcus Aurelius and the fact that it came from such a busy man with so many obligations and responsibilities should not be forgotten.&amp;nbsp;“&lt;strong&gt;If you seek tranquility&lt;/strong&gt;,” he said, “&lt;strong&gt;do less.&lt;/strong&gt;”&amp;nbsp;And then he follows the note to himself with some clarification. Not nothing, less. &lt;strong&gt;Do only what’s essential.&lt;/strong&gt; “Which brings a double satisfaction,” he writes “to do less, better.”&lt;/p&gt;&lt;p&gt;Follow this advice today and everyday. So much of what we think we must do, so much of what we end up doing is not essential. We do it out of habit. We do it out of guilt. We do it out of laziness or we do it out of greedy ambition. And then we wonder why our performance suffers. We wonder why our heart isn’t really in it.&amp;nbsp;Of course it isn’t. &lt;strong&gt;We know deep down there’s no point&lt;/strong&gt;.&amp;nbsp;But if we could do less inessential stuff, &lt;strong&gt;we’d be able to better do what is essential.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We’d also get a taste of that tranquility that Marcus was talking about. A double satisfaction.&amp;nbsp; &lt;/p&gt;&lt;cite&gt;The Daily Stoic&lt;/cite&gt;&lt;/blockquote&gt;
&lt;!-- /wp:quote --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The transcript above (emphasis my own) comes from one of the recent episodes in &lt;a href=&#34;https://dailystoic.com/podcast/&#34;&gt;&amp;ldquo;The Daily Stoic&amp;rdquo;&lt;/a&gt; podcast, talking about the idea of &lt;a href=&#34;https://open.spotify.com/episode/5WiVTsMI5oBoipBEZB4Q3X&#34;&gt;&amp;ldquo;Do Less, Better&amp;rdquo;&lt;/a&gt; . In this modern age of rapid information and content flow day in, day out, we might sometimes find ourselves lost in figuring out our true objectives in life. There seems to be too many things to work on and optimize, with too little time.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h2&gt;What then, is (in)essential?&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;It&amp;rsquo;s not difficult to comprehend the underlying message at first glance - focus on what&amp;rsquo;s essential (do less), and do them well (better). What then, would be considered essential? Well, for most people, it would along the lines of health, wealth, family and friends, etc.  As I reflect on my experiences and perspective in life today, these are the things that I deem are essential to me:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;My health - maintaining my health with diet and exercise&lt;/li&gt;&lt;li&gt;My livelihood - increasing my proficiency and knowledge in data science and machine learning, as this is my current and (very likely) future livelihood&lt;/li&gt;&lt;li&gt;My family and friends&lt;/li&gt;&lt;li&gt;Reading and writing - the best outlets for thoughts, reflections, analysis, and planning&lt;/li&gt;&lt;li&gt;My finances - investing well, seeking sustainable, feasible passive and alternative income. Instead of trading time for money like everyone in this modern rat race of life, choose to trade money for time and experience in this world.&lt;/li&gt;&lt;li&gt;My skills - self-sufficient skills that reasonably reduces living expenses and my reliance on society and others.&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;However,  I find it a lot more instructive in this case to think about what then would be inessential? Think about that for a moment. What is inessential in your life?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;~&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;To me, there is a clear relationship between essential/inessential and needs/wants. Inessential things are things that you don&amp;rsquo;t need, but they became talking points and occupy your &amp;ldquo;brainspace&amp;rdquo; in the first place simply because you want these things. And there are many things out there that induces a sense of want, though they are not necessarily needed. This is especially so on the typical social media platforms.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In this sense, to be truly efficient and satisfied, identify things that you might once thought you really need, but not necessarily so. Then, &lt;strong&gt;Do Less&lt;/strong&gt; - focus on the essential, and do them &lt;strong&gt;Better&lt;/strong&gt;. Spend your waking time and your physical and mental energy on them to optimize or master them.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;I am in this stage in my life right now where my focus in life gets sharper as time passes. Yes, sometimes I do get a little lost and confused, especially when my mental guard is down. However, that sense of tranquility will always kick in when you realize that at the end of the day, your time has been well-spent, improving, optimizing, and mastering the essential, while weeding out the inessential. It is indeed a satisfying experience.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] My Master of Science in Statistics programme in NUS</title>
      <link>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/09/short-my-master-of-science-in-statistics-programme-in-nus/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://www.science.nus.edu.sg/wp-content/uploads/2020/02/fos-logo.png&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I have gotten quite a couple of questions regarding my current &lt;a href=&#34;https://www.stat.nus.edu.sg/index.php/prospective-students/graduate-programme/m-sc-by-coursework-programme&#34;&gt;MSc Statistics programme&lt;/a&gt; in NUS. Here are some broadstroke information about the programme and how I am approaching it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I&amp;rsquo;m doing the MSc by Coursework programme, which means a research thesis is not part of my curriculum. A MSc by Research option is available.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Under the Coursework programme, there is a Track 1 (40MC) programme and a Track 2 (80MC) one. Basically dependent on whether you have a Honours in your Bachelor&amp;rsquo;s degree. I&amp;rsquo;m doing the Track 1 programme - 40MC is equivalent to 10 modules. Under usual circumstances, it takes 2 full-time semesters to finish 10 modules, i.e. 1 academic year. Semesters run as per typical undergraduate semesters in Singapore.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;There&amp;rsquo;s also the part-time option, where one would take 4 to 5 semesters to finish the 10 modules - 5 semesters is basically 2 modules x 5 semesters = 10 modules.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I&amp;rsquo;m on the part-time programme. Personally, 3 modules on a part-time basis per semester is too much for me to handle - so I opt to finish my MSc in 5 semesters, or 2.5 academic years.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I&amp;rsquo;m currently in my 4th semester, so would be finishing the programme requirements by Dec 2019 and graduate during July 2020 (commencement).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For MSc Statistics, lectures typically run from 7pm to 10pm weeknights. Each module has 1 lecture per week, with the typical workload of tutorials, homework assignments, individual or group projects, subjected to respective lecturer&amp;rsquo;s discretion.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, the programme &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GDTuitionCurrent.pdf&#34;&gt;cost&lt;/a&gt; &lt;b&gt;$2,500 per semester for Singaporeans who are taking this MSc programme as their first higher qualification programme under the &lt;a href=&#34;http://www.nus.edu.sg/registrar/info/gd/GD-Eligibility-Guidelines.pdf&#34;&gt;MOE Subsidy&lt;/a&gt;&lt;/b&gt;. Yes it&amp;rsquo;s pretty value for money if you ask me. This tuition fee amount is not unique to MSc Statistics, and is general to many other programmes in NUS, again provided if you belong to the above category.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My experience with the programme is a positive one so far.  Difficulty and commitment level is within my comfort zone, and I managed to learn quite a couple of new things. Modules that I have taken include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Applied Data Mining&lt;/li&gt;
&lt;li&gt;Advanced Statistical Methods in Finance&lt;/li&gt;
&lt;li&gt;Spatial Statistics&lt;/li&gt;
&lt;li&gt;Statistical Analysis of Networks&lt;/li&gt;
&lt;li&gt;Experimental Design&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, in any case, it feels good to be a student again. Each time I go to my stats lecture after a long day of work, it &lt;em&gt;almost&lt;/em&gt; always feels therapeutic. Yea, almost.&lt;/p&gt;

&lt;p&gt;Finally, if you are looking to advance your data science street cred via a postgraduate degree, this is just one of many options, even within NUS or Singapore. Do your research wisely before committing to any!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using waterfall charts to visualize feature contributions</title>
      <link>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/24/using-waterfall-charts-to-visualize-feature-contributions/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I am using waterfall charts drawn in ggplot2 to visualize GLM coefficients, for regression and classification. Source Rmd file can be found &lt;a href=&#34;https://github.com/thestatsguy/thestatsguy/blob/master/content/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Waterfall chart: inspired by their commonplace use in finance&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a simple visualization to illustrate the constituent components (numeric values) that make up the final model prediction, starting from the intercept term &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;. The idea is quickly see which features contribute positively and which negatively, and by how much. Important thing to note here is that the waterfall chart will differ from test datapoint to test datapoint - we first have to make a prediction using a test sample &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2, ..., x_p]\)&lt;/span&gt;, get the prediction, then visualize individual &lt;strong&gt;absolute&lt;/strong&gt; feature contribution to the prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/static/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-1.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature contributions chart: this one is simpler. Same idea as above (also dependent on test sample &lt;span class=&#34;math inline&#34;&gt;\([x_1, x_2, ..., x_p]\)&lt;/span&gt;), but plotted by ranking the numeric contributions by their proportions &lt;strong&gt;relative&lt;/strong&gt; to the prediction&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; like this: &lt;code&gt;contribution_proportion = feature_contribution / prediction&lt;/code&gt;, written below as &lt;code&gt;cont_prop &amp;lt;- featcont/pred&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/static/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.3
library(magrittr)
library(ggplot2)

data(Boston)
set.seed(123)

# mean centering
b2 &amp;lt;- preProcess(Boston, method = &amp;quot;center&amp;quot;) %&amp;gt;% predict(., Boston)

idx &amp;lt;- createDataPartition(b2$medv, p = 0.8, list = FALSE)
train &amp;lt;- Boston[idx,]
test &amp;lt;- Boston[-idx,]

mod0 &amp;lt;- lm(data = train, medv ~.)

sm &amp;lt;- summary(mod0)
betas &amp;lt;- sm$coefficients[,1]

testcase &amp;lt;- test[1,]
pred &amp;lt;- predict(mod0, testcase)

# dot product between feature vector and beta
featvec &amp;lt;- testcase[-which(testcase %&amp;gt;% names == &amp;quot;medv&amp;quot;)] %&amp;gt;% as.matrix
betas2 &amp;lt;- betas[-1]

nm &amp;lt;- names(betas)
#betas2 %*% t(featvec)

# feature contributions
featcont &amp;lt;- betas2*featvec
featcont &amp;lt;- c(betas[1], featcont, pred)
names(featcont) &amp;lt;- c(nm, &amp;quot;Prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;waterfall-chart-on-regression-feature-contributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Waterfall chart on regression feature contributions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# waterfall chart on feature contribution
plotdata &amp;lt;- data.frame(coef = names(featcont), featcont = featcont, row.names = NULL)
plotdata$coef &amp;lt;- factor(plotdata$coef, levels = plotdata$coef)
plotdata$id &amp;lt;- seq_along(plotdata$coef)
plotdata$Impact &amp;lt;- ifelse(plotdata$featcont &amp;gt; 0, &amp;quot;+ve&amp;quot;, &amp;quot;-ve&amp;quot;)
plotdata[plotdata$coef %in% c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;Prediction&amp;quot;), &amp;quot;Impact&amp;quot;] &amp;lt;- &amp;quot;Initial/Net&amp;quot;
plotdata$end &amp;lt;- cumsum(plotdata$featcont)
plotdata$end &amp;lt;- c(head(plotdata$end, -1), 0)
plotdata$start &amp;lt;- c(0, head(plotdata$end, -1))
plotdata &amp;lt;- plotdata[, c(3, 1, 4, 6, 5, 2)]

gg &amp;lt;- ggplot(plotdata, aes(fill = Impact)) +
 geom_rect(aes(coef,
               xmin = id - 0.45,
               xmax = id + 0.45,
               ymin = end,
               ymax = start)) +
 theme_minimal() +
 #scale_fill_manual(values=c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;))
 scale_fill_manual(values=c(&amp;quot;darkred&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkblue&amp;quot;)) +
 theme(axis.text.x=element_text(angle=90, hjust=1))
## Warning: Ignoring unknown aesthetics: x
#coord_flip()

if(sign(plotdata$end[1]) != sign(plotdata$start[nrow(plotdata)]))
 gg &amp;lt;- gg + geom_hline(yintercept = 0)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
cont_prop &amp;lt;- featcont/pred

plot_data &amp;lt;- data.frame(coef = names(cont_prop),
                        cont_prop = cont_prop,
                        row.names = NULL)
plot_data &amp;lt;- plot_data[-nrow(plot_data),]

plot_data &amp;lt;- plot_data[order(plot_data$cont_prop, decreasing = FALSE),]

plot_data$coef &amp;lt;- factor(plot_data$coef, levels = plot_data$coef)

p&amp;lt;-ggplot(data=plot_data, aes(x=coef, y = cont_prop)) +
    geom_bar(stat=&amp;quot;identity&amp;quot;, fill = &amp;quot;darkblue&amp;quot;) +
    coord_flip() +
    theme_minimal() +
    xlab(&amp;quot;Features&amp;quot;) +
    ggtitle(&amp;quot;Feature Contributions&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/r_prep2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;div id=&#34;preparation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kernlab)
## Warning: package &amp;#39;kernlab&amp;#39; was built under R version 3.5.3
## 
## Attaching package: &amp;#39;kernlab&amp;#39;
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     alpha
library(caret)
library(magrittr)
library(ggplot2)

data(spam)
set.seed(123)

# mean centering
s2 &amp;lt;- preProcess(spam, method = &amp;quot;center&amp;quot;) %&amp;gt;% predict(., spam)

idx &amp;lt;- createDataPartition(s2$type, p = 0.8, list = FALSE)
train &amp;lt;- s2[idx,]
test &amp;lt;- s2[-idx,]

mod0 &amp;lt;- glm(data = train, type ~., family =  binomial(link = logit))
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

sm &amp;lt;- summary(mod0)
betas &amp;lt;- sm$coefficients[,1]

testcase &amp;lt;- test[1,]
pred &amp;lt;- predict(mod0, testcase)

# dot product between feature vector and beta
featvec &amp;lt;- testcase[-which(testcase %&amp;gt;% names == &amp;quot;type&amp;quot;)] %&amp;gt;% as.matrix
betas2 &amp;lt;- betas[-1]

nm &amp;lt;- names(betas)
#betas2 %*% t(featvec)

# feature contributions
featcont &amp;lt;- betas2*featvec
featcont &amp;lt;- c(betas[1], featcont, pred)
names(featcont) &amp;lt;- c(nm, &amp;quot;Prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;waterfall-chart-on-classification-feature-contributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Waterfall chart on classification feature contributions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# waterfall chart on feature contribution
plotdata &amp;lt;- data.frame(coef = names(featcont), featcont = featcont, row.names = NULL)
plotdata$coef &amp;lt;- factor(plotdata$coef, levels = plotdata$coef)
plotdata$id &amp;lt;- seq_along(plotdata$coef)
plotdata$Impact &amp;lt;- ifelse(plotdata$featcont &amp;gt; 0, &amp;quot;+ve&amp;quot;, &amp;quot;-ve&amp;quot;)
plotdata[plotdata$coef %in% c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;Prediction&amp;quot;), &amp;quot;Impact&amp;quot;] &amp;lt;- &amp;quot;Initial/Net&amp;quot;
plotdata$end &amp;lt;- cumsum(plotdata$featcont)
plotdata$end &amp;lt;- c(head(plotdata$end, -1), 0)
plotdata$start &amp;lt;- c(0, head(plotdata$end, -1))
plotdata &amp;lt;- plotdata[, c(3, 1, 4, 6, 5, 2)]

gg &amp;lt;- ggplot(plotdata, aes(coef, fill = Impact)) +
 geom_rect(aes(x = coef,
               xmin = id - 0.45,
               xmax = id + 0.45,
               ymin = end,
               ymax = start)) +
 theme_minimal() +
 #scale_fill_manual(values=c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;))
 scale_fill_manual(values=c(&amp;quot;darkred&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkblue&amp;quot;)) +
 theme(axis.text.x=element_text(angle=90, hjust=1))
## Warning: Ignoring unknown aesthetics: x
 #coord_flip()
 
if(sign(plotdata$end[1]) != sign(plotdata$start[nrow(plotdata)]))
 gg &amp;lt;- gg + geom_hline(yintercept = 0)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-24-using-waterfall-charts-to-visualize-feature-contributions_files/figure-html/c_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Something like &lt;a href=&#34;https://en.wikipedia.org/wiki/Waterfall_chart&#34;&gt;this&lt;/a&gt; or &lt;a href=&#34;http://blog.slidemagic.com/2008/08/how-to-create-mckinsey-waterfall-chart.html&#34;&gt;this&lt;/a&gt;, for example&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://thestatsguy.rbind.io/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/&#34;&gt;this&lt;/a&gt; on feature contributions.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>[short] Worked example on setting up SQL Server with R ODBC connection</title>
      <link>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/21/short-worked-example-on-setting-up-sql-server-with-r-odbc-connection/</guid>
      <description>


&lt;p&gt;This is a worked example on how to set up SQL Server, SQL Server Management Studio, and a ODBC connection with R.&lt;/p&gt;
&lt;p&gt;Step 1: Install SQL Server from &lt;a href=&#34;https://www.microsoft.com/en-us/sql-server/sql-server-downloads&#34; class=&#34;uri&#34;&gt;https://www.microsoft.com/en-us/sql-server/sql-server-downloads&lt;/a&gt;. The SQL Server 2017 Express was good enough for me to run some analysis and modelling on my own. Once done, you should have a screen like this:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/post/images/Capture.PNG&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;p&gt;Step 2: Click on the “Install SSMS” button. SSMS stands for SQL Server Management Studio. Once done, connect to the server:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/thestatsguy/thestatsguy/master/public/post/images/Capture3.PNG&#34;&gt;
&lt;/center&gt;
&lt;p&gt;Step 3: Create a database on the server. You may follow the steps given in this page as a quick start: &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/ssms/tutorials/connect-query-sql-server?view=sql-server-2017&#34; class=&#34;uri&#34;&gt;https://docs.microsoft.com/en-us/sql/ssms/tutorials/connect-query-sql-server?view=sql-server-2017&lt;/a&gt;. If you do, you should have a database created named “TutorialDB” and a table named “Customers”.&lt;/p&gt;
&lt;p&gt;Step 4: Install and load the RODBC package in R.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#install.packages(&amp;quot;RODBC&amp;quot;)
library(RODBC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 5: Connect to the server and the database, and run a sample query.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conn &amp;lt;- odbcDriverConnect(&amp;#39;driver={SQL Server};server=SNG1049387\\SQLEXPRESS;database=TutorialDB;trusted_connection=true&amp;#39;)
customers &amp;lt;- sqlQuery(conn, &amp;#39;select * from dbo.Customers&amp;#39;)
str(customers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# &amp;#39;data.frame&amp;#39;: 4 obs. of  4 variables:
#  $ CustomerId: int  1 2 3 4
#  $ Name      : Factor w/ 4 levels &amp;quot;Donna&amp;quot;,&amp;quot;Janet&amp;quot;,..: 4 3 1 2
#  $ Location  : Factor w/ 4 levels &amp;quot;Australia&amp;quot;,&amp;quot;Germany&amp;quot;,..: 1 3 2 4
#  $ Email     : Factor w/ 4 levels &amp;quot;&amp;quot;,&amp;quot;donna0@adventure-works.com&amp;quot;,..: 1 4 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 6: Write an R data frame into your database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df &amp;lt;- read.csv(&amp;quot;data/adult.csv&amp;quot;)
sqlSave(conn, df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 7: Refresh the Database node in SMSS to verify if the data frame has been written into the database as a table.&lt;/p&gt;
&lt;p&gt;You are now ready to use SQL Server, SSMS, and R to run some analysis and modelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Machine Learning Life Cycle: how to run a ML project</title>
      <link>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/20/the-machine-learning-life-cycle-how-to-run-a-ml-project/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-10.41.30-AM-1024x579.png&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;I recently came across this &lt;a href=&#34;https://www.datarobot.com/wiki/machine-learning-life-cycle/&#34;&gt;page&lt;/a&gt; in the &lt;a href=&#34;https://www.datarobot.com/wiki/&#34;&gt;DataRobot Artificial Intelligence Wiki&lt;/a&gt;. If you don&#39;t already know, &lt;a href=&#34;https://www.datarobot.com/sg/&#34;&gt;DataRobot&lt;/a&gt; is currently one of the top &lt;a href=&#34;https://en.wikipedia.org/wiki/Automated_machine_learning&#34;&gt;automated machine learning&lt;/a&gt; platform in the market, with emphasis on supervised learning and citizen data science. I am quite a big fan of their platform - even though I don&#39;t use it in my work, I believe that they and their competitors in the market are heading into the right direction towards automated machine learning. In any case, this is not my intended topic for today.&lt;/p&gt;

&lt;p&gt;If you have worked on data science projects previously, odds are you would have heard of the term CRISP-DM, short of &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining&#34;&gt;CRoss Industry Standard Process for Data Mining&lt;/a&gt;. CRISP-DM was developed by five European countries, including Teradata, in 1997, though it&#39;s now largely recognized as being associated with IBM and SPSS.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/800px-CRISP-DM_Process_Diagram.png&#34; width=&#34;100%&#34;&gt;
The CRISP-DM Process
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The official CRISP-DM manual is this &lt;a href=&#34;ftp://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/18.0/en/ModelerCRISPDM.pdf&#34;&gt;50-page document&lt;/a&gt;, which if I said that I have read it, I would be lying. Nonetheless, CRISP-DM is intuitive enough for me to use it in my work, in order to scope and run data science projects. There are multiple ways to use CRISP-DM, such as manhours scoping and costing and milestones and success criteria setting. So naturally, I was intrigued by the DataRobot&#39;s version of CRISP-DM, and decided to look a little bit deeper.&lt;/p&gt;

&lt;h3&gt;How to run a ML project - a hypothetical walkthrough&lt;/h3&gt;

&lt;p&gt;In essence, I will also use this post to illustrate how a typical data science project can be run and managed hypothetically.&lt;/p&gt;

&lt;h3&gt;Running a project using the DataRobot Machine Learning Cycle - 5 major steps&lt;/h3&gt;

&lt;p&gt;There are five majors steps in the DataRobot (DR) Machine Cycle:&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;Define project objectives&lt;/li&gt;&lt;li&gt;Acquire and explore data&lt;/li&gt;&lt;li&gt;Model data&lt;/li&gt;&lt;li&gt;Interpret and communicate&lt;/li&gt;&lt;li&gt;Implement, document and maintain&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Let&#39;s walk through each of them and look slightly deeper.&lt;/p&gt;

&lt;h3&gt;1. Define project objectives&lt;/h3&gt;

&lt;p&gt;Under this step, there are:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Specify business problem&lt;/strong&gt; - this is usually related to defining the motherhood statement or problem statement of the project. Typically, a problem statement can be broken into multiple usecases. To give you an example in HR analytics, the problem statement of &lt;em&gt;&#34;How can the impact of employee attrition to the company be minimized?&#34;&amp;nbsp;&lt;/em&gt;can be broken down into at least 2 usecases:&lt;ol&gt;&lt;li&gt;Predict the attrition risk of a given employee&lt;/li&gt;&lt;li&gt;Predict the performance of a given employee&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Acquire subject matter expertise&lt;/strong&gt; - primarily, consulting the SMEs to make initial and final assessments on whether the project is feasible. Questions can include potential drivers of phenomenon, availability of data, or potential value created.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Define unit of analysis and prediction target&lt;/strong&gt; - this sounds trivial, right? In actual fact, this can turn out to be the most contentious and make-or-break factor in certain types of projects. To illustrate an example, consider a typical product hierarchy of a company that produces goods. There is typically a SKU or UPN level which describes a product in the most granular terms, which then cascades up multiple levels to the top. &lt;strong&gt;At which level do you think predictions of sales or demand makes the most sense?&lt;/strong&gt; Remember that this directly impacts model count - the lower the level, the more models there will be, while the higher the level, the less meaningful the models might possibly become. It&#39;s interesting how in some companies, demand planning and FP&amp;amp;A are set out into different directions on this from the get-go.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prioritize modelling criteria&lt;/strong&gt; - it&#39;s not abundantly clear to me what modelling criteria meant, but my best guess would be the modelling performance metrics, whether it&#39;s accuracy, log-loss, RMSE, ROC-AUC or others. Each has it&#39;s own context and significance in the business setting.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Consider risks and success criteria&lt;/strong&gt; - this should be straightforward. In many data science and ML projects, the emphasis typically lies in accuracy and/or automation. For example, if the as-is prediction or forecasting is already highly accurate then the emphasis should be on the to-be automation, with a success criteria of certain reduction in FTE.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Decide whether to continue&lt;/strong&gt; - a call that should be co-owned by business, IT, and data science.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;2. Acquire and explore data&lt;/h3&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Find appropriate data&lt;/strong&gt; - yep.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Merge data into a single table&lt;/strong&gt; - yep.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Conduct exploratory data analysis&lt;/strong&gt; (EDA) - this is extremely important. Illustrating and validating business assumptions, as well as retrieving any data artifacts or surprising insights, such as trends or correlations, improves both customer experience and the subsequent modelling process. 1 to 2 iterations of EDA would be ideal.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Find and remove any target leakage&lt;/strong&gt; - yep, though target leakage is not a concept that many are familiar with. Again, the DR AI Wiki does a good job &lt;a href=&#34;https://www.datarobot.com/wiki/target-leakage/&#34;&gt;explaining&lt;/a&gt; target leakage.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Feature engineering&lt;/strong&gt; - yep, though I would say that feature engineering should be a sustained and continuous activity throughout the project. For example, the 1 to 2 EDA iterations should give hints towards potential features to engineer.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;3. Model data&lt;/h3&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Variable selection&lt;/strong&gt; - yep. There are many, many ways to do variable selection in a data-driven manner, but I would like emphasize here on &lt;strong&gt;customer-driven variable selection&lt;/strong&gt;. Build some preliminary models based on a feature set that your customers, with their business knowledge, think are important. These prelim models can serve multiple purposes including EDA, assumption validation and setting a baseline model performance.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Build candidate models&lt;/strong&gt; - yep.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model validation and selection&lt;/strong&gt; - yep.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;4. Interpret and communciate&lt;/h3&gt;

&lt;p&gt;This is where it gets hairy, and where I see most data scientists struggle. Needless to say, this is also one of the most important steps in any project.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Interpret model&lt;/strong&gt; - where to even begin... No customer wants to hear that you are delivering a black box model to them, because no one wants to use a black box for day-to-day operations. Therefore, it&#39;s our job as data scientists to break things as much as possible for our customers. Here are some things that I usually consider:&lt;ul&gt;&lt;li&gt;How the selected algorithms work (in brief)&lt;/li&gt;&lt;li&gt;Why certain features are dropped (e.g. multicollinearity, leakage, low predictive power, low significance, unactionable in the future, low/no data availability in the future)&lt;/li&gt;&lt;li&gt;How are certain features engineered&lt;/li&gt;&lt;li&gt;Which are the important features&lt;/li&gt;&lt;li&gt;How to interpret feature importance&lt;/li&gt;&lt;li&gt;How/why certain test cases are given respective predictions (&lt;a href=&#34;https://thestatsguy.home.blog/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/&#34;&gt;feature contributions&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;How do certain features interact (effect modification, statistical interaction, confounding)&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Communicate model insights&lt;/strong&gt; - build a nice ppt deck with the pointers illustrated above to get customers buy-in.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;5. Implement, document and maintain&lt;/h3&gt;

&lt;p&gt;After buy-in and green light to productionize, last but definitely not the least we do the following:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Set up batch or API prediction system&lt;/strong&gt; - depending on problem statement / usecases / future data availability / business context / customer infrastructure readiness&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Document modelling process for reproducibility&lt;/strong&gt; - don&#39;t be lazy. In fact, this shouldn&#39;t be here in the last steps. This should be done consistently throughout the project!&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Create model monitoring and maintenance plan&lt;/strong&gt; - again dependent on multiple factors. There are multiple ways to refresh a model, from simply retuning with updated data, to start from scratch with the business assumptions. The right answer is always somewhere in the middle.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt;Running data science projects using process models&lt;/h3&gt;

&lt;p&gt;As mentioned above, I wanted to use this post to illustrate how a typical data science project can be run and managed hypothetically. Overall, my understanding on running ML projects is pretty close to the DR Machine Learning Cycle. Finally, note that each of these process models are built with their respective software in mind:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;DR Machine Learning Cycle for DR&lt;/li&gt;&lt;li&gt;CRISP-DM for IBM&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/SEMMA&#34;&gt;SEMMA&lt;/a&gt; for SAS - I didn&#39;t talk about SEMMA because it&#39;s too simplistic and focused on the actual data analysis and modelling rather than the business side of things.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;But that doesn&#39;t mean that these can&#39;t be extrapolated and modified to your needs. That&#39;s all for this post, thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Feature Contribution - another way to think about feature importance</title>
      <link>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/14/feature-contribution-another-way-to-think-about-feature-importance/</guid>
      <description>&lt;!-- wp:image --&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure class=&#34;wp-block-image&#34;&gt;&lt;img src=&#34;https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/XGBoost-Feature-Importance-Bar-Chart.png&#34; alt=&#34;&#34;/&gt;&lt;figcaption&gt;A typical feature importance plot. &lt;a href=&#34;https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/XGBoost-Feature-Importance-Bar-Chart.png&#34;&gt;Image source&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/center&gt;
&lt;!-- /wp:image --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In many machine learning models, feature importance or variable importance is an important output from the model as it informs us about the relative or absolute importance of each feature in contributing to the model. More specifically, feature importance tells us which are the features that are highly differentiating, in the case of classification, and which are those that are not.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;However, one shortfall of feature importance is that it&amp;rsquo;s global in nature - it informs the data scientist about the overall strength of the features as a whole in the dataset. What if something more granular and refined is required? Just because a feature is high up on the feature importance list does not mean that it&amp;rsquo;s always important.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Logistic regression as an example&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Suppose you have some logistic regression model in the form&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:preformatted --&gt;

&lt;p&gt;&lt;pre class=&#34;wp-block-preformatted&#34;&gt;logit(p) =  β&lt;sub&gt;0&lt;/sub&gt; + β&lt;sub&gt;1&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt; + &amp;hellip; + β&lt;sub&gt;p&lt;/sub&gt;x&lt;sub&gt;p&lt;/sub&gt;&lt;/pre&gt;
&lt;!-- /wp:preformatted --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;For a given test case with p features, you would get the predicted probability by substituting each actualized feature value into the model (and do the inverse logit transformation). Naturally, different test cases would have different actualized feature value, summing up to different predicted probabilities.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Therefore, it&amp;rsquo;s intuitive to think about how a feature contributes to the predicted probability of a given test case:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:preformatted --&gt;

&lt;p&gt;&lt;pre class=&#34;wp-block-preformatted&#34;&gt;feature contribution of x&lt;sub&gt;k&lt;/sub&gt; = |β&lt;sub&gt;k&lt;/sub&gt;x&lt;sub&gt;k&lt;/sub&gt;| / (|β&lt;sub&gt;0|&lt;/sub&gt; + Σ|β&lt;sub&gt;i&lt;/sub&gt;x&lt;sub&gt;i&lt;/sub&gt;|) ∈ [0,1]&lt;/pre&gt;
&lt;!-- /wp:preformatted --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;With this, we can say the x&lt;sub&gt;k&lt;/sub&gt; contributes to the predicted probability of a given test case for some proportion or percentage, i.e. the feature contribution.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Generalizing to tree-based models&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;This way of evaluating individual feature contribution can be generalized to beyond linear models. For example, in a decision tree, a given test case would have a given prediction pathway that it takes down the decision tree - passing through to multiple junctions in the tree. The gain or loss in predicted probabilities or predicted values can be tracked accordingly, leading to a similar notion of feature contribution.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Moreover, this can further generalized to ensemble models, such as Random Forest, ExtraTrees and even XGBoost.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;I leave you with these two blog posts and the treeinterpreter package in Python as it has already been explored:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;http://blog.datadive.net/interpreting-random-forests/&#34;&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/&#34;&gt;https://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/andosa/treeinterpreter&#34;&gt;https://github.com/andosa/treeinterpreter&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;That&amp;rsquo;s all for this short post, hope it helps in bringing you to think slightly deeper about feature importance.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A guide to dollar cost averaging in Singapore</title>
      <link>/post/2019/01/13/a-guide-to-dollar-cost-averaging-in-singapore/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/01/13/a-guide-to-dollar-cost-averaging-in-singapore/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://www.sgmoneymatters.com/wp-content/uploads/2017/02/collect-passive-income.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Happy New Year from Korea everyone! I am currently on a business trip in Seoul, visiting our Korean counterparts for 2 weeks. Besides the weeknight / weekend day trips to popular spots like Myeongdong and Dongdaemun, I have also found some time to do a bit of writing.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;With the new year comes the new year resolutions of many Singaporeans. If you have been trying to get started on investing but not very exactly sure how to get started, then this post on dollar cost averaging and regular savings plans should be a good guide for you. Here goes.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;a href=&#34;https://www.investopedia.com/terms/d/dollarcostaveraging.asp&#34;&gt;Dollar cost averaging&lt;/a&gt;&amp;nbsp;(DCA) is a tool in the repertoire of the long term investor to minimize impact on portfolios due to market turbulence. DCA works because cost of investing reduces to some average amount that generally performs better than if a lump sum amount is straightaway placed into the market, especially if the market is volatile. There are many resources explaining the numbers behind DCA and its pros and cons, so I won&amp;rsquo;t talk about those in this post.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;What I would to present in this post, however, is a guide to getting started in practicing DCA in Singapore. DCA in Singapore is generally referenced using the term &amp;ldquo;regular savings plan&amp;rdquo; (RSP), and if you do a quick Google search on RSPs in Singapore, you should be able to see a couple of prominent articles:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;Moneysmart -&amp;nbsp;&lt;a href=&#34;https://blog.moneysmart.sg/this-vs-that/posb-ocbc-poems-maybank-kim-eng-which-regular-savings-plan-to-use/&#34;&gt;https://blog.moneysmart.sg/this-vs-that/posb-ocbc-poems-maybank-kim-eng-which-regular-savings-plan-to-use/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Seedly -&amp;nbsp;&lt;a href=&#34;https://blog.seedly.sg/which-regular-savings-plan-is-the-cheapest/&#34;&gt;https://blog.seedly.sg/which-regular-savings-plan-is-the-cheapest&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Dollarsandsense -&amp;nbsp;&lt;a href=&#34;https://dollarsandsense.sg/which-monthly-investment-plan-is-suitable-for-you/&#34;&gt;https://dollarsandsense.sg/which-monthly-investment-plan-is-suitable-for-you/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;And from there, you should be able to see that there are four main providers of RSPs in Singapore, namely:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;The&amp;nbsp;&lt;a href=&#34;https://www.posb.com.sg/personal/investments/investing-in-funds/invest-saver&#34;&gt;POSB Invest-Saver&lt;/a&gt;&amp;nbsp;(IS)&lt;/li&gt;&lt;li&gt;The&amp;nbsp;&lt;a href=&#34;https://www.ocbc.com/personal-banking/investments/bluechip.html&#34;&gt;OCBC Blue Chip Investment Plan&lt;/a&gt;&amp;nbsp;(BCIP)&lt;/li&gt;&lt;li&gt;The&amp;nbsp;&lt;a href=&#34;https://www.poems.com.sg/rsp/&#34;&gt;Phillip POEMS Shares Builder Plan&lt;/a&gt;&amp;nbsp;(SBP)&lt;/li&gt;&lt;li&gt;The&amp;nbsp;&lt;a href=&#34;https://www.maybank-ke.com.sg/latest-offerings/listing/sgd0-commission-with-monthly-investment-plan/&#34;&gt;Maybank Kim Eng Monthly Investment Plan&lt;/a&gt;&amp;nbsp;(MIP)&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Each has their pluses and minuses, so in this post, I will build a simple framework to draw the important comparisons between each of these RSPs. If you decide to join the club and be a DCA-er like me, then hopefully this post can help you make a more informed decision.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Comparison metrics&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Can&amp;rsquo;t build a framework for analysis without having our basis of comparison:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;Range of counters available&lt;/li&gt;&lt;li&gt;Transaction fees and minimum charges&lt;/li&gt;&lt;li&gt;Dividend reinvestment&lt;/li&gt;&lt;li&gt;Payment modes and order execution&lt;/li&gt;&lt;li&gt;Selling and other hidden fees&lt;/li&gt;&lt;li&gt;Other factors of consideration&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;1. Range of counters available&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Following illustrates the range of counters available in each RSP:&lt;br&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table {&#34;className&#34;:&#34;is-style-regular&#34;} --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table is-style-regular&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;SGX&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Other exchanges&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;ETFs&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Bond-based&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;&amp;gt;230&lt;/td&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;The rest (US, HK, MY, TH)&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;It is immediately clear that the MIP gives investors acccess to a significantly larger range of counters, including those in the US and Hong Kong. MIP does not provide access to any ETFs. The other three provides only counters in SGX, with the SBP being the most varied.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Arguably, if you are thinking of doing DCA, then it&amp;rsquo;s unlikely that you would want to dabble in any counters traded in Malaysia or Thailand - maybe Hong Kong as well.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In terms of ETF coverage, following illustrates:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Equity-based ETFs&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Bond-based ETFs&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;G3B.SI&lt;/td&gt;&lt;td&gt;A35.SI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;G3B.SI&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;ES3.SI, OVQ.SI, CLR.SI&lt;/td&gt;&lt;td&gt;A35.SI, MBH.SI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;ES3.SI&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;2. Transaction fees and minimum charges&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The next thing we have to consider is of course transaction fees. Pitched as a low barrier of entry product for investors to start investing, transaction fees in RSPs are typically lower than that of brokerage accounts.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table {&#34;className&#34;:&#34;is-style-regular&#34;} --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table is-style-regular&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Transaction fees&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;1% for G3B.SI, 0.5% for A35.SI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;0.3% or $5 per counter, whichever is higher&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;&amp;lt; $1,000 transactions and  ≤ 2 counters: $6&lt;br&gt;&amp;lt; $1,000 transactions and  ≥ 3 counters: $10&lt;br&gt;&amp;gt; $1,000 transactions: 0.2% or $10, whichever is higher&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;&amp;lt; $1,000 transactions: 1%&lt;br&gt;&amp;gt; $1,000 transactions: 0.18% for SGX, 0.2% otherwise&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;From this, it&amp;rsquo;s clear that the selection of RSP product is a function of your total monthly investment amount, ceteris paribus. &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;&amp;lt; $500&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;$500 - $1,000&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;&amp;gt; $1,000&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt; ≤ $5&lt;/td&gt;&lt;td&gt;$5 - $10&lt;/td&gt;&lt;td&gt;1% or 0.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;$5&lt;/td&gt;&lt;td&gt;$5&lt;/td&gt;&lt;td&gt; 0.3%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;$6&lt;/td&gt;&lt;td&gt;$6 - $10&lt;/td&gt;&lt;td&gt;0.2%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt; ≤ $5&lt;/td&gt;&lt;td&gt;$5 - $10&lt;/td&gt;&lt;td&gt;0.18% or 0.2%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Simply put, if you have $500 tops to invest per month, then IS or MIP are probably your best options, as there are no minimum charges. On the other hand, if you have more than $1,000 to invest per month, then SBP or MIP would be better options for you as you would have crossed the minimum charges threshold.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;That leaves the $500 - $1,000 range. If you are in this range, I would think it&amp;rsquo;s reasonable to assume that in the short-term you should be able to increase your investment to above $1,000 per month. Therefore I would recommend making your decision with that in mind.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;3. Dividend reinvestment&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Many investors are divided (pun intended) between dividend withdrawal and dividend reinvestment. Personally I prefer to have the free cash for me to divert to other locations, but that&amp;rsquo;s me. For RSPs, here&amp;rsquo;s the lowdown:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Dividend reinvestment&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;Both available&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;Dividend withdrawal&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;Both available&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;Dividend withdrawal &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;4. Payment Modes and Order Execution&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Another straightforward factor of consideration&amp;hellip;:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Payment mode&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Order execution&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;Debit from DBS/POSB a/c*&lt;/td&gt;&lt;td&gt;Debit on 15th, buy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;Debit from OCBC a/c&lt;/td&gt;&lt;td&gt;Debit on 22nd, buy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;GIRO into SBP cash ledger&lt;/td&gt;&lt;td&gt;Debit 6BD from 18th, buy on 18th&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;GIRO into KE prefunded a/c&lt;/td&gt;&lt;td&gt;Debit 1st BD, buy on 8th&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&amp;hellip; with some specific details on mechanisms:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;The SBP cash ledger is a special ledger created for the purposes of the SBP. It is not the Phillip Money Market Fund and pays no interest. Dividends, if any, are paid into the cash ledger and can be withdrawn accordingly. Don&amp;rsquo;t have to worry about this. Nonetheless, you will still need to create a POEMS account first. Both KC or prepaid can be used for the SBP.&lt;/li&gt;&lt;li&gt;The KE prefunded account is a pre-requisite to the MIP. You will have to create a prefunded account in order to use the MIP. Dividends, if any, are paid into the prefunded account. You can also use the prefunded account to execute ordinary trades.&lt;/li&gt;&lt;li&gt;EPS (Electronic Payment for Shares) and CDP (SGX Central Depository) facilities are not required for any of the RSPs, though they are always good to have.&lt;/li&gt;&lt;li&gt;On the respective order execution dates (15th, 22nd, 18th, 8th), if it falls on a non-business day the order will be executed on the next BD.&lt;/li&gt;&lt;li&gt;If you are sharp enough, you would realize that GIRO equates to bill payments and can be used to qualify for certain deposit account bonus interest components.&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;*New IS applications count towards the Invest component of the &lt;a rel=&#34;noreferrer noopener&#34; aria-label=&#34;DBS Multiplier account (opens in a new tab)&#34; href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34; target=&#34;_blank&#34;&gt;DBS Multiplier account&lt;/a&gt;, but only for the first 12 months of transactions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h3&gt;5. Selling and other hidden fees&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Selling shares is not typically explained in great detail, as most DCA-ers are looking for long-term, buy-and-hold type of investing system. That said, it is still important to understand the fees associated with selling your shares, as well as other hidden fees that are buried in the terms and conditions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;Selling shares&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Selling transaction fees&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;IS&lt;/td&gt;&lt;td&gt;&lt;a href=&#34;https://dollarsandsense.sg/posb-invest-saver-heres-automatically-invest-strongest-singapore-companies-bonds-every-month/&#34;&gt;None&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BCIP&lt;/td&gt;&lt;td&gt;&lt;a href=&#34;https://www.ocbc.com/personal-banking/investments/bluechip.html#fees&#34;&gt; 0.30% of the total sales proceeds or S$5 per counter, whichever is higher &lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SBP&lt;/td&gt;&lt;td&gt;Same as &lt;a href=&#34;https://www.poems.com.sg/products/stocks/&#34;&gt;POEMS&lt;/a&gt; itself&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MIP&lt;/td&gt;&lt;td&gt;Same as &lt;a href=&#34;https://www.maybank-ke.com.sg/pricing/pricing-listing/stocks/&#34;&gt;KE&lt;/a&gt; itself&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;Other hidden fees&lt;/strong&gt; - this is largely driven by my personal experience with these products! Which means there could be things that I have missed out e.g. BCIP&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list --&gt;

&lt;p&gt;&lt;ul&gt;&lt;li&gt;IS: None that I know of - IS is a really simple and efficient product to work with :)&lt;/li&gt;&lt;li&gt;BCIP: I don&amp;rsquo;t use BCIP, so I don&amp;rsquo;t have any hands-on experience with it&lt;/li&gt;&lt;li&gt;SBP: &lt;strong&gt;Dividend charges&lt;/strong&gt; - while it is listed &lt;a href=&#34;https://www.poems.com.sg/docs/SBP-Infosheet.pdf&#34;&gt;here &lt;/a&gt;that dividends are charged at 1% with a minimum of $1 capped at $50, what&amp;rsquo;s not mentioned is that this dividend charge is applied &lt;strong&gt;per dividend line item&lt;/strong&gt;. For example, just last month (March 2019), Mapletree Industrial Trust (ME8U) did their &lt;a href=&#34;http://www.mapletreeindustrialtrust.com/en/Investor-Relations/Distribution/Distribution-History.aspx&#34;&gt;3QFY18/19 and advanced 4QFY18/19 distribution, at 3.07 cents and 1.71 cents per unit&lt;/a&gt;, on pay dates of 8th March and 26th March respectively. However, due to either legal or tax reasons that I have yet to understand fully, typical REIT dividend distributions are split across multiple line items, such as that listed &lt;a href=&#34;https://www.dividends.sg/view/ME8U&#34;&gt;here&lt;/a&gt;. As such and rather incredulously, the SBP dividend charge is applied across each line item. What&amp;rsquo;s more, for every line item that do not meet the $1 minimum, the entire dividend line item is forfeited. Therefore, the best way to really mitigate this is to accumulate sufficient units and transfer them into your CDP ($20.40 per counter inclusive of GST) when the time comes. I did write to Phillip complaining about the application of the dividend charge in this manner, but I didn&amp;rsquo;t manage to effect any change. To be honest, in view of the cheap SBP transaction fees relative to actual monthly DCA execution via other means, I will just have to live with this for now.&lt;/li&gt;&lt;li&gt;MIP: based on my experience, the MIP in itself is rather transparent. The thing to note here however, is that the associated KE Prefunded account comes with its set of charges and fees, such as the &lt;a href=&#34;https://www.maybank-ke.com.sg/pricing/pricing-listing/stocks/&#34;&gt;foreign custody fee&lt;/a&gt; for non-CDP securities. In my view, these charges are fairly reasonable by Singapore market standards.&lt;/li&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading --&gt;

&lt;p&gt;&lt;h3&gt;6. Other factors of consideration&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;It must be mentioned that the factors that I am listing below must be weighed against all the prior pointers, especially transaction fees. For example, low transaction fees could jolly well compensate for bad user experience, depending of course on how low is low and how bad is bad. It&amp;rsquo;s all subjective anyway. Also, goes without saying that a good user experience is preferable to a bad one, ceteris paribus.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;POSB IS is definitely the easiest to work with and get started&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;IS is hands-down the simplest of the gang to get started with investing and DCA. First of all, it is probably accurate to say that every Singaporean son and daughter has a POSB account of some sorts, likely to be the Savings account. That&amp;rsquo;s already half the requirements for IS - the other half is to have iBanking. After which you can simply log on and kick start the RSP. No additional account, no GIRO arrangements, no additional platform or portal to log on. No fuse no nothing. And it&amp;rsquo;s really transparent in terms of  transaction costs. For these reasons, IS is my favorite for its pure and joyful simplicity. POSB Savings account + IS was also how I got started as well.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;By extension, even though I don&amp;rsquo;t have any hands-on experience with the BCIP, I can imagine it to be as simple as the IS within OCBC internet banking, relative to the SBP or MIP.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;The DBS/POSB iBanking user experience is not be taken granted for&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;If you are like me and never had prior experience in using any other internet banking platforms beyond DBS and OCBC, then you would really take their UI/UX (user interface/user experience) for granted. While the POEMS platform is what I would deemed as acceptable, the KE platform is rather horrendous. I am no UX expert but there are definitely plenty of areas of improvements for both POEMS and KE. You can stick a query into Google Images to sneak a peek at both. I could feel like I was using a Bloomberg terminal with way less platform capabilities.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;What IS lacks in ETF and general coverage, SBP and MIP more than makes up for it&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;That said, IS only provides two ETFs, the &lt;a href=&#34;https://www.nikkoam.com.sg/etf/sti&#34;&gt;G3B&lt;/a&gt; and &lt;a href=&#34;https://www.nikkoam.com.sg/etf/abf&#34;&gt;A35&lt;/a&gt;. Even for a &lt;a href=&#34;https://www.bogleheads.org/wiki/Lazy_portfolios&#34;&gt;lazy portfolio&lt;/a&gt; consideration, this is insufficient. A Singaporean lazy portfolio could look like something like &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/09/16/short-book-review-rich-by-retirement-by-joshua-giersch/&#34;&gt;IWDA.LN + ES3 + A35&lt;/a&gt;, or any other variation on global equity exposure, local equity exposure, local bond exposure (e.g. &lt;a href=&#34;https://www.nikkoam.com.sg/etf/sgd-investment-grade-corp-bond&#34;&gt;MBH&lt;/a&gt;), with or without EM exposure (e.g. &lt;a href=&#34;https://www.bloomberg.com/quote/EIMI:LN&#34;&gt;EIMI.LN&lt;/a&gt;), small cap exposure or alternatives exposure (e.g. REITs).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;With this, IS is in itself definitely insufficient even for the laziest of portfolios. In terms of ETF coverage, both SBP and MIP is much broader, allowing DCA access into &lt;a href=&#34;https://www.spdrs.com.sg/etf/fund/spdr-straits-times-index-etf-ES3.html&#34;&gt;ES3&lt;/a&gt;, A35, MBH, &lt;a href=&#34;http://www.lionglobalinvestors.com/en/funds/lion-phillip-s-reit-etf/index.html&#34;&gt;CLR&lt;/a&gt;, as well as &lt;a href=&#34;http://www.phillipfunds.com/home/sing-income-etfs&#34;&gt;OVQ&lt;/a&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;(In particular, OVQ is one that I particular watched and took a slightly &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/10/20/analysis-of-the-phillip-sing-income-etf/&#34;&gt;deeper look)&lt;/a&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Therefore, it seems ideal to complement IS with either of SBP and MIP (MIP especially for DCA access to non-SGX securities), or simply skip IS all together.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;IS can be used to fulfill DBS Multiplier Investment component, while SBP/MIP GIRO deductions can be used to fulfill GIRO bill payment requirements for OCBC 360 etc.&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;This one is trivial. New IS applications count towards the Invest component of the &lt;a rel=&#34;noreferrer noopener&#34; href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34; target=&#34;_blank&#34;&gt;DBS Multiplier account&lt;/a&gt;, but only for the first 12 months of transactions. (If you already had IS running off another DBS/POSB account, simply switching the funding account to the DBS Multipler account counts as a &amp;ldquo;new IS application&amp;rdquo;.) On the other hand, GIRO deductions used to fund the SBP or MIP can count as bill payments for accounts like OCBC 360 and UOB One.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;Finally, my personal implementation of DCA includes the IS, SBP, and MIP.&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;I decided early on that the BCIP seems expensive relative to the others. Having started out with IS, I now use IS for G3B and A35, SBP for OVQ, MBH, and a variety of REITs and stocks, and MIP for &lt;a href=&#34;https://sg.finance.yahoo.com/quote/JNJ?p=JNJ&#34;&gt;JNJ.N&lt;/a&gt;. That said, what that works for me may not work for you. Do your own research!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;That&amp;rsquo;s all for this post, hope you gained a little something. Thanks for reading :) &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some data scientist interview questions - with a twist</title>
      <link>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/28/some-data-scientist-interview-questions-with-a-twist/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/12/machine_learning.png&#34; width=&#34;50%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Machine Learning. Nothing to do with my intended topic, just a random xkcd comic that I thought is funny. &lt;a href=&#34;https://xkcd.com/1838/&#34;&gt;Source&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Been wanting to do this consolidation for some time, so here goes. I won&amp;rsquo;t be touching on any language-specific questions (e.g. packages or functions), as I don&amp;rsquo;t believe they are relevant in this Google/Stack Overflow era - kind of missing the forest for the trees. Also, won&amp;rsquo;t be going over basic questions like &amp;ldquo;What is logistic regression&amp;rdquo; or &amp;ldquo;How does collaborative filtering works&amp;rdquo; or the like.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Rather, I want to be more focused on ML concepts that are closely tied to &lt;strong&gt;the consumption of ML models by business&lt;/strong&gt;. Once ML matures in the market, the next more well-sought after set of skills should be closely related to &lt;strong&gt;translation of business requirements&lt;/strong&gt;, &lt;strong&gt;model deep-diving&lt;/strong&gt;, &lt;strong&gt;ML pipeline management (roadmapping)&lt;/strong&gt;, and&amp;nbsp;&lt;strong&gt;curation of models&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Questions will be categorized according to the following:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Business understanding&lt;/strong&gt; - developing business understanding in a short amount of time is a key skillset for a data scientist, in order to build superb ML models. In addition, managing your customers in the context between your ML practice and their business expertise is key to project success&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Statistics&lt;/strong&gt; - contrary to plenty of data scientists out there, I continue to believe statistics is a pre-requisite core skillset to being a good data scientist (hence even the name of my blog)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model Building&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model Selection&lt;/strong&gt; - it&amp;rsquo;s easy to select models based on metrics like accuracy or ROC-AUC. What happens if there are additional concerns or complications from the business?&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model Maintenance&lt;/strong&gt; - a data scientist&amp;rsquo;s job doesn&amp;rsquo;t stop at building the models, it should also include keeping our models healthy for business consumption&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Business Understanding&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;1. As data scientists, a big part of our job is to understand the business in which our usecases originate from, and glean subtle ways in which our analysis and modelling could be influenced or impacted. This typically starts from multiple requirements gathering meetings we have with our customers. Given a predictive usecase, what would be some of the questions that you ask your customers in order to arrive at the relevant information?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Identify drivers, (strength of) business assumptions, get clarity of data availability of drivers, span of historical data available, exceptions, macro-environment changes and their impact, availability of external data&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;2. In the initial stages of modelling/exploratory data analysis, how would you validate your new found understanding the business and validate them with your customers?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Construct preliminary models, use customers&amp;rsquo; input directly as feature selection means&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;3. Suppose your analysis and/or models are giving plenty of output that deviates substantially from your customers&amp;rsquo; expectations - for example, unintuitive feature importance from a high quality model, or unexpected predicted values. How would you develop a compromise between good ML practice and their business knowledge and negotiate your way through the project?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;4. What is customer success and why is it important in data science projects?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In many data science units, including both inward- and outward-facing units, customer success (CS) is often neglected as a form of &amp;ldquo;post-sales&amp;rdquo; activity. CS refers to the continued and sustained effort to ensure that the deliverables, regardless of models or tools etc., become an integral part of the customer&amp;rsquo;s processes or workflows.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;CS is about maximizing the adoption and usage of the deliverables, to ensure that the customer is successful in operating the tools that have been delivered to them.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;While it seems reasonable to scope a project in terms of producing the deliverables subsequently ending the engagement thereafter, this is an unhealthy way of running any projects, including data science. CS is important because it is an indication of the return of investment for a project - for a outward-facing data science unit, CS and post-sales activities maximizes customer experience and quality of account, leading to sales/presales pipeline development. This is relevant for both software as well as professional services vendors. For a inward-facing unit, user adoption and value delivered can be directly measured to assess true value throughput of the data science unit in serving the company.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;As it turns out, CS is a function of multiple factors, including customer experience, user experience, and change management. As data scientists, there are a couple of things which we can do to directly contribute to CS:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;During project scoping and requirements gathering, &lt;strong&gt;ensure that the project is well-scoped in time and space&lt;/strong&gt;, in terms of e.g. unit of analysis, target variable, number of models, modelling criteria and success criteria. Strive to &lt;strong&gt;deliver clear understanding on data science terminologies&lt;/strong&gt; to business stakeholders and users, minimize ambiguity and &lt;strong&gt;align expectations&lt;/strong&gt;. For example, ML projects are typically pitched to create value in accuracy and/or automation. Ensure this expectation on accuracy and/or automation is aligned.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Reduce the use of ad-hoc data&lt;/strong&gt; (e.g. standalone, manually curated/maintained spreadsheets) where possible. If these datapoints turn out to be valuable to the models, create scripts or workflows to ensure data refreshes can be done as hassle-free as possible.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Build confidence with customers&lt;/strong&gt; by performing e.g. 1 to 2 rounds of exploratory data analysis (EDA). Illustrating and validating business assumptions, as well as retrieving any data artifacts or surprising insights, such as trends or correlations, improves customer experience and the subsequent modelling process. Present these pre-modelling results to customers for validation and communication.&lt;/li&gt;&lt;li&gt;Ensure the &lt;strong&gt;modelling process is clearly illustrated&lt;/strong&gt; in an &amp;ldquo;explain-like-I&amp;rsquo;m-5&amp;rdquo; manner. Let customers understand why certain features are dropped, or why a certain feature is engineered in a particular manner. Customers should be able to understand feature importance in a model. Most importantly, &lt;strong&gt;customers should not feel that models are black-box&lt;/strong&gt; as this increases the fear of the unknown and reduces model adoption.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Document all work products&lt;/strong&gt; throughout the project, from business assumptions, data preparation, modelling, to model deployment. This ensure reproducibility.&lt;/li&gt;&lt;li&gt;Finally, &lt;strong&gt;develop a reasonable model monitoring and maintenance process&lt;/strong&gt;. Both data refreshes as well as model refreshes should be not too frequent, manual or time-consuming. A reasonable maintenance cadence maximizes model adoption and customer success.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Statistics&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;1. How do you detect statistical interactions in a dataset, and how would that affect your modelling?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Effect modification. Classical way is to use multiplicative terms in modelling, though not always scalable - O(n^2)-type complexity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;2. How do you detect confounding in a dataset, and how would that affect your modelling? How does confounding differ from multicollinearity?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Tackle from epidemiology standpoint&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;3. Would it be apt to use p-values, either from univariate- or multivariate-type analysis, as indications of feature importance or as a means of feature selection? If yes, how would you use it? If no, what are your considerations?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;No. Definition of p-values and null hypothesis differs subtly from feature importance, but with substantial impact in interpretation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;4. What is bias, and how would bias affect your analysis, modelling and interpretations of data?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Selection bias, information bias&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;5. What is the bias-variance tradeoff, and how would the tradeoff affect your modelling?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Model Building&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;1. Why is there a general tradeoff between model performance and interpretability? Suppose you constructed a high performing model that is essentially a black box e.g. a deep learning model. How would you present the model to your customers in an more interpretable manner?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Complexity of business environment, opaque drivers with unknown interactions. Build simple regression or decision tree models with important features from black box as proxy for interpretation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;2. Given a reasonably clean dataset in a predictive usecase, what are the tasks standing between the dataset and a good model? Which are the tasks that would, in principle, take the longest time to perform?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Feature transformation, feature engineering, hyperparameter tuning, model selection. Feature engineering should be the challenging and take the longest. A good feature set can easily beat a well-tune model because the former is closer to the true underlying business context than the latter.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;3. Having a dataset with temporal element means that it lends itself to both typical supervised learning models as well as time series models. Again, in a predictive usecase, how would you decide which is a better way to reach quality predictions?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;4. Would your general modelling methodology differ substantially or at all if your customers are seeking explanations instead of predictions?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Broadly speaking, suppose we consider the divide between tree-based methods against linear regression methods. In the construction of a decision tree, the training dataset is divided into multiple discrete p-dimensional subspaces, where p is the number of features. To transcend from one subspace to its adjacent neighbor, one would have to make multiple unit increments in one direction towards a given subspace boundary, until the boundary is transcended. This means that throughout the unit increments, the predicted value of a test case remains the same, until the boundary is transcended. Contrast this to a linear regression-type method, where we can see the impact on predicted value for every unit increment of a given feature with its given coefficient. Intuitively, this could better aid in explanations and understanding, and can be used to perform sensitivity or what-if analysis for deeper insights.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In addition, we could consider fit-emphasizing metrics such as R&lt;sup&gt;2&lt;/sup&gt; instead of prediction-emphasizing metrics such as accuracy for model evaluation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;5. In a multiclass classification problem, it is typically challenging to develop a high performing model. How would you tackle a multiclass classification problem?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Collapse class, multi-tier modelling, though errors in classification will cascade. If class is ordinal, measure error wrt distance to the correct class rather than in absolute terms.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Model Selection&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;1. Given two black box models, how would you choose one over the other?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Generally, we can consider the following: i) model performances on the testing&amp;nbsp; sets, ii) model performances on fitted training sets, iii) feature importance, and iv) model simplicity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;i) Model performances on the testing set is obviously important as it directly points to the model&amp;rsquo;s ability to generalize to unseen cases.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;ii) Model performances on the fitted training sets can given an indication on the fundamental quality of the model itself. For example, if training performance outweigh testing performance by a large margin, then overfitting could be a concern. On its own, a low training performance indicates underfitting, while extremely high training performance could indicate target leakage.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;iii) Feature importance illustrates the relative weightages of underlying features in the model and how they contribute to reaching a prediction or outcome. In a scenario where a strong model outperforms a weaker model but with a somewhat bizarre and unintuitive set of feature importance, it becomes a business decision to make a prudent model selection. This is especially important in industries e.g. banking, where decisions to reject a loan may need to be entirely transparent.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;iv) Finally, Occam&amp;rsquo;s razor would be a good heuristic to apply - for a given model performance, the simpler a model is, the better it is for human and business consumption.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;2. In usecases that typically exhibit class imbalance and yet not the extent where anomaly detection algorithms are appropriate, how would we ensure that the models selected are adequate for business consumption?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In classification problems with class imbalance, one of the first things to experiment is upsampling the minority class or downsampling the majority class. Typically, the former is preferred as then we don&amp;rsquo;t lose training samples. Subsequently, we can then follow up with the modelling, and more importantly, the selection of a metric robust to class imbalance, such as ROC-AUC or sensitivity or specificity per se. This is trivial.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;More importantly and very often neglected is a deliberate consideration of the risk or cost of a wrong prediction, especially in a class imbalance setting. For instance, suppose we are tackling an employee attrition usecase, where we are predicting if an employee is about to leave the company within the next 3 months, as an example. This is of course a typical class imbalance problem - most reasonably large companies have about 10 to 20% attrition rate as a healthy number.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;However, suppose the usecase is specific to high potential individuals within the employee population - employees who are earmarked by HR and management as e.g. successors of critical roles in the company (hence high potential). In this context, a false negative, i.e. wrongly predicting no attrition, becomes a costly mistake. In contrast, a false positive is a false alarm and is much less costly.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;How do we capture this asymmetry in cost/risk in our model evaluation process? A solution would be develop a utility function &lt;em&gt;U(m)&lt;/em&gt;, and assign relative utility to our prediction outcomes, as below:&lt;br&gt;
&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;&lt;figure class=&#34;wp-block-table&#34;&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Prediction outcome&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Utility&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;True positive&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;True negative&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;False positive&lt;/td&gt;&lt;td&gt;-5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;False negative&lt;/td&gt;&lt;td&gt;-50&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;!-- /wp:table --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Then, the utility of a model &lt;em&gt;m&lt;/em&gt; would be
&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:preformatted --&gt;

&lt;p&gt;&lt;pre class=&#34;wp-block-preformatted&#34;&gt;U(m) = TP(n&lt;sub&gt;1&lt;/sub&gt;) + TN(n&lt;sub&gt;2&lt;/sub&gt;) - 5FP(n&lt;sub&gt;3&lt;/sub&gt;) - 50FN(n&lt;sub&gt;4&lt;/sub&gt;)&lt;/pre&gt;
&lt;!-- /wp:preformatted --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;where &lt;em&gt;n&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; are the respective case counts of each prediction outcome. Model tuning and validation can then be done by maximizing utility.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;3. In most projects, multiple models are selected to be production-grade and their respective output are combined. What are the different ways we can combine multiple models?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;There are various ways in which models can be combined, such as boosting, ensemble learning and stacking. I would like to focus here on a less well-known approach known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Analytic_hierarchy_process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analytical hierarchy process&lt;/a&gt;&amp;nbsp;(AHP).&amp;nbsp;In essence, AHP is utility-driven way of combining different types of models to reach a single conclusion. The best way to illustrate AHP is the use of an example.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Consider a HR analytics problem statement: how can the impact of employee attrition to the company be minimized?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Typically, in HR analytics, we can say that primary outcomes that are impactful and can evaluated are performance, potential, and attrition. We can therefore formulate three usecases to culminate into our problem statement:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;Predict the short-term performance of a given employee&lt;/li&gt;&lt;li&gt;Predict the potential/runway of a given employee&lt;/li&gt;&lt;li&gt;Predict the attrition risk of a given employee&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;This necessarily means that we would at least have three different models, with three different target variables, to construct. How would we then combine these models to address the problem statement of minimizing employee attrition? This is done using AHP to build a utility function, by assign utilities or weights to the predicted probabilities of each model. Without going through the details of AHP, following are examples of simple utility functions:
&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:preformatted --&gt;

&lt;p&gt;&lt;pre class=&#34;wp-block-preformatted&#34;&gt;impact of attrition =
attrition risk * short-term performance + 5 * attrition risk * potential&lt;/p&gt;

&lt;p&gt;impact of attrition =
attrition risk * employee rank * (short-term performance + potential)&lt;/p&gt;

&lt;p&gt;impact of attrition =
attrition risk * (40 - tenure) * potential&lt;/pre&gt;
&lt;!-- /wp:preformatted --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Note that we can include additional layers of modelling in the AHP to capture specific intricacies and requirements - for example, the utility function need not be a linear combination of weights, but a decision tree or a matrix.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In general, AHP is an extremely powerful method to combine multiple models to address a large and encompassing problem statement.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:heading {&#34;level&#34;:3} --&gt;

&lt;p&gt;&lt;h3&gt;Model Maintenance&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;1. What are the points of consideration in deciding when and how to update or refresh a model?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;In general, the performance of a model erodes over time, with changes in business environment. Model refresh is therefore an important consideration even before commencement of any data science project.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;There are a number of factors when considering when and how to refresh a model:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:list {&#34;ordered&#34;:true} --&gt;

&lt;p&gt;&lt;ol&gt;&lt;li&gt;Availability of data - when can fresh datapoints be captured and sufficiently verified? For example, fresh datapoints could be captured on a weekly or monthly basis. This implies that models can be refreshed at most at a weekly or monthly basis. On the other hand, with monthly captures but only quarterly validation means that models can be refreshed at most at a quarterly basis.&lt;/li&gt;&lt;li&gt;Changes in business environment - has there been a major change in the environment? For example, new policies announced or implemented by authorities, entry of new competitors, new product launches, major news or events could all justify model refreshes. Of course, this is also dependent on data availability.&lt;/li&gt;&lt;li&gt;Refresh/no refresh cost-benefit analysis - for example, a model refresh could require collection of updated datapoints with some man effort in the executing the refresh, which leads some dollar value cost. On the other hand, the benefit of the refresh is deemed to be of low consequence and unlikely to steer decision making or operations in significant manner, which leads to some dollar value benefit.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- /wp:list --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;&lt;strong&gt;2. What are the ways you can refresh a model (from ML perspective, not engineering perspective)?&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;The simplest way to refresh a model is the simply rerun the training algorithm on the updated data with a fresh round of hyperparameter tuning. The next order of complexity would be to re-execute the training process with the same feature set (with updated data), but re-experimenting with the various training algorithms. Layering on that could be additional feature engineering for potentially better model performances.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;p&gt;Finally, the most labour-intensive form of refresh would be to rework the entire model from scratch, starting again with requirements gathering and assumptions validation, with possibly new feature sets.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why ensemble modelling works so well - and one often neglected principle</title>
      <link>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/25/why-ensemble-modelling-works-so-well-and-one-often-neglected-principle/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/12/r-user-group-singapore-data-mining-with-r-workshop-ii-random-forests-12-638.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Putting models together in an ensemble learning fashion is a popular technique amongst data scientists
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Ensemble learning is the simultaneous use of multiple predictive models to arrive at a single prediction, based on a collective decision made together by all models in the ensemble. It&#39;s a common and popular technique used in predictive modelling, especially when individual models are failing to produce the required performance levels, in terms of e.g. accuracy.&lt;/p&gt;
&lt;p&gt;Ensemble learning is often introduced towards the end of any Data Science 101-type content, and often emphasized in terms of implementation rather than the underlying reason behind its success. It&#39;s also a question I get asked often.&lt;/p&gt;
&lt;p&gt;In this post I will conduct a simple statistical treatment to illustrate why ensemble learning works, and &lt;strong&gt;one important catch that most data scientists neglect&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Binary classifiers as biased coin flips&lt;/h3&gt;
&lt;p&gt;Consider a binary classifier c&lt;sub&gt;1&lt;/sub&gt; on a yes/no classification problem. Being a reasonably constructed classifier, c&lt;sub&gt;1&lt;/sub&gt; has an accuracy of 60%. This means that the probability of c&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;giving the correct prediction is 0.60, like a flip of a biased coin.&lt;/p&gt;
&lt;p&gt;Now consider putting three such classifiers together in a democratic fashion. This means that the set of classifiers (the ensemble) would give the correct prediction if and only if two of the three (doesn&#39;t matter which two) or more give the correct prediction.&lt;/p&gt;
&lt;p&gt;If you do your combinations calculations correctly, you would be able to arrive at the overall performance of the ensemble being 0.648:&lt;/p&gt;
&lt;pre&gt;Pr({correct, correct, wrong})   = (0.6²)(0.4)&amp;nbsp;= 0.144
Pr({correct, correct, correct}) = 0.6³ = 0.216

Accuracy of ensemble = 0.216 + (0.144)(3) = 0.648 &amp;gt; 0.6
(&lt;em&gt;3 times because there are 3 different ways of getting 2 correct, 1 wrong.&lt;/em&gt;)
&lt;/pre&gt;
&lt;p&gt;Generally, we can say that as the number of models within the ensemble increase, so does the accuracy of the ensemble. &lt;strong&gt;However, this result is valid only if the individual classifiers are independent amongst each other&lt;/strong&gt; - something which most data scientists fail to understand or appreciate. Consider this next piece of math.&lt;/p&gt;
&lt;p&gt;(In reality, true independence is hard to either attain or assess, so we settle with low or zero correlation.)&lt;/p&gt;
&lt;h3&gt;Binary classifiers as Bernoulli trials&lt;/h3&gt;
&lt;p&gt;Every time we ask our binary classifier c&lt;sub&gt;1&lt;/sub&gt; for a prediction, we are essentially conducting a Bernoulli trial with:&lt;/p&gt;
&lt;pre&gt;E(c&lt;sub&gt;1&lt;/sub&gt;) = p
Var(c&lt;sub&gt;1&lt;/sub&gt;) = p(1-p)&lt;/pre&gt;
&lt;p&gt;Putting together our ensemble of 3 independent classifiers again:&lt;/p&gt;
&lt;pre&gt;(&lt;em&gt;ens.&lt;/em&gt;) = ⅓(c&lt;sub&gt;1&lt;/sub&gt; + c&lt;sub&gt;2&lt;/sub&gt; + c&lt;sub&gt;3&lt;/sub&gt;)
E(&lt;em&gt;ens.&lt;/em&gt;) = p (&lt;em&gt;unbiased&lt;/em&gt;)
Var(&lt;em&gt;ens.&lt;/em&gt;) = ⅓p(1-p) &amp;lt; p(1-p) = Var(c&lt;sub&gt;1&lt;/sub&gt;)&lt;/pre&gt;
&lt;p&gt;With this, it&#39;s clear why the independence or negligible correlation condition is necessary - otherwise:&lt;/p&gt;
&lt;pre&gt;Var(&lt;em&gt;ens.&lt;/em&gt;) = ⅓p(1-p) + Cov(c&lt;sub&gt;1&lt;/sub&gt;,c&lt;sub&gt;2&lt;/sub&gt;) + Cov(c&lt;sub&gt;1&lt;/sub&gt;,c&lt;sub&gt;3&lt;/sub&gt;) + Cov(c&lt;sub&gt;2&lt;/sub&gt;,c&lt;sub&gt;3&lt;/sub&gt;)
(&lt;em&gt;all pairwise covariances&lt;/em&gt;)&lt;/pre&gt;
&lt;p&gt;With the additional pairwise covariance terms, it is &lt;strong&gt;no longer guaranteed&lt;/strong&gt; that&lt;/p&gt;
&lt;pre&gt;Var(&lt;em&gt;ens.&lt;/em&gt;) &amp;lt; Var(c&lt;sub&gt;1&lt;/sub&gt;)&lt;/pre&gt;
&lt;p&gt;Without going through the math again, this set of results can be applied to regression problems with no loss of generality.&lt;/p&gt;
&lt;h3&gt;What does this mean and what I can do with this&lt;/h3&gt;
&lt;p&gt;Clearly, we need our ensemble to be reliable and not wobble all over the place with high prediction variance. It&#39;s&amp;nbsp;intuitive why the negligible correlation condition makes sense - correlated models would more often than not support each other and make the same yes/no predictions simultaneously, even if the given test case could jolly well be in the grey zone.&lt;/p&gt;
&lt;p&gt;In addition, it should be clear to you now that there&#39;s not much use in assembling strong learners together in an ensemble - they are likely to be accurate per se, and thereby correlated with each other with the test cases. All you are doing is to increase the variance of your predictions. On the other hand, putting a bunch of weak learners would make sense because they are likely to be less correlated amongst each other.&lt;/p&gt;
&lt;p&gt;Finally, the next time when someone presents an ensemble learning approach, ask if they ever consider the correlations amongst the underlying models. Odds are that they would take you a blank look and not sure why that&#39;s necessary :P&lt;/p&gt;
&lt;p&gt;(If you are interested to learn more about ensemble learning and how it works in algorithms like random forests, feel free to take a look at this &lt;a href=&#34;https://github.com/thestatsguy/RUGS-RF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt; on my Github.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Good news for fans of Singapore Savings Bonds</title>
      <link>/post/2018/12/21/short-good-news-for-fans-of-singapore-savings-bonds/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/12/21/short-good-news-for-fans-of-singapore-savings-bonds/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src= &#34;https://thestatsguyhome.files.wordpress.com/2018/12/SSB.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
The Singapore Savings Bonds programme
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Been a while since I last posted on my blog - the last month or so has been hectic for me, with the semester final examinations for my part-time Masters as well as travelling for business. Still, no excuses for slipping on my 2 posts per month commitment, so will try to make up for it in December.&lt;/p&gt;
&lt;p&gt;Also, quick update: following my analysis of the various deposit accounts &lt;a href=&#34;https://thestatsguy.rbind.io/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;, I went ahead and opened my DBS Multiplier account online, together with the DBS Live Fresh credit card. Now all I need to do is to redirect both my salary crediting and the CDP Direct Crediting Service to the Multiplier account, and I am all set to enjoy my interest rates :)&lt;/p&gt;
&lt;p&gt;Back to my intended topic - good news for folks who have particularly enjoyed the SSB programme - &lt;a href=&#34;http://www.sgs.gov.sg/~/media/SGS/SGS%20Announcements%20pdf/SSB%20PDF/MAS%20Media%20Release%20on%20SSB%20Applications%20via%20SRS%20Funds.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;MAS has announced three big moves for SSB on 17th Dec 2018&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;With effect from 1st February 2019, SSBs can be purchased using SRS funds, via the respective SRS operators (DBS/POSB, OCBC, UOB).&lt;/li&gt;
&lt;li&gt;With effect from 1st February 2019, the Individual Limit for SSBs will be increased from SGD100,000 to SGD200,000.&lt;/li&gt;
&lt;li&gt;In March 2019, MAS will be launching a &#34;My Savings Portal&#34; to allow investors to review their SSB portfolios, for both cash and SRS purchases.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;SRS + SSB = Awesome Combo&lt;/h3&gt;
&lt;p&gt;These are really terrific news for anyone who utilizes SSBs in their portfolios. Firstly, SRS funds can finally be parked in an essentially risk-free instrument - a first. I will definitely be using this new feature of the SRS account to get &lt;em&gt;both&lt;/em&gt; my tax breaks and risk-free rate (which was basically 0.05% p.a. in the SRS account).&lt;/p&gt;
&lt;p&gt;A quick reminder - before you do any top-ups to your SRS accounts, please do fully understand the SRS scheme and evaluate how the SRS account can play a part in your portfolio or finances. Here&#39;s a good&amp;nbsp;&lt;a href=&#34;https://dollarsandsense.sg/supplementary-retirement-scheme-4-things-need-understand-opening-srs-account/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;primer&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Individual Limit of SGD200,000&lt;/h3&gt;
&lt;p&gt;The increase in Individual Limit is great as well. Since SSBs can be considered as free cash flow (pro-rated interest payouts + redemption latency of 1 month), SGD100,000 is not difficult for working professionals with decent savings rates to hit. The increase in Individual Limit is certainly more than welcomed.&lt;/p&gt;
&lt;h3&gt;My Savings Portal&lt;/h3&gt;
&lt;p&gt;Lastly, a SSB portal to review holdings - yea, why not? Currently, investors have to log in into CDP accounts to review our SSB holdings, which is good enough an user experience. Hopefully this new portal encompasses all relevant information regarding SSBs, including purchase calendars, redemption calendars, payout calendars, historical rates, bond ladders, planning tools... At least that&#39;s how I would like the UX for such a portal to be - probably wishful thinking on my part :P&lt;/p&gt;
&lt;p&gt;Looking forward to the implementation of these changes in February 2019. For more information on SSBs, the &lt;a href=&#34;http://www.sgs.gov.sg/savingsbonds.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;SSB website&lt;/a&gt; would be in itself a great resource.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of the Phillip SING Income ETF</title>
      <link>/post/2018/10/20/analysis-of-the-phillip-sing-income-etf/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/20/analysis-of-the-phillip-sing-income-etf/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/47348-etf-poems-landing-page-2.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Earlier this month (Oct 2018), Phillip Securities introduced a brand new ETF - the Phillip SING Income ETF, to be managed by Phillip Capital Management, Based on the name of the ETF alone, it&#39;s easy to guess the focus of this ETF: dividend income, which is an investment style popular amongst retail investors in Singapore.&lt;/p&gt;
&lt;p&gt;Here&#39;s how the ETF is described on &lt;a href=&#34;http://poems.com.sg/&#34;&gt;poems.com.sg&lt;/a&gt;:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;h3&gt;&lt;span style=&#34;color: #0000ff;&#34;&gt;Harvest Your Regular Income with Phillip SING Income ETF&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The Phillip SING Income ETF focuses on 30 high quality Singapore Listed stocks to offer investors a cost-effective and diversified exposure to the Singapore market. By using a rule/factor-based approach for stocks selection, the ETF aims to deliver stable and quality income for investors.&lt;/p&gt;
&lt;div class=&#34;m-top&#34;&gt;
&lt;p&gt;The Phillip SING Income ETF provides investors with:&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Easy access to 30 high quality Singapore Listed Stocks&lt;/li&gt;
&lt;li&gt;Liquidity, transparency and diversification across the Singapore market&lt;/li&gt;
&lt;li&gt;Stable and quality income with semi-annual dividend distribution&lt;/li&gt;
&lt;li&gt;Strategic Beta for stocks selection with emphasis on
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Business Quality&lt;/li&gt;
&lt;li&gt;Financial Health&lt;/li&gt;
&lt;li&gt;Dividend Yield&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div&gt;
&lt;p&gt;Here&#39;s a compilation of articles written by various sources on this ETF:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://fifthperson.com/phillip-sing-income-etf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Fifth Person&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fool.sg/2018/10/02/the-phillip-sing-income-etf-a-new-option-for-income-hungry-singapore-investors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Motley Fool Singapore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dollarsandsense.sg/an-alternative-to-the-sti-etf-introducing-the-phillip-sing-income-etf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dollars And Sense&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bedokianportfolio.blogspot.com/2018/09/the-phillip-sing-income-etf.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bedokian Portfolio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.smallcapasia.com/5-things-to-know-about-phillip-sing-income-etf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Small Cap Asia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
&lt;p&gt;In this post, I will be doing my analysis on this ETF and evaluate the potential role it can play in my portfolio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;&lt;b&gt;Comparison to other Singapore exposure-only ETFs on SGX&lt;/b&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;One of the first questions to ask when evaluating new ETFs is how this new ETF compares with existing ETFs on the exchange. For context, there are plenty of &lt;a href=&#34;http://www.sgx.com/wps/wcm/connect/eccd3a44-73bf-4f03-a665-db9abb5432d5/ETF_Summary_Info_27+Aug+2018.pdf?MOD=AJPERES&amp;amp;CONVERT_TO=url&amp;amp;CACHEID=eccd3a44-73bf-4f03-a665-db9abb5432d5&#34;&gt;ETFs listed on the SGX&lt;/a&gt;, and there are 7 of them that are specific to gaining Singapore exposure:&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;ol&gt;
&lt;li&gt;O9A.SI - Xtrackers MSCI Singapore UCITS ETF&lt;/li&gt;
&lt;li&gt;CLR.SI - Lion Phillip S-REIT ETF&lt;/li&gt;
&lt;li&gt;G3B.SI - Nikko AM Singapore STI ETF&lt;/li&gt;
&lt;li&gt;ES3.SI - SPDR Straits Times Index ETF&lt;/li&gt;
&lt;li&gt;A35.SI - ABF Singapore Bond Index ETF&lt;/li&gt;
&lt;li&gt;MBH.SI - Nikko AM SGD Investment Grade Corporate Bond ETF&lt;/li&gt;
&lt;li&gt;KV4.SI - Xtrackers II Singapore Government Bond UCITS ETF&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;
&lt;p&gt;Of these 7,&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;ul&gt;
&lt;li&gt;3 of them (A35, MBH, KV4) are fixed income ETFs&lt;/li&gt;
&lt;li&gt;1 (CLR) is a REIT ETF&lt;/li&gt;
&lt;li&gt;2 of them (G3B, ES3) are essentially the same (both tracks the STI).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence those valid for comparison are:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;ol&gt;
&lt;li&gt;O9A.SI - Xtrackers MSCI Singapore UCITS ETF&lt;/li&gt;
&lt;li&gt;Either of the STI ETFs&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;
&lt;p&gt;&lt;b&gt;Underlying indices and their constituents&lt;/b&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;The STI is, well, the flagship index for the SGX, and tracks the top 30 stocks (by market cap) listed on the SGX. O9A, on the other hand, tracks the MSCI Singapore Index, and the Phillip SING Income ETF will be tracking the Morningstar Singapore Yield Focus Index.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;The Morningstar Singapore Yield Focus Index&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;A brief note on the historical performance of the Morningstar Singapore Yield Focus Index (MSYF) - the index has delivered an &lt;a href=&#34;https://www.fool.sg/2018/10/02/the-phillip-sing-income-etf-a-new-option-for-income-hungry-singapore-investors/&#34;&gt;annualized return of 9.6%&lt;/a&gt; since 2005, which can be broken down into capital gains of 5.2% and dividend returns of 4.4%. Bear in mind that the this period also includes the 2007-2008 financial crisis.&lt;/p&gt;
&lt;p&gt;The key feature of the MSYF is the avoidance of dividend traps - stocks that generate high yield and yet have weak fundamentals. This is done by the &lt;a href=&#34;https://www.poems.com.sg/wp-content/uploads/2018/09/Sing-Inc-Index-Information.pdf&#34;&gt;index construction process of the MSYF&lt;/a&gt;. I won&#39;t be covering the math part of it here, because even just by eyeballing the index, we can tell what it can do for us.&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;&lt;u&gt;Smart Beta ETF&lt;/u&gt;&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;One thing to point out here is that the Phillip SING Income ETF is a classic example of a Smart Beta ETF. Unlike a typical market cap-weighted index ETF, a Smart Beta ETF usually relies on rules, such as the MSYF index construction rules, to lay out parameters to build the ETF, so as to beat the beta. Once these rules are constructed, the ETF is autonomously functional - hence Smart Beta ETFs are typically regarded as semi-passive ETFs.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Top 10 holdings&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;Following is how the indices differ in top 10 holdings.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/e07ea-capture.png&#34; width=&#34;100%&#34;&gt;
Top 10 holdings of respective indices
&lt;/center&gt;&lt;/p&gt;

&lt;p class=&#34;separator&#34;&gt;The first thing you should notice is that the top 4 holdings of the three indices are the same, albeit in different order. DBS, OCBC, UOB, and Singtel all feature in the respective top 4. The STI is well-known to be overweighted in our local banks (40.96%), which is similar for the MSCI Singapore Index (47.43%). Whether it&#39;s good for an index to be overweighted in banks is another topic all together - however what&#39;s refreshing here is that the &lt;b&gt;banks take up only 23.9% of the MSYF&lt;/b&gt;.&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;This is the first key characteristic for lay investors like us to consider: the &lt;b&gt;MSYF has a hard cap of 10% per stock&lt;/b&gt;. To me, this is biggest upside of this ETF - an ETF on SGX that&#39;s not overweighted in banks.&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;&lt;u&gt;Full holdings and sector distribution&lt;/u&gt;&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;Following are the full holdings of each index.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/156f5-full2bholdings.png&#34; width=&#34;100%&#34;&gt;
Full holdings of respective indices
&lt;/center&gt;&lt;/p&gt;

&lt;p class=&#34;separator&#34;&gt;And here are the sector distributions.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/78bbc-sector2bdistri.png&#34; width=&#34;100%&#34;&gt;
Sector distributions of respective indices
&lt;/center&gt;&lt;/p&gt;

&lt;p class=&#34;separator&#34;&gt;Again, just by eyeballing, what catches my lay investor&#39;s attention immediately is the &lt;b&gt;30.1% representation of real estate&lt;/b&gt; (mainly REITs) in the ETF. Personally, I like S-REITs as dividend generating vehicles, but neither STI nor the S-REIT ETF can offer a diversified and cheap solution to gain REITs exposure in Singapore (the S-REIT ETF is pretty expensive with a 0.6% expense ratio).&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;&lt;b&gt;Dividend yield and expense ratio&lt;/b&gt;&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;Of course, two important things that must be considered for an ETF are the dividend yield and expense ratio.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/b3761-dividend2byield2band2bexpense2bratio.png&#34; width=&#34;100%&#34;&gt;
Dividend yields and expense ratios of respective indices
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;p class=&#34;separator&#34;&gt;For the Phillip SING Income ETF, both 5% yield and 0.4% management fees are indicative numbers given by the fund managers as the most probable. If both numbers come true, this would be another plus point for this ETF - competitive fees with superior yield. The 5% indicative yield comes from the fact that there is a combined 60% of banks and real estate in the ETF.&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;&lt;b&gt;What to think about when the ETF starts trading&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Tax transparency&lt;/u&gt;: something that I didn&amp;rsquo;t thought about previously, so quoting the &lt;a href=&#34;https://fifthperson.com/phillip-sing-income-etf/&#34;&gt;article from The Fifth Person&lt;/a&gt; here - &amp;ldquo;Distributions from S-REITs are tax-free for investors. From 1 July 2018, this tax transparency treatment was also extended to ETFs invested in S-REITs. Previously, distributions from S-REITs to ETFs were taxed at 17%, which made investing in S-REIT ETFs tax inefficient. However, this treatment only applies to ETFs that invest entirely in REITs (like the Lion-Phillip S-REIT ETF). The Phillip SING Income doesn’t qualify for tax transparency as it invests in other securities besides REITs. Phillip Capital Management plans to speak to the authorities about extending the tax transparency treatment to any ETF that invests in REITs, but for now the dividend yield for the Phillip SING Income ETF is effectively 4.8% after factoring the 17% withholding tax on S-REIT distributions.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Tracking error:&lt;/u&gt; Tracking error matters for an ETF: the closer the ETF is to its target index, the better. Since the ETF has not began trading, it remains to be seen whether tracking error will be acceptable.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Inclusion into Regular Savings Plan&lt;/u&gt;: since this ETF is managed by Phillip, you can be sure that it will be included into the &lt;a href=&#34;https://www.poems.com.sg/rsp/&#34;&gt;POEMS SBP&lt;/a&gt;, potentially even the &lt;a href=&#34;https://www.posb.com.sg/personal/investments/investing-in-funds/invest-saver&#34;&gt;POSB Invest-Saver&lt;/a&gt;. Something for the DCA-ers like myself to think about.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&#34;separator&#34;&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;My take is that the Phillip SING Income ETF is a superior alternative to the STI ETF, given the&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-overweightage of banks&lt;/li&gt;
&lt;li&gt;Hard cap of 10% per stocks&lt;/li&gt;
&lt;li&gt;30% representation in REITs&lt;/li&gt;
&lt;li&gt;Reasonable fees of 0.4%&lt;/li&gt;
&lt;li&gt;And that sweet, sweet 5% yield &lt;i&gt;if it comes true&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
&lt;p&gt;Therefore, I have participated in the subscription of this ETF, putting in a few grand, and potentially including it into my DCA if it turns out well.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;The Phillip SING Income ETF will go live on the SGX on 29th Oct 2018.&lt;/p&gt;
&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OCBC 360 Interest structure change: what&#39;s the impact on you and me?</title>
      <link>/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/20/ocbc-360-interest-structure-change-whats-the-impact-on-you-and-me/</guid>
      <description>&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/76eb9-changes2beffective2b1nov.jpg&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
What&amp;rsquo;s changed for the OCBC 360 account bonus interest structure
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Effective from 1st Nov 2018, there will be changes to the interest structure of the&amp;nbsp;&lt;a href=&#34;https://www.ocbc.com/personal-banking/accounts/360-account.html&#34;&gt;OCBC 360 account&lt;/a&gt;. If you are either exploring deposit accounts out there or currently using OCBC 360 (like myself), you should pay attention to these changes.&lt;/p&gt;
&lt;p&gt;Following is the existing interest structure:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/0d873-ocbc2b3602bcurrent.jpg&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Existing interest structure
&lt;/center&gt;&lt;/p&gt;

&lt;p class=&#34;separator&#34;&gt;It&#39;s a relatively straightforward structure that has requirements on salary crediting (which earns you 1.2% p.a.), at least 3 bill payments (0.3%), at spending $500 spending (0.3%), and investing on selected products (either 0.6% or 1.2%).&lt;/p&gt;
&lt;p class=&#34;separator&#34;&gt;Moving forward, this is the updated structure:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/10/fef2c-ocbc2b3602bupdated.jpg&#34; width=&#34;75%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
Updated interest structure (effective 1st Nov 2018)
&lt;/center&gt;&lt;/p&gt;

&lt;p class=&#34;separator&#34;&gt;Major differences between the updated and old structure are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;u&gt;Introduction of 2-tiering&lt;/u&gt;: first 35K and next 35K&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Step-up bonus&lt;/u&gt;: as long as you can increase your account balance by at least $500 relative to previous month, you can earn 0.3% or 0.6% depending on tier. This step-up bonus was offered as an ad-hoc promotion sometime earlier this year (Aug and Sep 2018), but will become a permanent feature in the 360 account.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Grow bonus&lt;/u&gt;: for balances above $200K, the first 70K gets another 1%.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Boost bonus&lt;/u&gt;: positive delta between current and previous month-end balance gets another 1%, up to 1mil. Read carefully - it&#39;s only the difference between the month-end balances that gets the 1%, not the month-end balance itself.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Payment bonus&lt;/u&gt;: no longer offered. So there&#39;s no bonus/requirement on bill payments anymore.&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;
&lt;p&gt;&lt;b&gt;What&#39;s the impact on me?&lt;/b&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I have been using the 360 account since I graduated from university in 2015. Back then I didn&#39;t really put too much thought into it, as I was damn broke anyway. I was already using the FRANK account and OCBC internet banking, so it became natural for me to move into the 360 account and get the accompanying 365 credit card.&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;&lt;u&gt;Earning that 1.8% per annum&lt;/u&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I never had any issues hitting the interest requirements, other than the Invest bonus. The products that can allow me to get this bonus are products that I have no interest in, such as ILPs, structured deposits, and unit trusts, and in minimum amounts of 20K. Makes no sense to me at all. Therefore, for the longest time, I have been getting 1.2% + 0.3% + 0.3%&amp;nbsp; = 1.8% of interest - which is decent.&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;&lt;u&gt;Then and now&lt;/u&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Fast forward 3 years and my income and net worth has grown by, well, a rather pleasant amount. This also made the 360 account less and less viable for me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I was already drawn to the fact that my month-end balances are becoming flat-lined or dipping, simply because my monthly dollar cost averaging investments are getting higher and higher each month. With this new OCBC 360 interest structure, I will definitely&amp;nbsp;not be able to consistently earn the Step-up bonus.&lt;/li&gt;
&lt;li&gt;The 2-tier structure means that I need to put more than 35K in the account in order to sit in the next tier, as compared to a 1-tier structure with no such requirement. Personally, the 360 account serves also as the location of my emergency funds, and I have no intention to keep that much emergency funds. 70K is too much to be able to utilize this new interest structure to the fullest.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ocbc.com/personal-banking/investments/bluechip.html&#34;&gt;OCBC BCIP&lt;/a&gt;&amp;nbsp;is still not included in the Invest bonus component. On the other hand, the&amp;nbsp;&lt;a href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34;&gt;DBS Multiplier account&lt;/a&gt;&amp;nbsp;considers the&amp;nbsp;&lt;a href=&#34;https://www.posb.com.sg/personal/investments/investing-in-funds/invest-saver&#34;&gt;POSB Invest-Saver&lt;/a&gt;&amp;nbsp;as a qualifying investment. Seems like DBS is more progressive?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In any case, this has triggered my search for the next deposit account that can meet my needs. In the following sections, I will put together some numbers and make a decision from there. If you are using OCBC 360 account as well, I strongly encourage you to review whether this account is optimal for your needs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shortlisted accounts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After some quick research (OK, this one &lt;a href=&#34;https://blog.moneysmart.sg/savings-accounts/best-savings-accounts-singapore/&#34;&gt;article&lt;/a&gt; summarized it all up), I will be focusing specifically on the following:&lt;/p&gt;
&lt;ul style=&#34;font-weight: 400;&#34;&gt;
&lt;li&gt;OCBC 360 account&lt;/li&gt;
&lt;li&gt;DBS Multiplier account&lt;/li&gt;
&lt;li&gt;UOB One account&lt;/li&gt;
&lt;li&gt;Bank of China SmartSaver account&lt;/li&gt;
&lt;li&gt;Maybank SaveUp account&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, a special mention of the &lt;a href=&#34;https://www.hsbc.com.sg/advance/&#34;&gt;HSBC Advance programme&lt;/a&gt;. I won&#39;t be elaborating on it here, but do check it out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison metrics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What would be our basis of comparison?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Interest structure&lt;/strong&gt;, and how much interest I will be getting &lt;strong&gt;realistically &lt;/strong&gt;- it may not be possible to fulfill all components in the interest structure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balance cap on the bonus interest&lt;/strong&gt; - how much of my balance qualify for the bonus interest?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Banking convenience&lt;/strong&gt; - since I will be using this account for salary crediting, emergency funds, and everyday expenses, it&#39;s important that I can bank with this account conveniently. This means plenty of ATMs and a decent online banking system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Here we go!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;OCBC 360 account&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s start with 360 first. Based on the new structure, I doubt I can consistently get the Step-up bonus, and there&#39;s no way I will purchase any insurance or investment products from OCBC - so here&#39;s how much I think I can get. &#34;Realistic interest rates&#34; are in (brackets).&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&#34;125&#34;&gt; &lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;Salary&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;Spend&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;Step-up&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;Invest&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;First 35K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;1.2% (1.2%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;0.3% (0.3%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;0.3% (0%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;0.6% (0%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;125&#34;&gt;&lt;strong&gt;Next 35K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;1.5% (1.5%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;0.6% (0.6%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;0.6% (0%)&lt;/td&gt;
&lt;td width=&#34;125&#34;&gt;1.2% (0%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Realistic average interest rate for me: 1.8% (from Salary and Spend)&lt;/li&gt;
&lt;li&gt;Balance cap on bonus interest: 70K&lt;/li&gt;
&lt;li&gt;Banking convenience: High (OCBC shares ATMs with UOB and others, and I am already a satisfied user of their online banking system.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;DBS Multiplier account&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34;&gt;DBS Multiplier account&lt;/a&gt; is by far one of the most lenient accounts when it comes to qualifying for bonus interest. There are five qualifying components, namely salary, credit card spend, home loan installments, insurance, and investments. The salary component is compulsory, and for every thing else, it&#39;s based on total flow of funds across 2 or 3 components. What do I mean by that?&lt;/p&gt;
&lt;p&gt;For example, if I have a salary credit of $3,000 per month, with just $1 spend on credit card and $1 on investments (I will explain this later), then it would be total flow of $3,002 across 3 components. This means I can qualify for 2.00% p.a. No minimum amounts on specific components, just considering total flow of funds. It&#39;s that simple.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://thestatsguyhome.files.wordpress.com/2018/11/51b97-capture.png&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;
&lt;center&gt;
DBS Multiplier Account Interest Structure
&lt;/center&gt;

&lt;p&gt;With that, it&#39;s easy to immediately see how much bonus interest I would be getting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Realistic average interest for me: 2.20%&lt;/li&gt;
&lt;li&gt;Balance cap on bonus interest: 50K&lt;/li&gt;
&lt;li&gt;Banking convenience: Medium-High (POSB/DBS ATMs have the longest queues.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the investment component, the easiest way to qualify is via either using &lt;a href=&#34;https://www.posb.com.sg/personal/investments/investing-in-funds/invest-saver&#34;&gt;POSB Invest-Saver&lt;/a&gt; (only for the first 12 months - read the fineprint under the investment component &lt;a href=&#34;https://www.dbs.com.sg/personal/landing/dbs-multiplier/&#34;&gt;here&lt;/a&gt;) or using &lt;a href=&#34;http://www.sgs.gov.sg/savingsbonds.aspx&#34;&gt;Singapore Savings Bonds&lt;/a&gt;. I won&#39;t elaborate further on this here, but be sure to check out this &lt;a href=&#34;https://kpo-and-czm.blogspot.com/2018/04/dbs-multiplier-ssbs-joint-account-higher-interest.html&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;UOB One account&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.uob.com.sg/personal/save/chequeing/one-account.page&#34;&gt;UOB One account&lt;/a&gt; is pretty straightforward in their interest structure. There are only two qualifying components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Min. card spend of $500 in the month&lt;/li&gt;
&lt;li&gt;Min. card spend of $500 with either salary crediting or 3 GIRO deductions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So for most folks, hitting both should be relatively easy.&lt;/p&gt;
&lt;table style=&#34;height: 247px;&#34; width=&#34;375&#34;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt; &lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;&lt;strong&gt;2 components&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt;&lt;strong&gt;First 15K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;1.85%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt;&lt;strong&gt;Next 15K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;2.00%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt;&lt;strong&gt;Next 15K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;2.15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt;&lt;strong&gt;Next 15K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;2.30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;76&#34;&gt;&lt;strong&gt;Next 15K&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;302&#34;&gt;3.88%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul style=&#34;font-weight: 400;&#34;&gt;
&lt;li&gt;Realistic average interest for me: 2.075% (Probably going to keep up to 60K of cash at any point in time. It&#39;s already a rather significant amount to keep in cash.)&lt;/li&gt;
&lt;li&gt;Balance cap on bonus interest: 75K&lt;/li&gt;
&lt;li&gt;Banking convenience: High&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;BOC SmartSaver account&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.bankofchina.com/sg/pbservice/pb1/201611/t20161130_8271280.html&#34;&gt;BOC SmartSaver account&lt;/a&gt; is probably more suited for relatively higher income earners and spenders, based on their minimum requirements of $1,500 monthly spending (1.60%) and $6,000 income (1.20%). Anything short and you would be losing out by using this account.&lt;/p&gt;
&lt;ul style=&#34;font-weight: 400;&#34;&gt;
&lt;li&gt;Realistic average interest for me: 1.55%&lt;/li&gt;
&lt;li&gt;Balance cap on bonus interest: 60K&lt;/li&gt;
&lt;li&gt;Banking convenience: Low (in the next 3 seconds, think of a location where you can recall seeing a BOC ATM.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;Maybank SaveUp account&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;Finally, the &lt;a href=&#34;http://info.maybank2u.com.sg/saveup/&#34;&gt;Maybank SaveUp account&lt;/a&gt; is Maybank&#39;s attempt at competing in deposit accounts. There are 9 (!) bonus components to choose from which you would need to qualify for 3. As long as you can qualify for 3, the bonus interest would be straight out 3.0625%. Qualifying for 2 gives less than 1.20%, so the jump is tremendous.&lt;/p&gt;
&lt;p&gt;When I was first doing my homework on all these deposit accounts, Maybank looked like the obvious choice to me - I didn&#39;t think I would have any problems hitting the 3.06%. This was until I realized that minimum monthly credit card spend of $500 - which incredulously, is a challenge for me. Yes, the amount of money I spend every month is very low and a significant chunk of my spending cannot come from credit cards - unless kopitiams and hawker centers are ready to accept them.&lt;/p&gt;
&lt;p&gt;Therefore, unless credit card spending on insurance or income tax counts, it is unlikely that I can get the 3.06%. Which turns out to be the case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Realistic average interest for me: 1.11%&lt;/li&gt;
&lt;li&gt;Balance cap on bonus interest: 60K&lt;/li&gt;
&lt;li&gt;Banking convenience: Medium&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;font-weight: 400;&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;&lt;strong&gt;Account&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;&lt;strong&gt;Realistic average interest for me&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;&lt;strong&gt;Balance cap&lt;/strong&gt;&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;&lt;strong&gt;Banking convenience&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;OCBC 360&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;1.80%&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;70K&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;DBS Multiplier&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;2.20%&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;50K&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;Medium-High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;UOB One&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;2.08%&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;75K&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;BOC SmartSaver&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;1.55%&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;60K&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;Low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&#34;156&#34;&gt;Maybank SaveUp&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;1.11%&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;60K&lt;/td&gt;
&lt;td width=&#34;156&#34;&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p style=&#34;font-weight: 400;&#34;&gt;Looks like the answer is clear - keep 50K of cash in DBS Multiplier account.&lt;/p&gt;
&lt;p style=&#34;font-weight: 400;&#34;&gt;&lt;strong&gt;Assumptions  / Points to note&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;font-weight: 400;&#34;&gt;
&lt;li&gt;This turns out that a large part of this analysis hinges on an assumption of monthly credit card spending of less than $500 - which is true &lt;em&gt;most of the time&lt;/em&gt; for me. There are definitely months where I am able to exceed $500 of credit card spending, but it&#39;s pretty inconsistent. Mostly dependent on whether there&#39;s a special occasion during the month.&lt;/li&gt;
&lt;li&gt;The above assumption will no longer be valid if it&#39;s possible to rack up credit card spend with insurance premiums, income tax payments, or other consistent forms of payments.&lt;/li&gt;
&lt;li&gt;Another assumption I made was that I would be unable to consistently increase my month end balances. This assumption is valid simply because of the presence of SSBs.&lt;/li&gt;
&lt;li&gt;OCBC may one fine day emulate DBS and include the &lt;a href=&#34;https://www.ocbc.com/personal-banking/investments/bluechip.html&#34;&gt;BCIP&lt;/a&gt; into the 360 investment component. Then the 360 account would work out for me.&lt;/li&gt;
&lt;li&gt;Finally, do your own homework, and especially consider whether my assumptions make sense in your situation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That&#39;s all for this post!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Book Review: Rich by Retirement by Joshua Giersch</title>
      <link>/post/2018/09/16/short-book-review-rich-by-retirement-by-joshua-giersch/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/09/16/short-book-review-rich-by-retirement-by-joshua-giersch/</guid>
      <description>

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51kcmMI3s7L._SX331_BO1,204,203,200_.jpg&#34; width=&#34;50%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Joshua Giersch, better known as “Shiny Things”, is an investment guru who has been generously giving investing advice on &lt;a href=&#34;https://forums.hardwarezone.com.sg/money-mind-210/%2Aofficial%2A-shiny-things-club-part-2-a-5813566.html&#34;&gt;HardwareZone&lt;/a&gt; and his &lt;a href=&#34;https://moneygowherefind.com/joshua-giersch-aka-shiny-things-hwz/&#34;&gt;blog&lt;/a&gt;. His advice are consistently dished out with the SG context in mind, and his book &lt;a href=&#34;https://www.amazon.com/Rich-Retirement-Singaporeans-Invest-Wealthy-ebook/dp/B01JXW17ZM&#34;&gt;&amp;ldquo;Rich by Retirement: How Singaporeans Can Invest Smart and Retire Wealthy&amp;rdquo;&lt;/a&gt;, is, in essence, the consolidation of all of his tips and hacks on being a successful investor in SG.&lt;/p&gt;

&lt;p&gt;The most interesting component of his philosophy is his very simple portfolio strategy. Whether it is too simple for your liking is for you to decide, and in this post I will summarize, from my perspective, the key points in his book. Enjoy.&lt;/p&gt;

&lt;h1 id=&#34;emergency-fund&#34;&gt;Emergency fund&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Keep an emergency fund with 6 months worth of expense&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;insurance&#34;&gt;Insurance&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Get hospitalization plan (paid in part using Medisave), and term life if you have dependents&lt;/li&gt;
&lt;li&gt;All other forms of insurance are neither necessary nor worthwhile&lt;/li&gt;
&lt;li&gt;Especially avoid ILPs and endowments&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;brokerage-accounts&#34;&gt;Brokerage accounts&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Only pay &amp;lt;= 0.2% for trading SG stocks, &amp;lt;= 0.3% for US/UK stocks; UTs/ETFs with &amp;lt;= 0.5% expense ratio are ok&lt;/li&gt;
&lt;li&gt;Anything beyond these percentages are not acceptable&lt;/li&gt;
&lt;li&gt;Ideally regulated in SG or US&lt;/li&gt;
&lt;li&gt;No custody or dividend handling fees&lt;/li&gt;
&lt;li&gt;Recommended: SCB, Interactive Brokers&lt;/li&gt;
&lt;li&gt;Exception to the &amp;lt;= 0.2% recommendation: POSB Invest-Saver, because of the lack of min amount per txn. Good for small investors who are just starting out&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;asset-class-allocation&#34;&gt;Asset class allocation&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Stocks: 110 – current age; bonds: the rest&lt;/li&gt;
&lt;li&gt;For stocks, 50:50 on SG and int’l&lt;/li&gt;
&lt;li&gt;For bonds, only SG&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;exchange-traded-funds&#34;&gt;Exchange traded funds&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;ETFs are great because of the instant diversification. The ideal ETF would be one with &amp;lt;= 0.3% expense ratio, &amp;gt; 100mil in assets, and is a physical ETF&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;the-shiny-things-3-etf-portfolio&#34;&gt;The Shiny Things 3-ETF portfolio&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;IWDA.LN (iShares Core MSCI World UCITS ETF – for Int’l exposure)&lt;/li&gt;
&lt;li&gt;ES3.SI (SPDR Straits Times Index ETF – for SG exposure)&lt;/li&gt;
&lt;li&gt;A35.SI (ABF Singapore Bond Index Fund – for SG bonds)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;cpf-srs&#34;&gt;CPF/SRS&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Keep CPF in cash, don’t buy any counters&lt;/li&gt;
&lt;li&gt;Consider using SRS top-ups for tax breaks; can use SRS to buy ES3 (use OCBC BCIP – though A35 is not available)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;dollar-cost-averaging&#34;&gt;Dollar cost averaging&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Don’t time the market – “time in market” &amp;gt; “time the market”&lt;/li&gt;
&lt;li&gt;POSB Invest-Saver is great for DCA&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;portfolio-rebalancing&#34;&gt;Portfolio rebalancing&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Only rebalance in May and Nov every year&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simple enough? Pick up his book from Amazon or NLB (I had to make a reservation – $1.55 reservation fee) and you will be able to understand his justifications for each of these pointers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Quick start to using Git and GitHub</title>
      <link>/post/2017/04/12/short-quick-start-to-using-git-and-github/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/04/12/short-quick-start-to-using-git-and-github/</guid>
      <description>

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;https://miro.medium.com/max/4000/1*8HHpgXJkc6jQSiNT42EiBg.png&#34; width=&#34;100%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Git is pretty awesome. My most common &amp;ldquo;use case&amp;rdquo; for Git is working as a solo developer, working on different machines (home and work machines) and using GitHub to keep my work in place.&lt;/p&gt;

&lt;p&gt;Following is the most straightforward way to start use Git and Github in this no-brainer manner. During development, only run steps indicated by * (7 - code, 9 - stage, 10 - commit, 12 - push).&lt;/p&gt;

&lt;h3 id=&#34;initial-set-up&#34;&gt;Initial set-up&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-Installing-Git&#34;&gt;Install Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/signing-up-for-a-new-github-account&#34;&gt;Create GitHub account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-new-repository&#34;&gt;Create GitHub repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;1-set-up-credentials-user-email&#34;&gt;1. Set up credentials, user.email:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git config --global user.email &amp;quot;&amp;lt;your github email&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1a-set-up-credentials-user-email-specific-to-a-git-repo-only&#34;&gt;1a. Set up credentials, user.email, specific to a git repo only:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git config user.email &amp;quot;&amp;lt;your github email&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-set-up-credentials-user-name&#34;&gt;2. Set up credentials, user.name:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git config --global user.name &amp;quot;&amp;lt;your github username&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2a-set-up-credentials-user-name-specific-to-a-git-repo-only&#34;&gt;2a. Set up credentials, user.name, specific to a git repo only:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git config user.name &amp;quot;&amp;lt;your github username&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2b-show-all-config&#34;&gt;2b. Show all config:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git config --list
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-create-the-remote-repo-online-on-github&#34;&gt;3. Create the remote repo online on Github&lt;/h3&gt;

&lt;h3 id=&#34;4-create-a-folder-with-the-same-name-as-the-remote-name-locally&#34;&gt;4. Create a folder with the same name as the remote name locally&lt;/h3&gt;

&lt;h3 id=&#34;5-initialize-an-empty-repo-navigate-to-the-folder-in-command-line-then-run&#34;&gt;5. Initialize an empty repo: navigate to the folder in command line, then run:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git init
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-pull-the-remote&#34;&gt;6. Pull the remote:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git pull &amp;lt;url of remote (.git)&amp;gt; &amp;lt;branch name, e.g. master&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;7-go-ahead-and-code&#34;&gt;*7. Go ahead and code&lt;/h3&gt;

&lt;h3 id=&#34;8-add-remote-repo&#34;&gt;8. Add remote repo:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git remote add &amp;lt;name of repo&amp;gt; &amp;lt;url of repo (.git)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;9-stage-all-changes&#34;&gt;*9. Stage all changes:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git add --all
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;10-commit-all-staged-changes&#34;&gt;*10. Commit all staged changes:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git commit -m &amp;quot;&amp;lt;commit message&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;11-set-remote-as-upstream&#34;&gt;11. Set remote as upstream:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git push --set-upstream &amp;lt;repo name&amp;gt; &amp;lt;branch name, e.g. master&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;12-push-to-remote&#34;&gt;*12. Push to remote:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git push &amp;lt;repo name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>[short] Travelling and productivity</title>
      <link>/post/2017/03/05/travelling-and-productivity/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/03/05/travelling-and-productivity/</guid>
      <description>&lt;p&gt;Two things I want to talk about here: how travelling affects productivity and how to alleviate some of its effects.&lt;/p&gt;

&lt;p&gt;Since October 2016, I have been travelling in out of the Philippines for a data science project. Between October 2016 to February 2017, I made 11 trips to the Philippines, each trip spanning on average about 1 work week.&lt;/p&gt;

&lt;p&gt;At first it seems fun; which young professional wouldn’t want to experience a work life of constant flying, staying in 4 or 5 stars hotels while working on interesting projects? I would be lying if I said these didn’t excite me at the start.&lt;/p&gt;

&lt;p&gt;But, there is one major downside to all the travelling - the loss of routine and schedule. If you are someone like me who values routine for maximal productivity, then you may not like travelling as much. I recall my most productive times when schooling and working is when I follow the same schedule, week in, week out. With travelling, you are almost guaranteed to lose most, if not all of this scheduling.&lt;/p&gt;

&lt;p&gt;Of course on the plus side, you get uninterrupted time on the flight, lest for mealtime and the “please put your seat back up upright sir” from the air stewardess. I usually take this time to read or do some writing or thinking. It does helps, actually, that you have no internet connection.&lt;/p&gt;

&lt;p&gt;So, to help mitigate the lowered productivity due to travelling, these are some of the things I do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Have a business trip packing list. This reduces my mental burden when packing. By referring to the same list whenever I pack, over time I know this list is reliable and has everything I need - which means I can pack mindlessly from then on.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ensure that I have an aisle seat. As a tall person, I need to have an aisle seat, otherwise it will be an extremely extremely miserable flight. No way I can do any work if I’m not even comfortable. To ensure that I have an aisle seat, I constantly check seats availability online. You can do so for Singapore Airlines even before check-in is open.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check-in for my flight. This is so that when I get to the airport, I can go straight to the &amp;ldquo;internet check-in” queue which is a lot faster. This is especially important for MNL-SIN flights because there aren’t any self check-in machines at the Manila airport, and the SIA queues are usually long.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go to the airport early. Yes early, not &amp;ldquo;just nice”. This is best for productivity because I can work in both Changi Airport and Ninoy Aquino Airport. More importantly, reaching early means I can break my working time up in larger chunks. Compare this to reaching the airport just 1 or 2 hours prior - you get inside the transit area, grab a bite and try to settle down to work. By the time you unpack your laptop and notebook and truly settle down, its probably time to go to the gate.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bring a book and a notebook in my carry-on. Needless to say why.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get to the hotel as quickly as possible after touch down and check-in. If I’m taking the 7pm SIN-MNL flight, I will go to bed immediately so that I can still get up early the following day. If I’m taking an earlier flight, then I can take my time, check-in and spend the rest of the day working on personal stuff. I usually don’t head to the office on the same day of my flight unless there’s an important meeting.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My learnings on Apache Spark</title>
      <link>/post/2017/02/14/my-learnings-on-apache-spark/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/14/my-learnings-on-apache-spark/</guid>
      <description>

&lt;h2 id=&#34;one-simple-way-to-optimise-spark-jobs-on-yarn&#34;&gt;One simple way to optimise Spark jobs on YARN&lt;/h2&gt;

&lt;p&gt;When submitting Spark jobs to YARN on the CLI, we would use a submission script that typically looks like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spark-submit \
--master yarn-cluster \
--driver-memory 20G \
--driver-cores 10 \
--executor-cores 10 \
--executor-memory 20G \
--num-executors 10 \
--total-executor-cores 100\
script_to_submit.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are options that allows the user to specify the amount of resources to allocate to the submitted job. Not every option is always available - it depends on the type of cluster manager. There are currently three types available to Spark: standalone, Mesos, and YARN.&lt;/p&gt;

&lt;p&gt;Simply put, the standalone cluster manager comes with the Spark distribution, while Mesos and YARN are clusters managers designed to be compatible to Spark, with YARN coming together with Hadoop distributions.&lt;/p&gt;

&lt;p&gt;In brief, the available options for each cluster manager are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Standalone - driver-cores, executor-cores, total-executor-cores&lt;/li&gt;
&lt;li&gt;Mesos - total-executor-cores&lt;/li&gt;
&lt;li&gt;YARN - driver-cores, executor-cores, num-executors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest, namely driver-memory and executor-memory are available to all three.&lt;/p&gt;

&lt;p&gt;I haven&amp;rsquo;t had any experience with the standalone manager as well as Mesos, so I will just talk about YARN. On the YARN web UI, under &amp;ldquo;Cluster Metrics&amp;rdquo;, there are two entries that read &amp;ldquo;Memory Total&amp;rdquo; and &amp;ldquo;VCores Total&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;To optimise the amount of resources allocated to your job:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Memory Total&amp;rdquo; should be roughly and less than num-executors x executormemory&lt;/li&gt;
&lt;li&gt;&amp;ldquo;VCores Total&amp;rdquo; should be roughly and less than num-executors x executor-cores&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seems intuitive but I didn&amp;rsquo;t fully put this optimisation in my conscious mind until one of our engineers explicitly enlighten me of this.&lt;/p&gt;

&lt;h2 id=&#34;other-learnings-on-spark&#34;&gt;Other learnings on Spark&lt;/h2&gt;

&lt;p&gt;This is PySpark.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Writing text files to HDFS using sc.saveAsTextFile() - use high driver memory. RDD has to fit in the driver memory when writing.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use rdd.coalesce(n) to save to n text files. On the YARN UI, each file will be represented as a task.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If the saveAsTextFile() stage keeps stopping at the last task, check the data. There is most probably something wrong with the data in the program.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;There is a difference between using 50GB RAM times 10 executors versus 20GB times 30 executors. The memory used reflected on the YARN UI differs greatly - for my case, the former gives 550GB while the latter, 220GB. I&amp;rsquo;m guessing it&amp;rsquo;s best to match the number of executors to the number of datanodes in the cluster.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Whenever a task or a stage cannot succeed, check the data within the program - columns, counts, datatypes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A conventional way to debug code or scripts is always to print statements or data onto the console or terminal. Note that this debugging technique cannot work for some spark Spark apps, because of Spark&amp;rsquo;s lazy evaluation. Methods in Spark can be classified as either actions or transformations. Unlike actions, transformation methods are parsed and interpreted by Spark, without any actual work done on the data structures; only when actions are called will work be done. Therefore interjecting your code with print statements doesn&amp;rsquo;t help too much.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;UDFs can run without checking for errors in the data within the program. Suspect that UDFs are transformations and not actions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In YARN, container RAM is the RAM of 1 datanode. When setting the RAM for each container, leave about 5GB for overheads and OS functions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;df.printSchema() can work even without reading any data into the program - even lazier than transformations if I&amp;rsquo;m not wrong.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Actions / transformations of one RDD cannot be performed inside the actions / transformations of another RDD, as all actions and transformations of the former RDD will require the spawning of new workers and jobs, within the current workers and jobs on the latter RDD, which is not supported.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The UDF function turns a regular Python function to a function that is applied on all elements of the input column. This function cannot any Spark functions, as calling any Spark functions may require the needs to spawn new workers and jobs. (10) is a generalisation of this.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;book-getting-started-with-apache-spark-from-inception-to-production&#34;&gt;Book: Getting Started with Apache Spark: From Inception to Production&lt;/h2&gt;

&lt;p&gt;This book, published by MapR, serves as an introduction to Apache Spark. It&amp;rsquo;s a free book I got from the Strata Hadoop 2016 conference in Singapore. A relatively short and lightweight intro to Spark, this is a good read for anyone who wants to learn a little more about Spark. Topics include installation, architecture overview, Hadoop and Spark, data streaming, and machine learning using MLlib.&lt;/p&gt;

&lt;p&gt;Pdf version available here: &lt;a href=&#34;http://www.bigdatatoronto.com/2016/assets/getting_started_with_apache_spark.pdf&#34;&gt;http://www.bigdatatoronto.com/2016/assets/getting_started_with_apache_spark.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Scientific desktop research: tentative conclusions</title>
      <link>/post/2017/02/13/scientific-desktop-research-tentative-conclusions/</link>
      <pubDate>Mon, 13 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/13/scientific-desktop-research-tentative-conclusions/</guid>
      <description>&lt;p&gt;Useful tip I got from &amp;ldquo;Extreme Productivity&amp;rdquo; by Robert Pozen, still reading.&lt;/p&gt;

&lt;p&gt;Desktop research, or simply known as googling in today&amp;rsquo;s context, is the act of collecting information about a particular topic or domain, with a broad/generic question in mind, or even without any question at all.&lt;/p&gt;

&lt;p&gt;One way to do this is simply keep googling related search terms and noting down facts and statistics, and constantly drilling down on a specific term until it&amp;rsquo;s milked - sort of like a depth-first search of collecting information. After we are satisfied with all the information we collect, we then try to piece together a story. But is this a good approach?&lt;/p&gt;

&lt;p&gt;Pozen argues that &amp;ldquo;although extensive research might seem a logical first step, it&amp;rsquo;s actually very inefficient.&amp;rdquo; This is because &amp;ldquo;there are literally thousands of facts that could be relevant to any project; do you really want to collect them all?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Instead, Pozen suggests that we first form &amp;ldquo;tentative conclusions&amp;rdquo;, by first thinking hard about our problem. &amp;ldquo;After a day or so of gathering relevant information, write down your tentative conclusions for the project. These will allow you to more quickly engage in analysis - rather than description - by providing a focus for your subsequent research.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;If you think about it, this makes a lot of sense and sounds more productive. And it&amp;rsquo;s a pretty scientific way of doing things - scientists don&amp;rsquo;t endeavour to do all the experiments that they can possibly imagine and then collecting all the data. Instead, they formulate hypotheses and design experiments to refute their claims. If their claims are not refuted, or are even proven, then they become published and reliable facts, for now.&lt;/p&gt;

&lt;p&gt;And this applies to more than just desktop research:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Think of how you first learn a programming language outside of a school setting. Odds are you &amp;ldquo;hacked&amp;rdquo; your way through to your current level of proficiency, by relying on Google and Stackoverflow. You were probably required to write some production or project code, had an idea, didn&amp;rsquo;t work out, googled, and try again and again. Not going through things like &amp;ldquo;Basic data structures in Python&amp;rdquo; and work from thereon. I am definitely guilty of this - when I first started learning about Spark and writing PySpark code, I tried multiple times to start from some tutorial and slowly work my way up toward proficiency. Each time, I failed either because the material gets boring, or I lose patience, or something like that. It just doesn&amp;rsquo;t work.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Which is the more efficient of doing exploratory data analysis - plotting every single plot and collecting every single summary statistics there is to your data, or coming up with several hypotheses and then visualising and testing them? &amp;ldquo;My guess is that age is not normally distributed in this dataset, let&amp;rsquo;s see.&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[short] Types of tasks</title>
      <link>/post/2017/02/12/types-of-tasks/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/02/12/types-of-tasks/</guid>
      <description>&lt;p&gt;In my work so far, there are typically a few type of tasks:&lt;/p&gt;

&lt;p&gt;&lt;li&gt;Analytics tasks (planning, architecture, setup, coding etc)
&lt;li&gt;Writing (documentations, reflections)
&lt;li&gt;Desktop research
&lt;li&gt;Emails (replying, drafting new ones)
&lt;li&gt;Ideations
&lt;li&gt;Team huddles
&lt;li&gt;External meetings with customers
&lt;li&gt;Internal project meetings
&lt;li&gt;Preparing decks and presentations
&lt;li&gt;Organising materials
&lt;li&gt;Admin&lt;/p&gt;

&lt;p&gt;I think they can divided into four quadrants:&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td style=&#34;vertical-align:top&#34;&gt;
  &lt;b&gt;High amount of visible results, low value&lt;/b&gt;
  &lt;ul&gt;
       &lt;li&gt;Emails (replying, drafting new ones)&lt;/li&gt;
       &lt;li&gt;Organising materials&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/td&gt;
  &lt;td style=&#34;vertical-align:top&#34;&gt;
  &lt;b&gt;High amount of visible results, high value&lt;/b&gt;
  &lt;ul&gt;
    &lt;li&gt;Analytics tasks (coding, planning)&lt;/li&gt;
    &lt;li&gt;Writing&lt;/li&gt;
    &lt;li&gt;Preparing decks and presentations&lt;/li&gt;
    &lt;li&gt;Customer meetings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/td&gt;
 &lt;/tr&gt;
 
 &lt;tr&gt;
  &lt;td style=&#34;vertical-align:top&#34;&gt;
  &lt;b&gt;Low amount of visible results, low value&lt;/b&gt;
  &lt;ul&gt;
    &lt;li&gt;Admin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/td&gt;
  &lt;td style=&#34;vertical-align:top&#34;&gt;
  &lt;b&gt;Low amount of visible results, high value&lt;/b&gt;
  &lt;ul&gt;
    &lt;li&gt;Desktop research&lt;/li&gt;
    &lt;li&gt;Ideations, brainstorming&lt;/li&gt;
    &lt;li&gt;Team huddles&lt;/li&gt;
    &lt;li&gt;Internal project meetings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/td&gt;
 &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;There&amp;rsquo;s a lot of unpack here, so one by one:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Visible results: a deck, an email, a document, a discussion with potential customers - tangible stuff&lt;/li&gt;
&lt;li&gt;Value: high value means generating a good idea, building strong relationships with customers or fellow colleagues in the same or different team, getting a clearer idea of something that allows you to move forward from current status quo&lt;/li&gt;
&lt;li&gt;A good day at work would be to spend most if not all of your time in Quadrant 1, generating both value and results&lt;/li&gt;
&lt;li&gt;Quadrant 2 gives high value because these are the types of tasks that spark the team and yourself off towards a good week or month. These tasks generally give ideas and directions, and hence high value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a morning person, my energy dwindle towards the end of the day, especially with a trough after lunch. Within this framework there must be a way to optimise my productivity in a work day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Taking a part-time Masters this year in 2017</title>
      <link>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</link>
      <pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/01/30/taking-a-part-time-masters-this-year-in-2017/</guid>
      <description>&lt;p&gt;I am leaning towards taking a part-time Masters this year in 2017. Points of considerations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Masters or PhD?&lt;/li&gt;
&lt;li&gt;Full-time or part-time?&lt;/li&gt;
&lt;li&gt;(If masters) technical or non-technical?&lt;/li&gt;
&lt;li&gt;(If full-time) Overseas or local?&lt;/li&gt;
&lt;li&gt;(If local) NUS or NTU?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Full-time is personally a non-option for me, as I don&amp;rsquo;t see the financial sense in taking a sabbatical to pursue a full-time programme. That leaves full-time and overseas out.&lt;/p&gt;

&lt;p&gt;Based on what I know about a PhD programme, part-time PhD sounds like a nightmare. That leaves part-time Masters in Singapore as my option.&lt;/p&gt;

&lt;p&gt;Next question: technical or non-technical? Well I am leaning towards to doing something with technical content when studying - non-technical content can be picked up most of the time simply by being widely read and learning from work experiences. This means statistics or computing for me.&lt;/p&gt;

&lt;p&gt;And NUS is probably the better choice than NTU. SMU is not in my consideration.&lt;/p&gt;

&lt;p&gt;So for now, my choice is going to be M.Sc. Statistics from NUS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three Needs Theory - Power, Achievement, Affliation</title>
      <link>/post/2017/01/02/three-needs-theory-power-achievement-affliation/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/01/02/three-needs-theory-power-achievement-affliation/</guid>
      <description>&lt;p&gt;The Three Needs Theory was developed by psychologist David McClelland in the 1960s. The theory identifies three main needs in the professional setting, which form the drivers of motivation. These needs are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The need for power (n-Pow)&lt;/li&gt;
&lt;li&gt;The need for achievement (n-Ach)&lt;/li&gt;
&lt;li&gt;The need for affiliation (n-Aff)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I first encountered this idea in the book Managing Scientists: Leadership Strategies in Scientific Research by Alice Sapienza, which I have yet to finish reading. Simply put, every individual identifies with one, two (usually one major, one minor), or even three (rare) of these needs. Fulfilling these needs act as motivators, which in turn determine one&amp;rsquo;s course of action in a given set of circumstances and options.&lt;/p&gt;

&lt;p&gt;In general, individuals who identify significantly with a particular need tend towards certain types of behaviour, which may serve as tell-tale signs from the perspective of others. However, this doesn&amp;rsquo;t mean these behaviours are indeed exhibited in reality, due to constraints such as rank or resources.&lt;/p&gt;

&lt;p&gt;Folks with high n-Pow tend to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Control other people for personal or greater good (need for personal power or need for institutional power; personal power or socialised power)&lt;/li&gt;
&lt;li&gt;Seek neither recognition nor approval, only agreement and compliance&lt;/li&gt;
&lt;li&gt;Be argumentative&lt;/li&gt;
&lt;li&gt;Be assertive&lt;/li&gt;
&lt;li&gt;Practise discipline&lt;/li&gt;
&lt;li&gt;Be concerned about reputation&lt;/li&gt;
&lt;li&gt;Arouse emotions in others to push towards a cause&lt;/li&gt;
&lt;li&gt;There is also a distinction between the need for personal power versus institutional power.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Needless to say, the need for personal power is not exactly the healthiest need in a professional setting.&lt;/p&gt;

&lt;p&gt;On the other hand, folks with high n-Ach tend to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enjoy practising and honing their skillsets&lt;/li&gt;
&lt;li&gt;Enjoy winning&lt;/li&gt;
&lt;li&gt;Seek improvements&lt;/li&gt;
&lt;li&gt;Seek to do things more efficiently&lt;/li&gt;
&lt;li&gt;Appeal more to the intrinsic value of a task (technical difficulties or complexities) than the &amp;ldquo;extrinsic value&amp;rdquo; of the task towards a greater goal (company P&amp;amp;L, personal or company reputation etc.)&lt;/li&gt;
&lt;li&gt;Enjoy tasks where effort is approximately proportionate to results&lt;/li&gt;
&lt;li&gt;Prefer tasks that are neither low risk (not challenging) nor high risk (too much left to chance but not effort)&lt;/li&gt;
&lt;li&gt;Enjoy constructive feedback, both positive and negative&lt;/li&gt;
&lt;li&gt;Set self-imposed goals&lt;/li&gt;
&lt;li&gt;Be inventive or creative&lt;/li&gt;
&lt;li&gt;Enjoy working alone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, folks with high n-Aff tend to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Value interpersonal relationships&lt;/li&gt;
&lt;li&gt;Conform to norms in the workplace&lt;/li&gt;
&lt;li&gt;Prefer collaboration over competition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Naturally, an individual&amp;rsquo;s dominant need(s) will determine the type of role in which he/she will excel in. For example, entrepreneurs tend to be of high n-Ach. The best senior management leaders tend to be of high n-Pow (institutional power) and high n-Aff. The best middle management leaders, on the other hand, tend to be high n-Ach and a bit less of n-Aff. (Can you figure out why? This is because in a typical corporate setting, middle managers are promoted into their positions due to the competencies they have shown in the working level, displaying high n- Ach. However, to rise further up into senior management, these without high n-Pow will be eliminated eventually.) The best salesperson will be of high-Ach and high n-Aff, while a high n-Aff works well for project managers.&lt;/p&gt;

&lt;p&gt;Personally, I find this theory to be a useful mental model - look into your workplace, and consider both your colleagues (peers and reportees), and your immediate boss. Can you confidently identify their motivational needs, be it power, achievement, or affiliation? It&amp;rsquo;s not hard to do so, isn&amp;rsquo;t it&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s probably worthwhile to learn more about these needs, how they manifest in the workplace and the role they play in motivation, productivity, and workplace relationships. I will definitely write another piece on this. Even if you are not interested in how these needs play out amongst others in your workplace, you would still be interested in identifying your personal needs (though if you think this way, you probably belong to the low n-Pow, high n-Ach, and low n-Aff camp).&lt;/p&gt;

&lt;p&gt;For now, here&amp;rsquo;s a rather insightful HBR article discussing the three needs and the role they play in leadership: &lt;a href=&#34;https://hbr.org/2003/01/power-is-the-great-motivator&#34;&gt;https://hbr.org/2003/01/power-is-the-great-motivator&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[short] Book Review: The McKinsey Edge</title>
      <link>/post/2016/12/31/book-review-the-mckinsey-edge/</link>
      <pubDate>Sat, 31 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/12/31/book-review-the-mckinsey-edge/</guid>
      <description>&lt;p&gt;The McKinsey Edge by Shu Hattori is a collection of &amp;ldquo;principles&amp;rdquo; that the author collected while he was at
consultant at McKinsey. The forty-odd principles in the book, while easily understood, stood out
when they are collated together in a single book, as many productivity and professional hacks
strung together. A rather interesting read.&lt;/p&gt;

&lt;p&gt;Following are his principles divided into four chapters, and in bold are those that I thought to be
particularly useful, and would like to expound on them in future writings.&lt;/p&gt;

&lt;p&gt;Building the Better Self&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Focus on what really matters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Start with the hard stuff in the morning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Catch small signals and make a difference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a 30-second answer to everything&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frontload your project&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create the right end output image&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smile when you are under stress&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go beyond your self-perceived limit&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Always imagine the worst-case scenario&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Start following up&lt;/li&gt;
&lt;li&gt;Push back with less emotion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be flexible on the perception of your passion&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What would Marvin do?&amp;rdquo; Find your role models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Know what gives your the most energy in your day&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go jogging to smell the flowers&lt;/li&gt;
&lt;li&gt;Create a commitment plan&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Growing with Others&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always memorize the first three sentences of a presentation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Communicate using fewer words&lt;/li&gt;
&lt;li&gt;Pause three seconds before answering difficult questions&lt;/li&gt;
&lt;li&gt;Question more and talk less&lt;/li&gt;
&lt;li&gt;Turn no into yes&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t show half-baked output&lt;/li&gt;
&lt;li&gt;Instantly find a connection in the room&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be a giver, not a receiver&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Find the best intent in people&lt;/li&gt;
&lt;li&gt;Learn team member&amp;rsquo;s defining moments and personal sides&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Think of everyone as a helpful individual, not a &amp;ldquo;resource&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Go out for a meal with interesting people every week&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consciously gauge your people&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign team members meaningful tasks&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create followership through deliberate on-the-job coaching&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Deliver feedback using positive criticism&lt;/li&gt;
&lt;li&gt;Please your assistants and support staff&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Execlling in Process Management&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always prepare an agenda before meetings&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create &amp;ldquo;four boxes&amp;rdquo; to dos&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus on outcomes not activities&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Know your meeting modes in advance&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proactively manage e-mail communication using the 5D rules&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speak up as early as possible&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create a minimalist presentation toolkit&lt;/li&gt;
&lt;li&gt;Create an easy-to-use template for updates&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Going the Extra mile&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Give away knowledge and tools unsparingly&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Get rid of your physical barriers&lt;/li&gt;
&lt;li&gt;Ask the second order questions&lt;/li&gt;
&lt;li&gt;Learn to write fewer notes&lt;/li&gt;
&lt;li&gt;Prepare to renew your life&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create your own profile as a leader&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Random Forests in R</title>
      <link>/post/2016/07/20/random-forests-in-r/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/07/20/random-forests-in-r/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As the name suggests, random forest models basically contain an ensemble of decision tree models, with each decision tree predicting the same response variable. The response may be categorical, in which case being a classification problem, or continuous / numerical, being a regression problem.&lt;/p&gt;
&lt;p&gt;In this short tutorial, we will go through the use of tree-based methods (decision tree, bagging model, and random forest) for both classification and regression problems.&lt;/p&gt;
&lt;p&gt;This tutorial is divided into two sections. We will first use tree-based methods for classification on the &lt;strong&gt;spam&lt;/strong&gt; dataset from the &lt;strong&gt;kernlab&lt;/strong&gt; package. Subsequently, we will apply these methods on a regression problem, with the &lt;strong&gt;imports85&lt;/strong&gt; dataset from the &lt;strong&gt;randomForest&lt;/strong&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tree-based-methods-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tree-based methods for classification&lt;/h2&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Let’s start by loading the spam dataset and doing some preparations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# packages that we will need:
#  @ kernlab:      for the spam dataset
#  @ tree:         for decision tree construction
#  @ randomForest: for bagging and RF
#  @ beepr:        for a little beep
#  @ pROC:         for plotting of ROC

# code snippet to install and load multiple packages at once
# pkgs &amp;lt;- c(&amp;quot;kernlab&amp;quot;,&amp;quot;tree&amp;quot;,&amp;quot;randomForest&amp;quot;,&amp;quot;beepr&amp;quot;,&amp;quot;pROC&amp;quot;)
# sapply(pkgs,FUN=function(p){
#        print(p)
#        if(!require(p)) install.packages(p)
#        require(p)
# })

# load required packages
suppressWarnings(library(kernlab))
suppressWarnings(library(tree))
suppressWarnings(library(randomForest))
## randomForest 4.6-14
## Type rfNews() to see new features/changes/bug fixes.
suppressWarnings(library(beepr)) # try it! beep()
suppressWarnings(library(pROC))
## Type &amp;#39;citation(&amp;quot;pROC&amp;quot;)&amp;#39; for a citation.
## 
## Attaching package: &amp;#39;pROC&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     cov, smooth, var

# load dataset
data(spam)

# take a look
str(spam)
## &amp;#39;data.frame&amp;#39;:    4601 obs. of  58 variables:
##  $ make             : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
##  $ address          : num  0.64 0.28 0 0 0 0 0 0 0 0.12 ...
##  $ all              : num  0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
##  $ num3d            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ our              : num  0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
##  $ over             : num  0 0.28 0.19 0 0 0 0 0 0 0.32 ...
##  $ remove           : num  0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...
##  $ internet         : num  0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...
##  $ order            : num  0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...
##  $ mail             : num  0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...
##  $ receive          : num  0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...
##  $ will             : num  0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...
##  $ people           : num  0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...
##  $ report           : num  0 0.21 0 0 0 0 0 0 0 0 ...
##  $ addresses        : num  0 0.14 1.75 0 0 0 0 0 0 0.12 ...
##  $ free             : num  0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...
##  $ business         : num  0 0.07 0.06 0 0 0 0 0 0 0 ...
##  $ email            : num  1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...
##  $ you              : num  1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...
##  $ credit           : num  0 0 0.32 0 0 0 0 0 3.53 0.06 ...
##  $ your             : num  0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...
##  $ font             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num000           : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...
##  $ money            : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...
##  $ hp               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ hpl              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ george           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num650           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ lab              : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ labs             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ telnet           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num857           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ data             : num  0 0 0 0 0 0 0 0 0.15 0 ...
##  $ num415           : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num85            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ technology       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num1999          : num  0 0.07 0 0 0 0 0 0 0 0 ...
##  $ parts            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ pm               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ direct           : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ cs               : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ meeting          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ original         : num  0 0 0.12 0 0 0 0 0 0.3 0 ...
##  $ project          : num  0 0 0 0 0 0 0 0 0 0.06 ...
##  $ re               : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ edu              : num  0 0 0.06 0 0 0 0 0 0 0 ...
##  $ table            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ conference       : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charSemicolon    : num  0 0 0.01 0 0 0 0 0 0 0.04 ...
##  $ charRoundbracket : num  0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...
##  $ charSquarebracket: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ charExclamation  : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
##  $ charDollar       : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
##  $ charHash         : num  0 0.048 0.01 0 0 0 0 0 0.022 0 ...
##  $ capitalAve       : num  3.76 5.11 9.82 3.54 3.54 ...
##  $ capitalLong      : num  61 101 485 40 40 15 4 11 445 43 ...
##  $ capitalTotal     : num  278 1028 2259 191 191 ...
##  $ type             : Factor w/ 2 levels &amp;quot;nonspam&amp;quot;,&amp;quot;spam&amp;quot;: 2 2 2 2 2 2 2 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we will attempt to predict whether an email is spam or nonspam. To do so, we will construct models on one subset of the data (training data), and use the constructed model on another disparate subset of the data (the testing data). This is known as cross validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preparation for cross validation:
# split the dataset into 2 halves,
# 2300 samples for training and 2301 for testing
num.samples &amp;lt;- nrow(spam) # 4,601
num.train   &amp;lt;- round(num.samples/2) # 2,300
num.test    &amp;lt;- num.samples - num.train # 2,301
num.var     &amp;lt;- ncol(spam) # 58

# set up the indices
set.seed(150715)
idx       &amp;lt;- sample(1:num.samples)
train.idx &amp;lt;- idx[seq(num.train)]
test.idx  &amp;lt;- setdiff(idx,train.idx)

# subset the data
spam.train &amp;lt;- spam[train.idx,]
spam.test  &amp;lt;- spam[test.idx,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a quick glance at the &lt;strong&gt;type&lt;/strong&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(spam.train$type)
## 
## nonspam    spam 
##    1397     903
table(spam.test$type)
## 
## nonspam    spam 
##    1391     910&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-tree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision tree&lt;/h3&gt;
&lt;p&gt;Now that we are done with the preparation, let’s start by constructing a decision tree model, using the &lt;strong&gt;tree&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree.mod &amp;lt;- tree(type ~ ., data = spam.train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how our model looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(tree.mod)
title(&amp;quot;Decision tree&amp;quot;)
text(tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model may be overtly complicated. Typically, after constructing a decision tree model, we may want to prune the model, by collapsing certain edges, nodes and leaves together without much loss of performance. This is done by iteratively comparing the number of leaf nodes with the model’s performance (by k-fold cross validation &lt;em&gt;within the training set&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.prune &amp;lt;- cv.tree(tree.mod, FUN = prune.misclass)
plot(cv.prune$size, cv.prune$dev, pch = 20, col = &amp;quot;red&amp;quot;, type = &amp;quot;b&amp;quot;,
     main = &amp;quot;Decision tree: Cross validation to find optimal size of tree&amp;quot;,
     xlab = &amp;quot;Size of tree&amp;quot;, ylab = &amp;quot;Misclassifications&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Having 9 leaf nodes may be good (maximising performance while minimising complexity).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best.tree.size &amp;lt;- 9

# pruning (cost-complexity pruning)
pruned.tree.mod &amp;lt;- prune.misclass(tree.mod, best = best.tree.size)

# here&amp;#39;s the new tree model
plot(pruned.tree.mod)
title(paste(&amp;quot;Pruned decision tree (&amp;quot;, best.tree.size, &amp;quot; leaf nodes)&amp;quot;,sep = &amp;quot;&amp;quot;))
text(pruned.tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_tree4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with our new model, let’s make some predictions on the testing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree.pred &amp;lt;- predict(pruned.tree.mod,
                     subset(spam.test, select = -type), 
                     type = &amp;quot;class&amp;quot;)

# confusion matrix
# rows are the predicted classes
# columns are the actual classes
print(tree.pred.results &amp;lt;- table(tree.pred, spam.test$type))
##          
## tree.pred nonspam spam
##   nonspam    1308  164
##   spam         83  746

# What is the accuracy of our tree model?
print(tree.accuracy &amp;lt;- (tree.pred.results[1,1] + tree.pred.results[2,2]) / sum(tree.pred.results))
## [1] 0.8926554&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our decision tree model is able to predict spam vs. nonspam emails with about 89.27% accuracy. We will make comparisons of accuracies with other models later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Next, we turn our attention to the bagging model. Recall that bagging, a.k.a. &lt;em&gt;bootstrap aggregating&lt;/em&gt;, is the process of sampling (with replacement), samples from the training data. Each of these subsets are known as bags, and we construct individual decision tree models using each of these bags. Finally, to make a classification prediction, we use the majority vote from the ensemble of decision tree models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.mod&amp;lt;-randomForest(type ~ ., data = spam.train,
                     mtry = num.var - 1, # try all variables at each split, except the response variable
                     ntree = 300,
                     proximity = TRUE,
                     importance = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the bagging, and also the random forest model, there are often only two hyperparameters that we are interested in: &lt;strong&gt;mtry&lt;/strong&gt;, which is the number of variables to try from for each tree and at each split, and &lt;strong&gt;ntree&lt;/strong&gt;, the number of trees in the ensemble. Tuning the number of trees is relatively easy by looking at the out-of-bag (OOB) error estimate of the ensemble at each step of the way. For more details, refer to the slides. We set &lt;strong&gt;proximity = TRUE&lt;/strong&gt; and &lt;strong&gt;importance = TRUE&lt;/strong&gt;, in order to get some form of visualization of the model, and the variable importances respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(bg.mod$err.rate[,1], type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Bagging: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, 300 trees seems more than sufficient. One advantage of bagging and random forest models is that they provide a way of doing feature or variable selection, by considering the importance of each variable in the model. For exact details on how these importance measures are defined, refer to the slides.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(bg.mod,
           main = &amp;quot;Bagging: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In addition, we can visualize the classification done by the model using a multidimensional plot on the proximity matrix. The green samples in the figure represent nonspams, while the red samples are spams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MDSplot(bg.mod,
        fac = spam.train$type,
        palette = c(&amp;quot;green&amp;quot;,&amp;quot;red&amp;quot;),
        main = &amp;quot;Bagging: MDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_bagging4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let’s make some predictions on the testing data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.pred &amp;lt;- predict(bg.mod,
                   subset(spam.test, select = -type), 
                   type = &amp;quot;class&amp;quot;)

# confusion matrix
# rows are the predicted classes
# columns are the actual classes
print(bg.pred.results &amp;lt;- table(bg.pred, spam.test$type))
##          
## bg.pred   nonspam spam
##   nonspam    1336   87
##   spam         55  823

# what is the accuracy of our bagging model?
print(bg.accuracy &amp;lt;- sum(diag((bg.pred.results))) / sum(bg.pred.results))
## [1] 0.9382877&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our bagging model predicts whether an email is spam or not with about 93.83% accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;The only difference between the bagging model and random forest model is that the latter uses chooses only from a subset of variables to split on at each node of each tree. In other words, only the &lt;strong&gt;mtry&lt;/strong&gt; argument differs between bagging and random forest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.mod &amp;lt;- randomForest(type ~ ., data = spam.train,
                       mtry = floor(sqrt(num.var - 1)), # 7; only difference from bagging is here
                       ntree = 300,
                       proximity = TRUE,
                       importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees:
plot(rf.mod$err.rate[,1], type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Random forest: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Besides tuning the &lt;strong&gt;ntree&lt;/strong&gt; hyperparameter, we might also be interested in tuning the &lt;strong&gt;mtry&lt;/strong&gt; hyperparameter in random forest. The random forest model may be built using the &lt;strong&gt;mtry&lt;/strong&gt; value that minimises the OOB error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuneRF(subset(spam.train, select = -type),
       spam.train$type,
       ntreeTry = 100)
## mtry = 7  OOB error = 5.52% 
## Searching left ...
## mtry = 4     OOB error = 6.26% 
## -0.1338583 0.05 
## Searching right ...
## mtry = 14    OOB error = 5.83% 
## -0.05511811 0.05
##        mtry   OOBError
## 4.OOB     4 0.06260870
## 7.OOB     7 0.05521739
## 14.OOB   14 0.05826087
title(&amp;quot;Random forest: Tuning the mtry hyperparameter&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# variable importance
varImpPlot(rf.mod,
           main = &amp;quot;Random forest: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# multidimensional scaling plot
# green samples are non-spam,
# red samples are spam
MDSplot(rf.mod,
        fac = spam.train$type,
        palette = c(&amp;quot;green&amp;quot;,&amp;quot;red&amp;quot;),
        main = &amp;quot;Random forest: MDS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_rf3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# now, let&amp;#39;s make some predictions
rf.pred &amp;lt;- predict(rf.mod,
                   subset(spam.test,select = -type), 
                   type=&amp;quot;class&amp;quot;)

# confusion matrix
print(rf.pred.results &amp;lt;- table(rf.pred, spam.test$type))
##          
## rf.pred   nonspam spam
##   nonspam    1353   82
##   spam         38  828

# Accuracy of our RF model:
print(rf.accuracy &amp;lt;- sum(diag((rf.pred.results))) / sum(rf.pred.results))
## [1] 0.9478488&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our random forest model predicts whether an email is spam or not with about 94.78% accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-performances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization of performances&lt;/h3&gt;
&lt;p&gt;Let’s go ahead and make some comparisons on the performances of our model. For comparison sake, let’s also construct a logistic regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log.mod &amp;lt;- glm(type ~ . , data = spam.train,
             family = binomial(link = logit))
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

# predictions
log.pred.prob &amp;lt;- predict(log.mod,
                         subset(spam.test, select = -type), 
                         type = &amp;quot;response&amp;quot;)
log.pred.class &amp;lt;- factor(sapply(log.pred.prob,
                                FUN = function(x){
                                        if(x &amp;gt;= 0.5) return(&amp;quot;spam&amp;quot;)
                                        else return(&amp;quot;nonspam&amp;quot;)
                                }))

# confusion matrix
log.pred.results &amp;lt;- table(log.pred.class, spam.test$type)

# Accuracy of logistic regression model:
print(log.accuracy &amp;lt;- sum(diag((log.pred.results))) / sum(log.pred.results))
## [1] 0.9135159&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s compare the performances, considering first the model accuracies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(c(tree.accuracy,
          bg.accuracy,
          rf.accuracy,
          log.accuracy),
        main=&amp;quot;Accuracies of various models&amp;quot;,
        names.arg=c(&amp;quot;Tree&amp;quot;,&amp;quot;Bagging&amp;quot;,&amp;quot;RF&amp;quot;, &amp;quot;Logistic&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_viz1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that the ensemble models (bagging and random forest) outperforms the single decision tree, and also the logistic regression model. It turns out here that the bagging and the random forest models have about the same classification performance. Understanding the rationale of &lt;em&gt;random subspace sampling&lt;/em&gt; (refer to slides) should allow us to appreciate the potential improvement of random forest over the bagging model.&lt;/p&gt;
&lt;p&gt;Finally, let’s plot the ROC curves of the various models. The ROC is only valid for models that give probabilistic output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.pred.prob &amp;lt;- predict(bg.mod ,
                        subset(spam.test, select = -type),
                        type = &amp;quot;prob&amp;quot;)

rf.pred.prob &amp;lt;- predict(rf.mod ,
                        subset(spam.test, select = -type),
                        type = &amp;quot;prob&amp;quot;)

plot.roc(spam.test$type,
         bg.pred.prob[,1], col = &amp;quot;blue&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.3,
         main = &amp;quot;ROC-AUC of various models&amp;quot;)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;gt; cases

plot.roc(spam.test$type,
         rf.pred.prob[,1], col = &amp;quot;green&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.2,
         add = TRUE)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;gt; cases

plot.roc(spam.test$type,
         log.pred.prob, col = &amp;quot;red&amp;quot;,
         lwd = 3, print.auc = TRUE, print.auc.y = 0.1,
         add = TRUE)
## Setting levels: control = nonspam, case = spam
## Setting direction: controls &amp;lt; cases

legend(x = 0.6, y = 0.8, legend = c(&amp;quot;Bagging&amp;quot;,
                                    &amp;quot;Random forest&amp;quot;,
                                    &amp;quot;Logistic regression&amp;quot;),
       col = c(&amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;), lwd = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/c_viz2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tree-based-methods-for-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tree-based methods for regression&lt;/h2&gt;
&lt;p&gt;In the following section, we will consider the use of tree-based methods for regression. The materials that follows are analogous to that above, if not the similar.&lt;/p&gt;
&lt;div id=&#34;preparation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tree)
library(randomForest)

data(imports85)
imp &amp;lt;- imports85

# The following data preprocessing steps on
# the imports85 dataset are suggested by
# the authors of the randomForest package
# look at
# &amp;gt; ?imports85
imp &amp;lt;- imp[,-2]  # Too many NAs in normalizedLosses.
imp &amp;lt;- imp[complete.cases(imp), ]
# ## Drop empty levels for factors
imp[] &amp;lt;- lapply(imp, function(x) if (is.factor(x)) x[, drop=TRUE] else x)

# Also removing the numOfCylinders and fuelSystem
# variables due to sparsity of data
# to see this, run the following lines:
# &amp;gt; table(imp$numOfCylinders)
# &amp;gt; table(imp$fuelSystem)
# This additional step is only necessary because we will be
# making comparisons between the tree-based models
# and linear regression, and linear regression cannot
# handle sparse data well
imp &amp;lt;- subset(imp, select = -c(numOfCylinders,fuelSystem))

# also removing the make variable
imp &amp;lt;- subset(imp, select = -make)

# Preparation for cross validation:
# split the dataset into 2 halves,
# 96 samples for training and 97 for testing
num.samples &amp;lt;- nrow(imp) # 193
num.train   &amp;lt;- round(num.samples / 2) # 96
num.test    &amp;lt;- num.samples - num.train # 97
num.var     &amp;lt;- ncol(imp) # 25

# set up the indices
set.seed(150715)
idx       &amp;lt;- sample(1:num.samples)
train.idx &amp;lt;- idx[seq(num.train)]
test.idx  &amp;lt;- setdiff(idx,train.idx)

# subset the data
imp.train &amp;lt;- imp[train.idx,]
imp.test  &amp;lt;- imp[test.idx,]

str(imp.train)
## &amp;#39;data.frame&amp;#39;:    96 obs. of  22 variables:
##  $ symboling       : int  1 0 0 3 2 1 1 1 3 2 ...
##  $ fuelType        : Factor w/ 2 levels &amp;quot;diesel&amp;quot;,&amp;quot;gas&amp;quot;: 2 2 2 2 1 2 2 2 2 2 ...
##  $ aspiration      : Factor w/ 2 levels &amp;quot;std&amp;quot;,&amp;quot;turbo&amp;quot;: 2 1 2 1 1 1 1 1 2 2 ...
##  $ numOfDoors      : Factor w/ 2 levels &amp;quot;four&amp;quot;,&amp;quot;two&amp;quot;: 1 1 1 2 2 2 1 2 2 1 ...
##  $ bodyStyle       : Factor w/ 5 levels &amp;quot;convertible&amp;quot;,..: 4 4 4 3 4 4 4 3 3 4 ...
##  $ driveWheels     : Factor w/ 3 levels &amp;quot;4wd&amp;quot;,&amp;quot;fwd&amp;quot;,&amp;quot;rwd&amp;quot;: 2 2 3 3 2 3 2 2 2 2 ...
##  $ engineLocation  : Factor w/ 2 levels &amp;quot;front&amp;quot;,&amp;quot;rear&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ wheelBase       : num  96.3 97.2 108 102.9 97.3 ...
##  $ length          : num  172 173 187 184 172 ...
##  $ width           : num  65.4 65.2 68.3 67.7 65.5 64 63.8 66.5 65.4 66.5 ...
##  $ height          : num  51.6 54.7 56 52 55.7 52.6 54.5 53.7 49.4 56.1 ...
##  $ curbWeight      : int  2403 2302 3130 2976 2261 2265 1971 2385 2370 2847 ...
##  $ engineType      : Factor w/ 5 levels &amp;quot;dohc&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;ohc&amp;quot;,..: 3 3 2 1 3 1 3 3 3 1 ...
##  $ engineSize      : int  110 120 134 171 97 98 97 122 110 121 ...
##  $ bore            : num  3.17 3.33 3.61 3.27 3.01 3.24 3.15 3.39 3.17 3.54 ...
##  $ stroke          : num  3.46 3.47 3.21 3.35 3.4 3.08 3.29 3.39 3.46 3.07 ...
##  $ compressionRatio: num  7.5 8.5 7 9.3 23 9.4 9.4 8.6 7.5 9 ...
##  $ horsepower      : int  116 97 142 161 52 112 69 84 116 160 ...
##  $ peakRpm         : int  5500 5200 5600 5200 4800 6600 5200 4800 5500 5500 ...
##  $ cityMpg         : int  23 27 18 20 37 26 31 26 23 19 ...
##  $ highwayMpg      : int  30 34 24 24 46 29 37 32 30 26 ...
##  $ price           : int  9279 9549 18150 16558 7775 9298 7499 10595 9959 18620 ...
str(imp.test)
## &amp;#39;data.frame&amp;#39;:    97 obs. of  22 variables:
##  $ symboling       : int  -1 1 -1 1 1 -2 0 0 2 2 ...
##  $ fuelType        : Factor w/ 2 levels &amp;quot;diesel&amp;quot;,&amp;quot;gas&amp;quot;: 2 2 1 2 2 2 1 2 2 2 ...
##  $ aspiration      : Factor w/ 2 levels &amp;quot;std&amp;quot;,&amp;quot;turbo&amp;quot;: 1 1 2 1 1 1 2 1 1 1 ...
##  $ numOfDoors      : Factor w/ 2 levels &amp;quot;four&amp;quot;,&amp;quot;two&amp;quot;: 1 1 1 2 2 1 2 2 2 2 ...
##  $ bodyStyle       : Factor w/ 5 levels &amp;quot;convertible&amp;quot;,..: 4 4 5 4 4 4 2 4 1 3 ...
##  $ driveWheels     : Factor w/ 3 levels &amp;quot;4wd&amp;quot;,&amp;quot;fwd&amp;quot;,&amp;quot;rwd&amp;quot;: 3 3 3 3 2 3 3 3 3 2 ...
##  $ engineLocation  : Factor w/ 2 levels &amp;quot;front&amp;quot;,&amp;quot;rear&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ wheelBase       : num  115.6 103.5 110 94.5 94.5 ...
##  $ length          : num  203 189 191 169 165 ...
##  $ width           : num  71.7 66.9 70.3 64 63.8 67.2 70.3 70.6 65.6 64.4 ...
##  $ height          : num  56.5 55.7 58.7 52.6 54.5 56.2 54.9 47.8 53 50.8 ...
##  $ curbWeight      : int  3740 3055 3750 2169 1918 2935 3495 3950 2975 1944 ...
##  $ engineType      : Factor w/ 5 levels &amp;quot;dohc&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;ohc&amp;quot;,..: 5 3 3 3 3 3 3 5 3 3 ...
##  $ engineSize      : int  234 164 183 98 97 141 183 326 146 92 ...
##  $ bore            : num  3.46 3.31 3.58 3.19 3.15 3.78 3.58 3.54 3.62 2.97 ...
##  $ stroke          : num  3.1 3.19 3.64 3.03 3.29 3.15 3.64 2.76 3.5 3.23 ...
##  $ compressionRatio: num  8.3 9 21.5 9 9.4 9.5 21.5 11.5 9.3 9.4 ...
##  $ horsepower      : int  155 121 123 70 69 114 123 262 116 68 ...
##  $ peakRpm         : int  4750 4250 4350 4800 5200 5400 4350 5000 4800 5500 ...
##  $ cityMpg         : int  16 20 22 29 31 24 22 13 24 31 ...
##  $ highwayMpg      : int  18 25 25 34 37 28 25 17 30 38 ...
##  $ price           : int  34184 24565 28248 8058 6649 15985 28176 36000 17669 6189 ...

# take a quick look
hist(imp.train$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(imp.test$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will be predicting the price of imported automobiles in this example. While tree-based methods are scale-invariant with respect to predictor variables, this is not true for the response variable. Hence, let’s take a log-transformation on &lt;strong&gt;price&lt;/strong&gt; here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp.train$price &amp;lt;- log(imp.train$price)
imp.test$price &amp;lt;- log(imp.test$price)

# take a look again
hist(imp.train$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(imp.test$price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_prep2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-tree-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision tree&lt;/h3&gt;
&lt;p&gt;Done with the preparation, let’s begin with decision trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Construct decision tree model
tree.mod &amp;lt;- tree(price ~ ., data = imp.train)

# here&amp;#39;s how the model looks like
plot(tree.mod)
title(&amp;quot;Decision tree&amp;quot;)
text(tree.mod, cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s see if our decision tree requires pruning
cv.prune &amp;lt;- cv.tree(tree.mod, FUN = prune.tree)
plot(cv.prune$size, cv.prune$dev, pch = 20, col = &amp;quot;red&amp;quot;, type = &amp;quot;b&amp;quot;,
     main = &amp;quot;Cross validation to find optimal size of tree&amp;quot;,
     xlab = &amp;quot;Size of tree&amp;quot;, ylab = &amp;quot;Mean squared error&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# looks fine

# now let&amp;#39;s make some predictions
tree.pred &amp;lt;- predict(tree.mod,
                     subset(imp.test,select = -price), 
                     type = &amp;quot;vector&amp;quot;)

# Comparing our predictions with the test data:
plot(tree.pred, imp.test$price, main = &amp;quot;Decision tree: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1) # A prediction with zero error will lie on the y = x line&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_tree1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# What is the MSE of our model?
print(tree.mse &amp;lt;- mean((tree.pred - imp.test$price) ** 2))
## [1] 0.04716916&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our decision tree model predicts the price of imported automobiles with a mean squared error of 0.0472. As with the previous section, we will make comparsions on model performances later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Next, bagging.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bg.mod&amp;lt;-randomForest(price ~ ., data = imp.train,
                     mtry = num.var - 1, # try all variables at each split, except the response variable
                     ntree = 300,
                     importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees
# here, the error is the mean squared error,
# not classification error
plot(bg.mod$mse, type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Bagging: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# variable importance
varImpPlot(bg.mod,
           main = &amp;quot;Bagging: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s make some predictions
bg.pred &amp;lt;- predict(bg.mod,
                   subset(imp.test,select = -price))

# Comparing our predictions with test data:
plot(bg.pred,imp.test$price, main = &amp;quot;Bagging: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_bagging1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE of bagged model
print(bg.mse &amp;lt;- mean((bg.pred - imp.test$price) ** 2))
## [1] 0.03004431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our bagging model predicts the price of imported automobiles with a mean squared error of 0.03.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random forest&lt;/h3&gt;
&lt;p&gt;Finally, the random forest model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf.mod&amp;lt;-randomForest(price ~ ., data = imp.train,
                     mtry = floor((num.var - 1) / 3), # 7; only difference from bagging is here
                     ntree = 300,
                     importance = TRUE)

# Out-of-bag (OOB) error rate as a function of num. of trees
plot(rf.mod$mse, type = &amp;quot;l&amp;quot;, lwd = 3, col = &amp;quot;blue&amp;quot;,
     main = &amp;quot;Random forest: OOB estimate of error rate&amp;quot;,
     xlab = &amp;quot;Number of Trees&amp;quot;, ylab = &amp;quot;OOB error rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# tuning the mtry hyperparameter:
# model may be rebuilt if desired
tuneRF(subset(imp.train, select = -price),
       imp.train$price,
       ntreetry = 100)
## mtry = 7  OOB error = 0.02543962 
## Searching left ...
## mtry = 4     OOB error = 0.03064481 
## -0.2046095 0.05 
## Searching right ...
## mtry = 14    OOB error = 0.02643948 
## -0.03930348 0.05
##    mtry   OOBError
## 4     4 0.03064481
## 7     7 0.02543962
## 14   14 0.02643948
title(&amp;quot;Random forest: Tuning the mtry hyperparameter&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

# variable importance
varImpPlot(rf.mod,
           main = &amp;quot;Random forest: Variable importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# let&amp;#39;s make some predictions
rf.pred &amp;lt;- predict(rf.mod,
                   subset(imp.test, select = -price))

# Comparing our predictions with test data:
plot(rf.pred, imp.test$price, main = &amp;quot;Random forest: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_rf1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE of RF model
print(rf.mse &amp;lt;- mean((rf.pred - imp.test$price) ** 2))
## [1] 0.03139744&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our random forest model incurs a mean squared error of 0.0314 for the prediction of imported automobile prices&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-performances-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization of performances&lt;/h3&gt;
&lt;p&gt;For comparison purposes, let’s also construct a ordinary least squares (linear regression) model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ols.mod &amp;lt;- lm(price ~ ., data = imp.train)

# predictions
ols.pred &amp;lt;- predict(ols.mod,
                   subset(imp.test, select = -price))

# comparisons with test data:
plot(ols.pred, imp.test$price, main = &amp;quot;OLS: Actual vs. predicted&amp;quot;)
abline(a = 0, b = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_ols1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# MSE
print(ols.mse &amp;lt;- mean((ols.pred-imp.test$price) ** 2))
## [1] 0.03556617&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s compare their performances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Comparing MSEs of various models:
barplot(c(tree.mse,
          bg.mse,
          rf.mse,
          ols.mse),
        main = &amp;quot;Mean squared errors of various models&amp;quot;,
        names.arg = c(&amp;quot;Tree&amp;quot;, &amp;quot;Bagging&amp;quot;, &amp;quot;RF&amp;quot;, &amp;quot;OLS&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2016-07-20-random-forests-in-r_files/figure-html/r_viz1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our top performer here is the random forest model, followed by the bagging model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
