<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Paper Review: To Tune or Not to Tune the Number of Trees in Random Forest | The Stats Guy</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/ml-stats/">ML &amp; Stats</a></li>
      
      <li><a href="/categories/stats-finance/">Stats finance</a></li>
      
      <li><a href="/categories/finance-investing/">Finance &amp; Investing</a></li>
      
      <li><a href="/categories/corporate-rat-race/">Corporate rat race</a></li>
      
      <li><a href="/categories/stoicism/">Stoicism</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h2><span class="title">Paper Review: To Tune or Not to Tune the Number of Trees in Random Forest</span></h2>

<h3 class="date">2019/02/24</h3>
<h3>6min read</h3>
</div>

<main>
<p><center>
<img src="https://thestatsguyhome.files.wordpress.com/2019/02/download2-1.png" width="100%">
Plotting different performance metrics against the number of trees in random forest. <a href="https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf">Source</a>
</center></p>

<p>I came across the following paper during my Masters coursework that addresses a practical issue in the use of the random forest model, and in general, any other bootstrap aggregating ensembles:</p>

<p><a href="http://jmlr.org/papers/v18/17-269.html">Probst, P. &amp; Boulestix, A-L. (2018). To Tune or Not to Tune the Number of Trees in Random Forest. Journal of Machine Learning Research, 18(181), 1-18. </a></p>

<h2>1. Motivation</h2>

<p>This is an interesting paper as it directly addresses a fundamental question on whether the number of base learners in a bagging ensemble should be tuned. In the case of random forest (RF), the number of trees <em>T</em> is often regarded as a hyperparameter, in the sense that either too high or too low of a value would yield sub-par model performance. Tuning of <em>T</em> is typically done by plotting the one or multiple chosen out-of-bag (OOB) metrics, such as the error rate, as a function of <em>T</em>:</p>

<p><center>
<img src="https://thestatsguyhome.files.wordpress.com/2019/02/download.png" width="100%">
Different OOB metrics (error rate, Brier score, log loss, AUC) as a function of <em>T</em>. <a href="https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf">Source</a>
</center></p>

<p>In the case above, there are clear indications of convergence in <em>T</em>, and any further increase in <em>T</em> brings either marginal or zero improvements in model performance. However, that's not always the case, prompting the treatment of <em>T</em> as a model hyperparameter:</p>

<p><center>
<img src="https://thestatsguyhome.files.wordpress.com/2019/02/download2.png" width="100%">
Non-convergence of <em>T</em>. <a href="https://github.com/PhilippPro/tuneNtree/blob/master/graphics/binary_classification.pdf">Source</a>
</center></p>

<h2>2. Objectives of paper</h2>

<p>Based on this motivation, the authors move step by the step to give this issue a structured treatment along the following objectives:</p>

<figure class="wp-block-table"><table class=""><tbody><tr><td><strong>Quoted from abstract</strong></td><td><strong>Layman explanations</strong></td></tr><tr><td>(i) Provide theoretical results showing that the expected error rate may be a non-monotonous function of <em>T</em>, and explaining under which circumstances this happens</td><td>Show that the error rate of RF may increase or decrease as <em>T</em> increases (non-monotonous), depending on a certain phenomenon that could be observed from the testing dataset.</td></tr><tr><td>(ii) Provide theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the log loss (for classification) and the MSE (for regression)</td><td>Show that such non-monotonicity cannot be observed for performance metrics such as the Brier score, log loss and MSE, even though it can be observed for the error rate - sorry for the double negative, take a moment to digest that before you get confused further.<br><br>(It is also shown in the paper that the ROC-AUC follows such non-monotonicity.)</td></tr><tr><td>(iii) Illustrate the extent of the problem through an application to a large number (306) datasets</td><td>Validate findings via empirical experimentations using public ML datasets</td></tr><tr><td>(iv) Argue in favor of setting <em>T</em> to a computationally feasible large number as long as classical error measures based on average loss are considered</td><td>Conclude that <em>T</em> should not be tuned, but set to a feasibly large number.</td></tr></tbody></table></figure>

<h2>3. Structure of paper</h2>

<ol><li>Motivation and literature review</li><li>Build up RF and various performance metrics for theoretical context</li><li>Theoretical treatment (mainly binary classification, results for multiclass classification and regression are extrapolated and inferred)</li><li>Empirical validation</li><li>Conclusions and extensions</li></ol>

<h2>4. Five takeaways from the authors</h2>

<p>Without boring you as the reader on the specifics of the theoretical and empirical treatments to the problem, here are the key takeaways from the authors (with my translations):</p>

<ol><li><strong>Non-monotonicity (as a function of </strong><em><strong>T</strong></em><strong>) can only be observed for the error rate and the ROC-AUC</strong>, but not for typical performance metrics such as the Brier score, log loss, or MSE.</li><li><strong>*The behaviour of the error rate as a function of </strong><em><strong>T</strong></em><strong> (error rate curve) is largely dependent on the empirical OOB distributions of the prediction errors ε</strong><sub><strong>i</strong></sub> - this is the most important point in this paper for me, to be elaborated further.</li><li><strong>For regression, non-monotonicity can be observed from median-based (as opposed to mean-based) performance metrics</strong>. This makes sense since typical recursive partitioning in the base learners goes for squared error minimization.</li><li><strong>The biggest OOB performance gain comes from the first 250 trees; going from 250 to 2000 trees yields minimal performance gains</strong>. This is true for binary classification, multiclass classification, and regression.</li><li><strong>More trees are better</strong> - non-monotonicity is only observed under specific conditions, with specific performance metrics. Even under non-monotonicity, the difference between the converged metric and its global extreme is minimal. Therefore, <strong>set </strong><em><strong>T</strong></em><strong> to a computationally feasible large number</strong>.</li></ol>

<h2>5. Empirical OOB distributions of prediction errors</h2>

<p>One of the key takeaways from this paper is that the behaviour of the error rate as a function of <em>T</em> (error rate curve) is largely dependent on the empirical OOB distributions of the prediction errors ε<sub>i</sub>. In the paper, it was shown theoretically that the convergence rate of the error rate curve is only dependent on the distribution of ε<sub>i</sub>. In particular,</p>

<ul><li>In a largely accurate (many observations with ε<sub>i</sub>  ≈ 0, few observations with ε<sub>i</sub>  ≈ 1) ensemble, observations with ε<sub>i</sub> &gt; 0.5 will be compensated by observations with ε<sub>i</sub> &lt; 0.5 , in such a way that the error rate curve is monotonously decreasing.</li><li>However, in ensembles with many ε<sub>i</sub>  ≈ 0 and a few ε<sub>i</sub> ≥ 0.5 that are close to 0.5, a problem arises. By the authors' computations, due to these fringe cases (model is uncertain <strong>∴</strong> ε<sub>i</sub>  ≈ 0.5), the error rate curve falls down quickly, then grows again slowly to convergence. The following should make a convincing case:</li></ul>

<p><center>
<img src="https://thestatsguyhome.files.wordpress.com/2019/02/untitled.png" width="100%">
OOB error rate curves for three datasets from OpenML. <a href="http://jmlr.org/papers/v18/17-269.html">Source</a>
</center></p>

<p><center>
<img src="https://thestatsguyhome.files.wordpress.com/2019/02/untitled2.png" width="100%">
OOB ε<sub>i</sub> distributions from RF models on the same three datasets. <a href="http://jmlr.org/papers/v18/17-269.html">Source</a>
</center></p>

<p>These are rather high impact findings as <strong>in practice, the empirical OOB distributions of ε</strong><sub><strong>i</strong></sub><strong> are often not reviewed, especially in the setting of RF or any other ensemble&nbsp;modeling&nbsp;exercise</strong>. (On the other hand, if your training is in statistics, then linear regression diagnostics should be very familiar. Just sayin'.)</p>

<h2>6. Practical implications from this paper</h2>

<p>Finally, all that talk for what we can do in practice:</p>

<ol><li>As part of model maintenance, check on the distribution of ε<sub>i</sub>. When concept drift kicks in in the future, your model could become doubly misspecified - one count from training-testing data disparity and one count on the selection of <em>T</em>. Also notice that the distribution of ε<sub>i&nbsp;</sub>is a function of e.g. size of the dataset (both <em>n</em> and <em>p</em>). This should be intuitive.</li><li>Understand the behavior of your selected performance metric as a function of <em>T</em>. To give a naive example, optimizing ROC-AUC vs. optimizing log loss with respect to <em>T</em> should now be very different to you.</li><li>Finally, under normal or slightly perturbed conditions (again on <br>distribution of ε<sub>i)</sub> , a large <em>T</em> is still better.</li></ol>

</main>

  <footer>
  
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "thestatsguy90" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-165274801-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


<script src="//yihui.org/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  
  <hr/>
  &copy; The Stats Guy 2020 | <a href="https://thestatsguy.netlify.app">Home</a> | <a href="https://github.com/thestatsguy">GitHub</a>
  
  </footer>
  </body>
</html>
